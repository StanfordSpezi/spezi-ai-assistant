This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
Directory Structure
================================================================
.github/
  workflows/
    build-and-test.yml
    monthly-markdown-link-check.yml
    pull_request.yml
FogNode/
  auth/
    firebaseEmulator/
      .firebaserc
      .firebaserc.license
      Dockerfile
      firebase.json
      firebase.json.license
    src/
      authToken.ts
      firebase.ts
      index.ts
    .gitignore
    Dockerfile
    package-lock.json.license
    package.json
    package.json.license
    tsconfig.json
    tsconfig.json.license
  avahi/
    services/
      spezillmfog.service
    docker-entrypoint-sidecar.sh
    Dockerfile
    Dockerfile-Sidecar
  certs/
    openssl.cnf
    openssl.cnf.license
  traefik/
    dynamic_conf.yml
  docker-compose.avahi.yml
  docker-compose.dev.yml
  docker-compose.yml
  README.md
  setup.sh
LICENSES/
  MIT.txt
Sources/
  GeneratedOpenAIClient/
    GeneratedOpenAIClient.swift
    openapi-generator-config.yaml
    openapi.yaml
    package-lock.json.license
    package.json
    package.json.license
    preprocess-openapi-spec.js
    README.md
  SpeziLLM/
    Helpers/
      BundleDescription+Bundle.swift
      LLMContext+Append.swift
      LLMContext+Chat.swift
      LLMContext+Init.swift
    Mock/
      LLMMockPlatform.swift
      LLMMockSchema.swift
      LLMMockSession.swift
    Models/
      LLMContext.swift
      LLMContextEntity.swift
      LLMError.swift
      LLMState.swift
      LLMState+OperationState.swift
    Resources/
      Localizable.xcstrings
      Localizable.xcstrings.license
    SpeziLLM.docc/
      SpeziLLM.md
    Views/
      LLMChatView.swift
      LLMChatViewDisabledModifier.swift
      LLMChatViewSchema.swift
    LLMPlatform.swift
    LLMPlatformBuilder.swift
    LLMPlatformState.swift
    LLMRunner.swift
    LLMSchema.swift
    LLMSession.swift
    LLMSessionProvider.swift
  SpeziLLMFog/
    Configuration/
      LLMFogModelParameters.swift
      LLMFogParameters.swift
      LLMFogPlatformConfiguration.swift
    Connection/
      AuthMiddleware.swift
      URLSession+CertVerification.swift
    Helpers/
      Chat+OpenAI.swift
    Resources/
      Localizable.xcstrings
      Localizable.xcstrings.license
    SpeziLLMFog.docc/
      SpeziLLMFog.md
    LLMFogError.swift
    LLMFogPlatform.swift
    LLMFogSchema.swift
    LLMFogSession.swift
    LLMFogSession+Configuration.swift
    LLMFogSession+Generation.swift
    LLMFogSession+Setup.swift
  SpeziLLMLocal/
    Configuration/
      LLMLocalModel.swift
      LLMLocalParameters.swift
      LLMLocalPlatformConfiguration.swift
      LLMLocalSamplingParameters.swift
    Helpers/
      LLMContext+FormattedChat.swift
      LLMModel+numParameters.swift
    Resources/
      Localizable.xcstrings
      Localizable.xcstrings.license
    SpeziLLMLocal.docc/
      SpeziLLMLocal.md
    LLMLocalError.swift
    LLMLocalPlatform.swift
    LLMLocalSchema.swift
    LLMLocalSession.swift
    LLMLocalSession+Generate.swift
    LLMLocalSession+Setup.swift
    LLMLocalSession+Update.swift
  SpeziLLMLocalDownload/
    Resources/
      Localizable.xcstrings
      Localizable.xcstrings.license
    SpeziLLMLocalDownload.docc/
      SpeziLLMLocalDownload.md
    LLMLocalDownloadManager.swift
    LLMLocalDownloadManager+OperationState.swift
    LLMLocalDownloadView.swift
  SpeziLLMOpenAI/
    Configuration/
      LLMOpenAIModelParameters.swift
      LLMOpenAIParameters.swift
      LLMOpenAIPlatformConfiguration.swift
    FunctionCalling/
      Helpers/
        _LLMFunctionCollection.swift
        LLMFunctionBuilder.swift
        LLMFunctionParameterCodingKey.swift
        LLMFunctionParameterIntermediateRepresentation.swift
        LLMFunctionParameterItemSchema+ObjectInit.swift
        LLMFunctionParameterPropertySchema+Init.swift
      LLMFunction.swift
      LLMFunctionParameter.swift
      LLMFunctionParameterArray.swift
      LLMFunctionParameterEnum.swift
      LLMFunctionParameterSchemaCollector.swift
      LLMFunctionParameterValueCollector.swift
      LLMFunctionParameterWrapper.swift
      LLMFunctionParameterWrapper+ArrayTypes.swift
      LLMFunctionParameterWrapper+CustomTypes.swift
      LLMFunctionParameterWrapper+Enum.swift
      LLMFunctionParameterWrapper+Models.swift
      LLMFunctionParameterWrapper+NilValue.swift
      LLMFunctionParameterWrapper+OptionalTypes.swift
      LLMFunctionParameterWrapper+PrimitiveTypes.swift
    Helpers/
      Chat+OpenAI.swift
      LLMOpenAIConstants.swift
      LLMOpenAIStreamResult.swift
    Onboarding/
      LLMOpenAIAPITokenOnboardingStep.swift
      LLMOpenAIModelOnboardingStep.swift
    Resources/
      Localizable.xcstrings
      Localizable.xcstrings.license
    SpeziLLMOpenAI.docc/
      FunctionCalling.md
      SpeziLLMOpenAI.md
    LLMOpenAIAuthMiddleware.swift
    LLMOpenAIError.swift
    LLMOpenAIPlatform.swift
    LLMOpenAISchema.swift
    LLMOpenAISession.swift
    LLMOpenAISession+Configuration.swift
    LLMOpenAISession+Generation.swift
    LLMOpenAISession+Setup.swift
    LLMOpenAITokenSaver.swift
Tests/
  SpeziLLMTests/
    LLMOpenAIParameterTests+Array.swift
    LLMOpenAIParameterTests+CustomTypes.swift
    LLMOpenAIParameterTests+Enum.swift
    LLMOpenAIParameterTests+InvalidParameters.swift
    LLMOpenAIParameterTests+OptionalTypes.swift
    LLMOpenAIParameterTests+PrimitiveTypes.swift
  UITests/
    TestApp/
      Assets.xcassets/
        AccentColor.colorset/
          Contents.json
          Contents.json.license
        AppIcon.appiconset/
          Contents.json
          Contents.json.license
        Contents.json
        Contents.json.license
      LLMFog/
        Account/
          AccountSetupHeader.swift
          AccountSheet.swift
        LLMFogChatTestView.swift
      LLMLocal/
        Helpers/
          Binding+Negate.swift
          StorageKeys.swift
        Onboarding/
          LLMLocalOnboardingDownloadView.swift
          LLMLocalOnboardingFlow.swift
          LLMLocalOnboardingWelcomeView.swift
        LLMLocalChatTestView.swift
        LLMLocalTestView.swift
      LLMOpenAI/
        Functions/
          LLMOpenAIFunctionHealthData.swift
          LLMOpenAIFunctionPerson.swift
          LLMOpenAIFunctionWeather.swift
        Onboarding/
          LLMOpenAIModelOnboarding.swift
          LLMOpenAITokenOnboarding.swift
        LLMOpenAIChatTestView.swift
        LLMOpenAIOnboardingView.swift
      Resources/
        GoogleService-Info.plist
        GoogleService-Info.plist.license
        Localizable.xcstrings
        Localizable.xcstrings.license
      FeatureFlags.swift
      TestApp.entitlements
      TestApp.entitlements.license
      TestApp.swift
      TestAppDelegate.swift
      TestAppTestingSetup.swift
    TestAppUITests/
      TestAppLLMLocalUITests.swift
      TestAppLLMOpenAIUITests.swift
    UITests.xcodeproj/
      project.xcworkspace/
        xcshareddata/
          IDEWorkspaceChecks.plist
          IDEWorkspaceChecks.plist.license
        contents.xcworkspacedata
        contents.xcworkspacedata.license
      xcshareddata/
        xcschemes/
          TestApp.xcscheme
          TestApp.xcscheme.license
          TestAppRelease.xcscheme.license
      project.pbxproj
      project.pbxproj.license
    TestApp.xctestplan
    TestApp.xctestplan.license
.gitignore
.linkspector.yml
.spi.yml
.swiftlint.yml
CITATION.cff
CONTRIBUTORS.md
LICENSE.md
Package.swift
README.md

================================================================
Files
================================================================

================
File: .github/workflows/build-and-test.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

name: Build and Test

on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:

jobs:
  buildandtest_ios:
    name: Build and Test Swift Package iOS
    uses: StanfordSpezi/.github/.github/workflows/xcodebuild-or-fastlane.yml@v2
    strategy:
      matrix:
        include:
          - buildConfig: Debug
            artifactname: SpeziLLM-iOS.xcresult
            resultBundle: SpeziLLM-iOS.xcresult
          - buildConfig: Release
            artifactname: SpeziLLM-iOS-Release.xcresult
            resultBundle: SpeziLLM-iOS-Release.xcresult
    with:
      runsonlabels: '["macOS", "self-hosted"]'
      scheme: SpeziLLM-Package
      buildConfig: ${{ matrix.buildConfig }}
      resultBundle: ${{ matrix.resultBundle }}
      artifactname: ${{ matrix.artifactname }}
  buildandtest_visionos:
    name: Build and Test Swift Package visionOS
    uses: StanfordSpezi/.github/.github/workflows/xcodebuild-or-fastlane.yml@v2
    strategy:
      matrix:
        include:
          - buildConfig: Debug
            artifactname: SpeziLLM-visionOS.xcresult
            resultBundle: SpeziLLM-visionOS.xcresult
          - buildConfig: Release
            artifactname: SpeziLLM-visionOS-Release.xcresult
            resultBundle: SpeziLLM-visionOS-Release.xcresult
    with:
      runsonlabels: '["macOS", "self-hosted"]'
      scheme: SpeziLLM-Package
      destination: 'platform=visionOS Simulator,name=Apple Vision Pro'
      buildConfig: ${{ matrix.buildConfig }}
      resultBundle: ${{ matrix.resultBundle }}
      artifactname: ${{ matrix.artifactname }}
  buildandtest_macos:
    name: Build and Test Swift Package macOS
    uses: StanfordSpezi/.github/.github/workflows/xcodebuild-or-fastlane.yml@v2
    strategy:
      matrix:
        include:
          - buildConfig: Debug
            artifactname: SpeziLLM-macOS.xcresult
            resultBundle: SpeziLLM-macOS.xcresult
          - buildConfig: Release
            artifactname: SpeziLLM-macOS-Release.xcresult
            resultBundle: SpeziLLM-macOS-Release.xcresult
    with:
      runsonlabels: '["macOS", "self-hosted"]'
      scheme: SpeziLLM-Package
      destination: 'platform=macOS,arch=arm64'
      buildConfig: ${{ matrix.buildConfig }}
      resultBundle: ${{ matrix.resultBundle }}
      artifactname: ${{ matrix.artifactname }}
  buildandtestuitests_ios:
    name: Build and Test UI Tests iOS
    uses: StanfordSpezi/.github/.github/workflows/xcodebuild-or-fastlane.yml@v2
    strategy:
      matrix:
        include:
          - buildConfig: Debug
            resultBundle: TestApp-iOS.xcresult
            artifactname: TestApp-iOS.xcresult
          - buildConfig: Release
            resultBundle: TestApp-iOS-Release.xcresult
            artifactname: TestApp-iOS-Release.xcresult
    with:
      runsonlabels: '["macOS", "self-hosted"]'
      path: 'Tests/UITests'
      scheme: TestApp
      buildConfig: ${{ matrix.buildConfig }}
      resultBundle: ${{ matrix.resultBundle }}
      artifactname: ${{ matrix.artifactname }}
  buildandtestuitests_ipad:
    name: Build and Test UI Tests iPadOS
    uses: StanfordSpezi/.github/.github/workflows/xcodebuild-or-fastlane.yml@v2
    strategy:
      matrix:
        include:
          - buildConfig: Debug
            resultBundle: TestApp-iPad.xcresult
            artifactname: TestApp-iPad.xcresult
          - buildConfig: Release
            resultBundle: TestApp-iPad-Release.xcresult
            artifactname: TestApp-iPad-Release.xcresult
    with:
      runsonlabels: '["macOS", "self-hosted"]'
      path: 'Tests/UITests'
      scheme: TestApp
      destination: 'platform=iOS Simulator,name=iPad Air 11-inch (M2)'
      buildConfig: ${{ matrix.buildConfig }}
      resultBundle: ${{ matrix.resultBundle }}
      artifactname: ${{ matrix.artifactname }}
  buildandtestuitests_visionos:
    name: Build and Test UI Tests visionOS
    uses: StanfordSpezi/.github/.github/workflows/xcodebuild-or-fastlane.yml@v2
    strategy:
      matrix:
        include:
          - buildConfig: Debug
            resultBundle: TestApp-visionOS.xcresult
            artifactname: TestApp-visionOS.xcresult
          - buildConfig: Release
            resultBundle: TestApp-visionOS-Release.xcresult
            artifactname: TestApp-visionOS-Release.xcresult
    with:
      runsonlabels: '["macOS", "self-hosted"]'
      path: 'Tests/UITests'
      scheme: TestApp
      destination: 'platform=visionOS Simulator,name=Apple Vision Pro'
      buildConfig: ${{ matrix.buildConfig }}
      resultBundle: ${{ matrix.resultBundle }}
      artifactname: ${{ matrix.artifactname }}
  uploadcoveragereport:
    name: Upload Coverage Report
    needs: [buildandtest_ios, buildandtest_visionos, buildandtest_macos, buildandtestuitests_ios, buildandtestuitests_ipad, buildandtestuitests_visionos]
    uses: StanfordSpezi/.github/.github/workflows/create-and-upload-coverage-report.yml@v2
    with:
      coveragereports: 'SpeziLLM-iOS.xcresult SpeziLLM-visionOS.xcresult SpeziLLM-macOS.xcresult TestApp-iOS.xcresult TestApp-iPad.xcresult TestApp-visionOS.xcresult'
    secrets:
      token: ${{ secrets.CODECOV_TOKEN }}
  buildandtest_fognode:
    name: Build and Test Fog Node
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3
      - name: Set up Docker Compose
        uses: docker/setup-buildx-action@v2
      - name: Build and Start Docker Services
        run: |
          cd FogNode
          docker compose -f docker-compose.dev.yml up --build -d
      - name: Wait for Services to Start
        run: |
          cd FogNode
          MAX_WAIT=30
          WAIT_TIME=0

          while [ "$(docker compose -f docker-compose.dev.yml ps --filter "status=running" | grep -c "Up")" -lt 4 ]; do
            if [ $WAIT_TIME -ge $MAX_WAIT ]; then
              echo "Timeout: Not all services are up after $MAX_WAIT seconds."
              docker compose -f docker-compose.dev.yml ps
              exit 1
            fi
            echo "Waiting for services to start... ($WAIT_TIME seconds elapsed)"
            docker logs fognode-auth-service-1
            sleep 1
            WAIT_TIME=$((WAIT_TIME + 1))
          done

          echo "All services are up and running!"
      - name: Verify Container Status
        run: |
          cd FogNode
      
          # Check for problematic containers
          EXITED_COUNT=$(docker compose -f docker-compose.dev.yml ps --filter "status=exited" -q | wc -l)
          RESTARTING_COUNT=$(docker compose -f docker-compose.dev.yml ps --filter "status=restarting" -q | wc -l)
          DEAD_COUNT=$(docker compose -f docker-compose.dev.yml ps --filter "status=dead" -q | wc -l)
      
          # Log current counts for visibility
          echo "Exited Containers: $EXITED_COUNT"
          echo "Restarting Containers: $RESTARTING_COUNT"
          echo "Dead Containers: $DEAD_COUNT"
      
          # Fail if any problematic containers are detected
          if [ "$EXITED_COUNT" -gt 0 ] || [ "$RESTARTING_COUNT" -gt 0 ] || [ "$DEAD_COUNT" -gt 0 ]; then
            echo "Error: One or more containers are in a problematic state (exited, restarting, or dead)."
            docker compose -f docker-compose.dev.yml logs
            exit 1
          fi
      
          echo "All containers are running as expected."
      - name: Stop and Remove Containers
        if: always()
        run: |
          cd FogNode
          docker compose -f docker-compose.dev.yml down --remove-orphans

================
File: .github/workflows/monthly-markdown-link-check.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

name: Monthly Markdown Link Check

on:
  # Runs at midnight on the first of every month
  schedule:
    - cron: "0 0 1 * *"

jobs:
  markdown_link_check:
    name: Markdown Link Check
    uses: StanfordBDHG/.github/.github/workflows/markdown-link-check.yml@v2

================
File: .github/workflows/pull_request.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

name: Pull Request

on:
  pull_request:
  workflow_dispatch:

jobs:
  reuse_action:
    name: REUSE Compliance Check
    uses: StanfordSpezi/.github/.github/workflows/reuse.yml@v2
  swiftlint:
    name: SwiftLint
    uses: StanfordSpezi/.github/.github/workflows/swiftlint.yml@v2
  markdown_link_check:
    name: Markdown Link Check
    uses: StanfordBDHG/.github/.github/workflows/markdown-link-check.yml@v2

================
File: FogNode/auth/firebaseEmulator/.firebaserc
================
{
    "projects": {
        "default": "spezillmfog"
    }
}

================
File: FogNode/auth/firebaseEmulator/.firebaserc.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: FogNode/auth/firebaseEmulator/Dockerfile
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

FROM node:22-alpine

LABEL org.opencontainers.image.authors="Philipp Zagar <zagar@stanford.edu>" \
      org.opencontainers.image.version="0.1" \
      org.opencontainers.image.title="stanfordspezi/firebase-emulator-auth" \
      org.opencontainers.image.description="SpeziLLMFog Firebase Emulator Auth" \
      org.opencontainers.image.url="https://ghcr.io/stanfordspezi/firebase-emulator-auth" \
      org.opencontainers.image.source="https://github.com/StanfordSpezi/SpeziLLM"

# Install Firebase CLI
RUN npm install -g firebase-tools

WORKDIR /app

# Copy firebase emulator config files
COPY .firebaserc .firebaserc
COPY firebase.json firebase.json

# Expose web ui and auth service
EXPOSE 4000 9099

# Run the Firebase Emulators
CMD ["firebase", "emulators:start"]

================
File: FogNode/auth/firebaseEmulator/firebase.json
================
{
    "emulators": {
        "auth": {
            "port": 9099,
            "host": "0.0.0.0"
        },
        "ui": {
            "enabled": true,
            "port": 4000,
            "host": "0.0.0.0"
        },
        "singleProjectMode": true
    }
}

================
File: FogNode/auth/firebaseEmulator/firebase.json.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: FogNode/auth/src/authToken.ts
================
export function getTokenFromRequest(req: Request, res: Response): string | null {

================
File: FogNode/auth/src/firebase.ts
================
type EnvVar = string | undefined;
export const initializeFirebase = (): void => {
    const useFirebaseEmulator: EnvVar = process.env.USE_FIREBASE_EMULATOR;
    const firebaseAuthEmulatorHost: EnvVar = process.env.FIREBASE_AUTH_EMULATOR_HOST;
    const firebaseProjectId: EnvVar = process.env.FIREBASE_PROJECT_ID;
            throw new Error(`Environment variables FIREBASE_AUTH_EMULATOR_HOST and FIREBASE_PROJECT_ID are not properly set.`);

================
File: FogNode/auth/src/index.ts
================
app.all('*', async (req: Request, res: Response) => {

================
File: FogNode/auth/.gitignore
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
# 

# Firebase Admin SDK Service Account Key
serviceAccountKey.json

# Compiled TS project
/dist
# NPM dependencies
/node_modules

# IDE
.vscode/
.idea/

# TypeScript cache
*.tsbuildinfo

# Log files
*.log

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage

# nyc test coverage
.nyc_output

# Grunt intermediate storage (<https://gruntjs.com/creating-plugins#storing-task-files>)
.grunt

# Bower dependency directory (<https://bower.io/>)
bower_components

# Dependency directory
# Commenting this out is preferred by some developers, npm can
# handle it properly when it's symlinked (npm v3+)
# node_modules/

# TSD Debug info
tsd-debug.log

================
File: FogNode/auth/Dockerfile
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

# Stage 1: Build stage
FROM node:21-alpine3.19 AS builder

LABEL org.opencontainers.image.authors="Philipp Zagar <zagar@stanford.edu>" \
      org.opencontainers.image.version="0.1" \
      org.opencontainers.image.title="stanfordspezi/firebase-auth-service" \
      org.opencontainers.image.description="SpeziLLMFog Firebase Authentication Service" \
      org.opencontainers.image.url="https://ghcr.io/stanfordspezi/firebase-auth-service" \
      org.opencontainers.image.source="https://github.com/StanfordSpezi/SpeziLLM"

WORKDIR /usr/src/app

# Install npm dependencies
COPY package*.json ./
RUN npm install

# Copy source code and compile TypeScript project
COPY tsconfig.json ./
COPY src/ ./src
RUN npm run build

# Stage 2: Runtime stage
FROM node:21-alpine3.19

WORKDIR /usr/src/app

# Copy compiled files and necessary npm packages from the builder stage
COPY --from=builder /usr/src/app/dist ./dist
COPY --from=builder /usr/src/app/node_modules ./node_modules
COPY --from=builder /usr/src/app/package.json ./package.json

# Start the nodeJS application
CMD [ "node", "dist/index.js" ]

================
File: FogNode/auth/package-lock.json.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: FogNode/auth/package.json
================
{
  "name": "auth-service",
  "version": "1.0.0",
  "description": "Provides authentication services based on the SpeziLLMFog Firebase Admin SDK",
  "main": "index.js",
  "scripts": {
    "start": "node dist/index.js",
    "build": "tsc",
    "dev": "nodemon src/index.ts"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "dependencies": {
    "dotenv": "^16.4.5",
    "express": "^4.21.2",
    "firebase-admin": "^12.3",
    "@types/express": "^4.17.21",
    "@types/node": "^22",
    "ts-node": "^10.9.2",
    "typescript": "^5.5"
  },
  "devDependencies": {
    "nodemon": "^3.1.0"
  }
}

================
File: FogNode/auth/package.json.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: FogNode/auth/tsconfig.json
================
{
    "compilerOptions": {
        "target": "es6",
        "module": "commonjs",
        "outDir": "./dist",
        "strict": true,
        "esModuleInterop": true,
        "skipLibCheck": true
    },
    "include": [
        "src/**/*"
    ],
    "exclude": [
        "node_modules"
    ]
}

================
File: FogNode/auth/tsconfig.json.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: FogNode/avahi/services/spezillmfog.service
================
<!--
                  
This source file is part of the Stanford Spezi open source project

SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT
             
-->

<?xml version="1.0" standalone='no'?><!--*-nxml-*-->
<!DOCTYPE service-group SYSTEM "avahi-service.dtd">
<service-group>
  <name replace-wildcards="yes">SpeziLLMFog-Service</name>
  <service>
    <type>_https._tcp</type>
    <port>443</port>
  </service>
</service-group>

================
File: FogNode/avahi/docker-entrypoint-sidecar.sh
================
#!/bin/sh

#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

# Remove the D-Bus PID file if it exists to avoid startup error
rm -f /run/dbus/dbus.pid

# Start dbus-daemon
dbus-daemon --system --nofork &

# Wait a moment to ensure D-Bus is fully up
sleep 1

# Start avahi-daemon
avahi-daemon --no-chroot --debug

================
File: FogNode/avahi/Dockerfile
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

FROM alpine:3.19

LABEL org.opencontainers.image.authors="Philipp Zagar <zagar@stanford.edu>" \
      org.opencontainers.image.version="0.1" \
      org.opencontainers.image.title="stanfordspezi/avahi" \
      org.opencontainers.image.description="Avahi advertising services via mDNS" \
      org.opencontainers.image.url="https://ghcr.io/stanfordspezi/avahi" \
      org.opencontainers.image.source="https://github.com/StanfordSpezi/SpeziLLM"

# Install Avahi daemon without dbus dependency
RUN apk --no-cache --no-progress add avahi

# Setup services
RUN rm -rf /etc/avahi/services
COPY services/ /etc/avahi/services

# Disable D-Bus in Avahi's configuration
RUN sed -i 's/.*enable-dbus=.*/enable-dbus=no/' /etc/avahi/avahi-daemon.conf

# Run Avahi daemon with non-root user, avoid daemonizing to keep container running
CMD ["avahi-daemon", "--no-chroot"]

================
File: FogNode/avahi/Dockerfile-Sidecar
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

FROM alpine:3.19

LABEL org.opencontainers.image.authors="Philipp Zagar <zagar@stanford.edu>" \
      org.opencontainers.image.version="0.1" \
      org.opencontainers.image.title="stanfordspezi/avahi-sidecar" \
      org.opencontainers.image.description="Avahi sidecar discovering mDNS services" \
      org.opencontainers.image.url="https://ghcr.io/stanfordspezi/avahi-sidecar" \
      org.opencontainers.image.source="https://github.com/StanfordSpezi/SpeziLLM"

# Install Avahi daemon
RUN apk --no-cache --no-progress add avahi avahi-tools dbus

# Setup services
RUN rm -rf /etc/avahi/services
COPY services/ /etc/avahi/services

# Run Avahi daemon with non-root user, avoid daemonizing to keep container running
COPY docker-entrypoint-sidecar.sh /usr/local/bin/docker-entrypoint.sh
RUN chmod +x /usr/local/bin/docker-entrypoint.sh

# Create the D-Bus system bus socket directory
RUN mkdir -p /var/run/dbus && \
    chown messagebus:messagebus /var/run/dbus

ENTRYPOINT ["docker-entrypoint.sh"]

================
File: FogNode/certs/openssl.cnf
================
[req]
default_bits = 2048
prompt = no
default_md = sha256
distinguished_name = dn
req_extensions = req_ext
x509_extensions = v3_ca

[dn]
C = US
ST = California
L = San Francisco
O = Stanford
OU = StanfordSpezi
CN = spezillmfog.local

[req_ext]
subjectAltName = @alt_names
extendedKeyUsage = serverAuth, clientAuth

[ v3_ca ]
subjectAltName = @alt_names
extendedKeyUsage = serverAuth, clientAuth

[alt_names]
DNS.1 = spezillmfog.local

================
File: FogNode/certs/openssl.cnf.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: FogNode/traefik/dynamic_conf.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

tls:
  certificates:
    - certFile: /etc/traefik/certs/spezillmfog.local.crt
      keyFile: /etc/traefik/certs/spezillmfog.local.key
  stores:
    default:
      defaultCertificate:
        certFile: /etc/traefik/certs/spezillmfog.local.crt
        keyFile: /etc/traefik/certs/spezillmfog.local.key

================
File: FogNode/docker-compose.avahi.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

services:
  # Advertises dummy mDNS service to sidecar container
  avahi:
    build:
      context: avahi
    hostname: spezillmfog.local
    networks:
      - avahi

  # Receives advertised dummy mDNS service from avahi container
  avahi-sidecar:
    build:
      context: avahi
      dockerfile: Dockerfile-Sidecar
    hostname: spezillmfog-sidecar.local
    networks:
      - avahi

# Enables to bridge mDNS advertise packages between the two avahi containers
networks:
  avahi:
    driver: bridge

================
File: FogNode/docker-compose.dev.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

services:
  # Reverse proxy routing requests to the Ollama service
  traefik:
    image: traefik:v2.5
    restart: unless-stopped
    command:
      - "--api.insecure=true" # Enables the dashboard and API insecurely
      - "--log.level=DEBUG" # Adjust the log level as needed
      - "--accesslog=true" # Enables access logs
      - "--providers.docker=true"
      - "--providers.docker.exposedByDefault=false"
      - "--entrypoints.web.address=:80"
    ports:
      - "80:80"
      - "8080:8080" # Expose port 8080 for the dashboard
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      - web
    depends_on:
      - ollama
      - auth-service

  # LLM inference service Ollama
  ollama:
    image: ollama/ollama
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ollama.rule=Host(`spezillmfog.local`)"
      - "traefik.http.routers.ollama.entrypoints=web"
      - "traefik.http.routers.ollama.service=ollama-service"
      - "traefik.http.services.ollama-service.loadbalancer.server.port=11434"
      - "traefik.http.routers.ollama.middlewares=auth@docker"
      - "traefik.http.middlewares.auth.forwardauth.address=http://auth-service:3000/"   # Authorizes incoming LLM inference jobs via Firebase Emulator
      - "traefik.http.middlewares.auth.forwardauth.trustForwardHeader=true"     # Forwards all headers to authorization service
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
    networks:
      - web

  # Authorizes incoming LLM inference requests
  auth-service:
    build:
      context: auth
    hostname: auth-service
    restart: unless-stopped
    environment:
      - PORT=3000
      # Use the Firebase emulator
      - USE_FIREBASE_EMULATOR=true
      - FIREBASE_AUTH_EMULATOR_HOST=firebase-emulator:9099
      - FIREBASE_PROJECT_ID=spezillmfog
    labels:
      - "traefik.enable=false"
    ports:
      - "3000:3000"
    networks:
      - web
    depends_on:
      - firebase-emulator

  # Firebase emulator that authenticates the incoming LLM requests
  firebase-emulator:
    build:
      context: auth/firebaseEmulator
    hostname: firebase-emulator
    restart: unless-stopped
    labels:
      - "traefik.enable=false"
    ports:
      - "4000:4000"   # Expose web UI
      - "9099:9099"   # Expose auth emulator service
    networks:
      - web

# Enables persistence of downloaded LLMs by Ollama
volumes:
  ollama_storage:

networks:
  web:
    driver: bridge

================
File: FogNode/docker-compose.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

services:
  # Reverse proxy authenticating requests and routing them to the Ollama service
  traefik:
    image: traefik:v2.5
    restart: unless-stopped
    command:
      - "--providers.docker=true"
      - "--providers.docker.exposedByDefault=false"
      - "--providers.file.filename=/etc/traefik/certs/dynamic_conf.yml"   # Configures TLS certs
      - "--entrypoints.websecure.address=:443"
    ports:
      - "443:443"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "./certs/webservice:/etc/traefik/certs"   # Mount TLS certs into container
      - "./traefik/dynamic_conf.yml:/etc/traefik/certs/dynamic_conf.yml"  # Mount TLS config into container
    networks:
      - web
    depends_on:
      - ollama
      - auth-service
    profiles:
      - "!linux"

  traefik-linux:
    extends: traefik
    depends_on:
      - ollama
      - auth-service
      - avahi
    profiles:
      - linux
  
  # LLM inference service Ollama
  ollama:
    image: ollama/ollama
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ollama.rule=Host(`spezillmfog.local`)"
      - "traefik.http.routers.ollama.entrypoints=websecure"
      - "traefik.http.routers.ollama.tls=true"
      - "traefik.http.routers.ollama.service=ollama-service"
      - "traefik.http.services.ollama-service.loadbalancer.server.port=11434"
      - "traefik.http.routers.ollama.middlewares=auth@docker"
      - "traefik.http.middlewares.auth.forwardauth.address=http://auth-service:3000/"   # Authorizes incoming LLM inference jobs via Firebase
      - "traefik.http.middlewares.auth.forwardauth.trustForwardHeader=true"     # Forwards all headers to authorization service
    volumes:
      - ollama_storage:/root/.ollama
    networks:
      - web

  # Authorizes incoming LLM inference requests
  auth-service:
    build:
      context: auth
    hostname: auth-service
    restart: unless-stopped
    environment:
      - PORT=3000
    labels:
      - "traefik.enable=false"
    volumes:
      - ./auth/serviceAccountKey.json:/usr/src/app/serviceAccountKey.json  # Adjust the host mount location as needed
    networks:
      - web

  # On the Linux platform, advertise LLM inference service via mDNS from Avahi
  avahi:
    build:
      context: avahi
    hostname: spezillmfog.local
    network_mode: host  # Need to run in host network mode for mDNS 
    profiles:
      - linux
    restart: unless-stopped

# Enables persistence of downloaded LLMs by Ollama
volumes:
  ollama_storage:

networks:
  web:
    driver: bridge

================
File: FogNode/README.md
================
<!--
                  
This source file is part of the Stanford Spezi open source project

SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT
             
-->

# SpeziLLMFog FogNode

Offers the functionality to dynamically dispatch LLM inference jobs from mobile devices to fog nodes within the local network that implement the OpenAI API.

## Overview

The client-side implementation of the fog execution environment is part of the Swift-based `SpeziLLM` package, specifically the [`SpeziLLMFog` target](https://swiftpackageindex.com/StanfordSpezi/SpeziLLM/documentation/spezillmfog).
On the other hand, the server-side implementation is a web service that offers LLM inference capabilities within the local network. This setup tutorial demonstrates how to set up the server-side fog node.

## Architecture

The SpeziLLM fog node offers LLM inference capabilities within the local network.
It consists of the following components:

- **LLM Inference capabilities**: The LLM inference is performed by the [Ollama open-source framework](https://github.com/ollama/ollama) that executes openly available LLMs such as [Llama2](https://ollama.com/library/llama2) or [Gemma](https://ollama.com/library/gemma). A full list of the available models can be found [here](https://ollama.com/library). The API surface of [Ollama mirrors the OpenAI API](https://github.com/ollama/ollama/blob/main/docs/openai.md), at least for basic inference requests.
- **Service advertisement**: As SpeziLLM intends to operate within a [fog computing environment](https://en.wikipedia.org/wiki/Fog_computing), the LLM execution resource (the LLM webservice) is advertised within the local network via [mDNS](https://en.wikipedia.org/wiki/Multicast_DNS). On macOS, this is achieved via [Apple's Bonjour framework](https://developer.apple.com/bonjour), on Linux the [Avahi component](https://avahi.org/) is used (both of these services are interoperable with another). 
- **Secure local connections**: To ensure secure data transfer from and to the fog node within the local network, the [`traefik` reverse proxy](https://traefik.io/traefik/) only serves the LLM inference API via secure SSL connections. The TLS verification is achieved via a custom-issued [root CA certificate](https://en.wikipedia.org/wiki/Root_certificate) that signs the TLS certificate of the web service offering the LLM inference jobs. As these certificates need to be unique and secret, they are not part of the FogNode package but are rather generated by a script on the respective computing resource by the administrator (see setup instructions below)
- **User authentication**: The fog node requires user authentication by verifying the passed [Firebase User ID Bearer token](https://firebase.google.com/docs/auth/admin/verify-id-tokens) in the HTTP Authentication header. By default, the fog node only verifies the authenticity of the User ID token, not if the user is actually allowed to access the resource (this could be achieved by, e.g., custom claims in the token).
- **Packaging**: Lastly, as the fog node should be able to run on diverse platforms and the setup process should be as easy as possible, the entire fog component is packaged via [Docker](https://www.docker.com/).

## Setup

In order to correctly set up the Fog node, a couple of setup steps have to been taken. These steps are performed via the `setup.sh` shell script.

### Requirements

- Operating system: Either Linux or macOS
- [Docker Engine v25.0](https://docs.docker.com/engine/install/) as well as [Docker Compose v2](https://docs.docker.com/compose/install/)
- On macOS, one needs to use [Bonjour](https://developer.apple.com/bonjour) for mDNS advertisements (as Avahi only works on Linux distributions)

### Executing the setup script

The `setup.sh` script generates the custom CA root certificate as well as the web service certificate. They are persisted in the `ca` as well as `webservice` directories. Keep in mind that the application using the Fog Node (most likely via `SpeziLLMFog`) must trust this custom root CA certificate.
Once the script ran through, the last output of the script should be a warning about issuing the Firebase service account key via the Firebase console. Put the file within the `auth` directory under the name `serviceAccountKey.json`, it is then automatically picked up by the authorization service.

Lastly, start the container services via Docker Compose:
- On Linux, execute `docker compose --platform=linux up` to start the service, use the `-d` flag to run it in the background like: `docker compose --platform=linux up -d`. The service is automatically advertised by Avahi via mDNS from the Docker service.
- On macOS, run `docker compose up` to start the service. In addition, because of technical limitations of Avahi within a Docker container on macOS, one has to manually run the mDNS advertisement via Bonjour: `dns-sd -R "SpeziLLMFog Service" _https._tcp spezillmfog.local 443`. It advertises the service under the `spezillmfog.local` domain name with the `"SpeziLLMFog Service"` user-friendly name.

### Development

For development purposes, the `docker-compose.dev.yml` file starts up the fog node without TLS certificates and with the usage of the Firebase Emulator. In that case, one doesn't have to execute the setup script mentioned above (as no certificates are required without a TLS connection) and doesn't have to get the Firebase service account key from the Firebase Console.
In addition, this development compose file doesn't include an mDNS advertisement service. The developer is responsible for advertising the service. On macOS, which is the primary development environment for SpeziLLMFog, this can be done via Bonjour and the `dns-sd -R "SpeziLLMFog Service" _http._tcp spezillmfog.local 80` command. Note that the service advertises an `http` service with port 80, in contrast to the production setup with HTTPS and port 443 (secure traffic).

Another file for development purposes is the `docker-compose.avahi.yml` file. One container advertises an mDNS service via Avahi, another container discovers this service via an Avahi Sidecar. This setup is incredibly useful to test mDNS announcements on the Linux platform.

================
File: FogNode/setup.sh
================
#!/bin/bash

#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#

cd certs

# Issue the custom root CA certificate with a passphrase for the private key
openssl req -new -x509 -days 3650 -keyout ca/ca.key -out ca/ca.crt -subj "/CN=SpeziLLMFog CA" -passout pass:SpeziLLMFogPassword

# Create the web service key
openssl genrsa -out webservice/spezillmfog.local.key 2048

# Generate a signing request for the web service key
openssl req -new -key webservice/spezillmfog.local.key -out webservice/spezillmfog.local.csr -config openssl.cnf

# Sign the web service key with the CA certificate, using the CA's passphrase to access the private
openssl x509 -req -in webservice/spezillmfog.local.csr -CA ca/ca.crt -CAkey ca/ca.key -CAcreateserial -out webservice/spezillmfog.local.crt -days 365 -sha256 -extfile openssl.cnf -extensions v3_ca -passin pass:SpeziLLMFogPassword

RED=$(tput setaf 1)
GREEN=$(tput setaf 2)
RESET=$(tput sgr0)

echo ""
echo "${GREEN}Success: The root CA certificate as well as the webservice certificate were sucessfully issued.${RESET}"
echo "${RED}Warning: Issue the Firebase Admin Service Account key via the Firebase Console and place it within the 'auth' directory under the name 'serviceAccountKey.json', if not using the Firebase Emulator.${RESET}"

================
File: LICENSES/MIT.txt
================
MIT License

Copyright (c) 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

================
File: Sources/GeneratedOpenAIClient/GeneratedOpenAIClient.swift
================


================
File: Sources/GeneratedOpenAIClient/openapi-generator-config.yaml
================
# 
# This source file is part of the Stanford Spezi open source project
# 
# SPDX-FileCopyrightText: 2024 Stanford University and the project authors (see CONTRIBUTORS.md)
# 
# SPDX-License-Identifier: MIT
# 

generate:
  - types
  - client
accessModifier: package

================
File: Sources/GeneratedOpenAIClient/openapi.yaml
================
#
# This source file is part of the Stanford Spezi open source project.
# It is based on the official OpenAI OpenAPI spec with modifications by the Spezi project authors: https://github.com/openai/openai-openapi/blob/master/openapi.yaml
#
# SPDX-FileCopyrightText: 2024 Stanford University, OpenAI, and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
# 

openapi: 3.0.0
info:
  title: OpenAI API
  description: >-
    The OpenAI REST API. Please see
    https://platform.openai.com/docs/api-reference for more details.
  version: 2.3.0
  termsOfService: https://openai.com/policies/terms-of-use
  contact:
    name: OpenAI Support
    url: https://help.openai.com/
  license:
    name: MIT
    url: https://github.com/openai/openai-openapi/blob/master/LICENSE
servers:
  - url: https://api.openai.com/v1
tags:
  - name: Assistants
    description: Build Assistants that can call models and use tools.
  - name: Audio
    description: Turn audio into text or text into audio.
  - name: Chat
    description: >-
      Given a list of messages comprising a conversation, the model will return
      a response.
  - name: Completions
    description: >-
      Given a prompt, the model will return one or more predicted completions,
      and can also return the probabilities of alternative tokens at each
      position.
  - name: Embeddings
    description: >-
      Get a vector representation of a given input that can be easily consumed
      by machine learning models and algorithms.
  - name: Fine-tuning
    description: Manage fine-tuning jobs to tailor a model to your specific training data.
  - name: Batch
    description: Create large batches of API requests to run asynchronously.
  - name: Files
    description: >-
      Files are used to upload documents that can be used with features like
      Assistants and Fine-tuning.
  - name: Uploads
    description: Use Uploads to upload large files in multiple parts.
  - name: Images
    description: Given a prompt and/or an input image, the model will generate a new image.
  - name: Models
    description: List and describe the various models available in the API.
  - name: Moderations
    description: >-
      Given text and/or image inputs, classifies if those inputs are potentially
      harmful.
  - name: Audit Logs
    description: List user actions and configuration changes within this organization.
paths:
  /assistants:
    get:
      operationId: listAssistants
      tags:
        - Assistants
      summary: Returns a list of assistants.
      parameters:
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: order
          in: query
          description: >
            Sort order by the `created_at` timestamp of the objects. `asc` for
            ascending order and `desc` for descending order.
          schema:
            type: string
            default: desc
            enum:
              - asc
              - desc
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, starting with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          schema:
            type: string
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListAssistantsResponse"
      x-oaiMeta:
        name: List assistants
        group: assistants
        beta: true
        returns: A list of [assistant](/docs/api-reference/assistants/object) objects.
        examples:
          request:
            curl: |
              curl "https://api.openai.com/v1/assistants?order=desc&limit=20" \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              my_assistants = client.beta.assistants.list(
                  order="desc",
                  limit="20",
              )
              print(my_assistants.data)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const myAssistants = await openai.beta.assistants.list({
                  order: "desc",
                  limit: "20",
                });

                console.log(myAssistants.data);
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "asst_abc123",
                  "object": "assistant",
                  "created_at": 1698982736,
                  "name": "Coding Tutor",
                  "description": null,
                  "model": "gpt-4o",
                  "instructions": "You are a helpful assistant designed to make me better at coding!",
                  "tools": [],
                  "tool_resources": {},
                  "metadata": {},
                  "top_p": 1.0,
                  "temperature": 1.0,
                  "response_format": "auto"
                },
                {
                  "id": "asst_abc456",
                  "object": "assistant",
                  "created_at": 1698982718,
                  "name": "My Assistant",
                  "description": null,
                  "model": "gpt-4o",
                  "instructions": "You are a helpful assistant designed to make me better at coding!",
                  "tools": [],
                  "tool_resources": {},
                  "metadata": {},
                  "top_p": 1.0,
                  "temperature": 1.0,
                  "response_format": "auto"
                },
                {
                  "id": "asst_abc789",
                  "object": "assistant",
                  "created_at": 1698982643,
                  "name": null,
                  "description": null,
                  "model": "gpt-4o",
                  "instructions": null,
                  "tools": [],
                  "tool_resources": {},
                  "metadata": {},
                  "top_p": 1.0,
                  "temperature": 1.0,
                  "response_format": "auto"
                }
              ],
              "first_id": "asst_abc123",
              "last_id": "asst_abc789",
              "has_more": false
            }
    post:
      operationId: createAssistant
      tags:
        - Assistants
      summary: Create an assistant with a model and instructions.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateAssistantRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AssistantObject"
      x-oaiMeta:
        name: Create assistant
        group: assistants
        beta: true
        returns: An [assistant](/docs/api-reference/assistants/object) object.
        examples:
          - title: Code Interpreter
            request:
              curl: |
                curl "https://api.openai.com/v1/assistants" \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",
                    "name": "Math Tutor",
                    "tools": [{"type": "code_interpreter"}],
                    "model": "gpt-4o"
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                my_assistant = client.beta.assistants.create(
                    instructions="You are a personal math tutor. When asked a question, write and run Python code to answer the question.",
                    name="Math Tutor",
                    tools=[{"type": "code_interpreter"}],
                    model="gpt-4o",
                )
                print(my_assistant)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const myAssistant = await openai.beta.assistants.create({
                    instructions:
                      "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",
                    name: "Math Tutor",
                    tools: [{ type: "code_interpreter" }],
                    model: "gpt-4o",
                  });

                  console.log(myAssistant);
                }

                main();
            response: |
              {
                "id": "asst_abc123",
                "object": "assistant",
                "created_at": 1698984975,
                "name": "Math Tutor",
                "description": null,
                "model": "gpt-4o",
                "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",
                "tools": [
                  {
                    "type": "code_interpreter"
                  }
                ],
                "metadata": {},
                "top_p": 1.0,
                "temperature": 1.0,
                "response_format": "auto"
              }
          - title: Files
            request:
              curl: |
                curl https://api.openai.com/v1/assistants \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "instructions": "You are an HR bot, and you have access to files to answer employee questions about company policies.",
                    "tools": [{"type": "file_search"}],
                    "tool_resources": {"file_search": {"vector_store_ids": ["vs_123"]}},
                    "model": "gpt-4o"
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                my_assistant = client.beta.assistants.create(
                    instructions="You are an HR bot, and you have access to files to answer employee questions about company policies.",
                    name="HR Helper",
                    tools=[{"type": "file_search"}],
                    tool_resources={"file_search": {"vector_store_ids": ["vs_123"]}},
                    model="gpt-4o"
                )
                print(my_assistant)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const myAssistant = await openai.beta.assistants.create({
                    instructions:
                      "You are an HR bot, and you have access to files to answer employee questions about company policies.",
                    name: "HR Helper",
                    tools: [{ type: "file_search" }],
                    tool_resources: {
                      file_search: {
                        vector_store_ids: ["vs_123"]
                      }
                    },
                    model: "gpt-4o"
                  });

                  console.log(myAssistant);
                }

                main();
            response: |
              {
                "id": "asst_abc123",
                "object": "assistant",
                "created_at": 1699009403,
                "name": "HR Helper",
                "description": null,
                "model": "gpt-4o",
                "instructions": "You are an HR bot, and you have access to files to answer employee questions about company policies.",
                "tools": [
                  {
                    "type": "file_search"
                  }
                ],
                "tool_resources": {
                  "file_search": {
                    "vector_store_ids": ["vs_123"]
                  }
                },
                "metadata": {},
                "top_p": 1.0,
                "temperature": 1.0,
                "response_format": "auto"
              }
  /assistants/{assistant_id}:
    get:
      operationId: getAssistant
      tags:
        - Assistants
      summary: Retrieves an assistant.
      parameters:
        - in: path
          name: assistant_id
          required: true
          schema:
            type: string
          description: The ID of the assistant to retrieve.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AssistantObject"
      x-oaiMeta:
        name: Retrieve assistant
        group: assistants
        beta: true
        returns: >-
          The [assistant](/docs/api-reference/assistants/object) object matching
          the specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/assistants/asst_abc123 \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              my_assistant = client.beta.assistants.retrieve("asst_abc123")
              print(my_assistant)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const myAssistant = await openai.beta.assistants.retrieve(
                  "asst_abc123"
                );

                console.log(myAssistant);
              }

              main();
          response: |
            {
              "id": "asst_abc123",
              "object": "assistant",
              "created_at": 1699009709,
              "name": "HR Helper",
              "description": null,
              "model": "gpt-4o",
              "instructions": "You are an HR bot, and you have access to files to answer employee questions about company policies.",
              "tools": [
                {
                  "type": "file_search"
                }
              ],
              "metadata": {},
              "top_p": 1.0,
              "temperature": 1.0,
              "response_format": "auto"
            }
    post:
      operationId: modifyAssistant
      tags:
        - Assistants
      summary: Modifies an assistant.
      parameters:
        - in: path
          name: assistant_id
          required: true
          schema:
            type: string
          description: The ID of the assistant to modify.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ModifyAssistantRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AssistantObject"
      x-oaiMeta:
        name: Modify assistant
        group: assistants
        beta: true
        returns: >-
          The modified [assistant](/docs/api-reference/assistants/object)
          object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/assistants/asst_abc123 \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2" \
                -d '{
                    "instructions": "You are an HR bot, and you have access to files to answer employee questions about company policies. Always response with info from either of the files.",
                    "tools": [{"type": "file_search"}],
                    "model": "gpt-4o"
                  }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              my_updated_assistant = client.beta.assistants.update(
                "asst_abc123",
                instructions="You are an HR bot, and you have access to files to answer employee questions about company policies. Always response with info from either of the files.",
                name="HR Helper",
                tools=[{"type": "file_search"}],
                model="gpt-4o"
              )

              print(my_updated_assistant)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const myUpdatedAssistant = await openai.beta.assistants.update(
                  "asst_abc123",
                  {
                    instructions:
                      "You are an HR bot, and you have access to files to answer employee questions about company policies. Always response with info from either of the files.",
                    name: "HR Helper",
                    tools: [{ type: "file_search" }],
                    model: "gpt-4o"
                  }
                );

                console.log(myUpdatedAssistant);
              }

              main();
          response: |
            {
              "id": "asst_123",
              "object": "assistant",
              "created_at": 1699009709,
              "name": "HR Helper",
              "description": null,
              "model": "gpt-4o",
              "instructions": "You are an HR bot, and you have access to files to answer employee questions about company policies. Always response with info from either of the files.",
              "tools": [
                {
                  "type": "file_search"
                }
              ],
              "tool_resources": {
                "file_search": {
                  "vector_store_ids": []
                }
              },
              "metadata": {},
              "top_p": 1.0,
              "temperature": 1.0,
              "response_format": "auto"
            }
    delete:
      operationId: deleteAssistant
      tags:
        - Assistants
      summary: Delete an assistant.
      parameters:
        - in: path
          name: assistant_id
          required: true
          schema:
            type: string
          description: The ID of the assistant to delete.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteAssistantResponse"
      x-oaiMeta:
        name: Delete assistant
        group: assistants
        beta: true
        returns: Deletion status
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/assistants/asst_abc123 \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2" \
                -X DELETE
            python: |
              from openai import OpenAI
              client = OpenAI()

              response = client.beta.assistants.delete("asst_abc123")
              print(response)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const response = await openai.beta.assistants.del("asst_abc123");

                console.log(response);
              }
              main();
          response: |
            {
              "id": "asst_abc123",
              "object": "assistant.deleted",
              "deleted": true
            }
  /audio/speech:
    post:
      operationId: createSpeech
      tags:
        - Audio
      summary: Generates audio from the input text.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateSpeechRequest"
      responses:
        "200":
          description: OK
          headers:
            Transfer-Encoding:
              schema:
                type: string
              description: chunked
          content:
            application/octet-stream:
              schema:
                type: string
                format: binary
      x-oaiMeta:
        name: Create speech
        group: audio
        returns: The audio file content.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/audio/speech \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                  "model": "tts-1",
                  "input": "The quick brown fox jumped over the lazy dog.",
                  "voice": "alloy"
                }' \
                --output speech.mp3
            python: |
              from pathlib import Path
              import openai

              speech_file_path = Path(__file__).parent / "speech.mp3"
              response = openai.audio.speech.create(
                model="tts-1",
                voice="alloy",
                input="The quick brown fox jumped over the lazy dog."
              )
              response.stream_to_file(speech_file_path)
            node: |
              import fs from "fs";
              import path from "path";
              import OpenAI from "openai";

              const openai = new OpenAI();

              const speechFile = path.resolve("./speech.mp3");

              async function main() {
                const mp3 = await openai.audio.speech.create({
                  model: "tts-1",
                  voice: "alloy",
                  input: "Today is a wonderful day to build something people love!",
                });
                console.log(speechFile);
                const buffer = Buffer.from(await mp3.arrayBuffer());
                await fs.promises.writeFile(speechFile, buffer);
              }
              main();
  /audio/transcriptions:
    post:
      operationId: createTranscription
      tags:
        - Audio
      summary: Transcribes audio into the input language.
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: "#/components/schemas/CreateTranscriptionRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                oneOf:
                  - $ref: "#/components/schemas/CreateTranscriptionResponseJson"
                  - $ref: >-
                      #/components/schemas/CreateTranscriptionResponseVerboseJson
      x-oaiMeta:
        name: Create transcription
        group: audio
        returns: >-
          The [transcription object](/docs/api-reference/audio/json-object) or a
          [verbose transcription
          object](/docs/api-reference/audio/verbose-json-object).
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/audio.mp3" \
                  -F model="whisper-1"
              python: |
                from openai import OpenAI
                client = OpenAI()

                audio_file = open("speech.mp3", "rb")
                transcript = client.audio.transcriptions.create(
                  model="whisper-1",
                  file=audio_file
                )
              node: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const transcription = await openai.audio.transcriptions.create({
                    file: fs.createReadStream("audio.mp3"),
                    model: "whisper-1",
                  });

                  console.log(transcription.text);
                }
                main();
            response: |
              {
                "text": "Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that."
              }
          - title: Word timestamps
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/audio.mp3" \
                  -F "timestamp_granularities[]=word" \
                  -F model="whisper-1" \
                  -F response_format="verbose_json"
              python: |
                from openai import OpenAI
                client = OpenAI()

                audio_file = open("speech.mp3", "rb")
                transcript = client.audio.transcriptions.create(
                  file=audio_file,
                  model="whisper-1",
                  response_format="verbose_json",
                  timestamp_granularities=["word"]
                )

                print(transcript.words)
              node: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const transcription = await openai.audio.transcriptions.create({
                    file: fs.createReadStream("audio.mp3"),
                    model: "whisper-1",
                    response_format: "verbose_json",
                    timestamp_granularities: ["word"]
                  });

                  console.log(transcription.text);
                }
                main();
            response: |
              {
                "task": "transcribe",
                "language": "english",
                "duration": 8.470000267028809,
                "text": "The beach was a popular spot on a hot summer day. People were swimming in the ocean, building sandcastles, and playing beach volleyball.",
                "words": [
                  {
                    "word": "The",
                    "start": 0.0,
                    "end": 0.23999999463558197
                  },
                  ...
                  {
                    "word": "volleyball",
                    "start": 7.400000095367432,
                    "end": 7.900000095367432
                  }
                ]
              }
          - title: Segment timestamps
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/audio.mp3" \
                  -F "timestamp_granularities[]=segment" \
                  -F model="whisper-1" \
                  -F response_format="verbose_json"
              python: |
                from openai import OpenAI
                client = OpenAI()

                audio_file = open("speech.mp3", "rb")
                transcript = client.audio.transcriptions.create(
                  file=audio_file,
                  model="whisper-1",
                  response_format="verbose_json",
                  timestamp_granularities=["segment"]
                )

                print(transcript.words)
              node: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const transcription = await openai.audio.transcriptions.create({
                    file: fs.createReadStream("audio.mp3"),
                    model: "whisper-1",
                    response_format: "verbose_json",
                    timestamp_granularities: ["segment"]
                  });

                  console.log(transcription.text);
                }
                main();
            response: |
              {
                "task": "transcribe",
                "language": "english",
                "duration": 8.470000267028809,
                "text": "The beach was a popular spot on a hot summer day. People were swimming in the ocean, building sandcastles, and playing beach volleyball.",
                "segments": [
                  {
                    "id": 0,
                    "seek": 0,
                    "start": 0.0,
                    "end": 3.319999933242798,
                    "text": " The beach was a popular spot on a hot summer day.",
                    "tokens": [
                      50364, 440, 7534, 390, 257, 3743, 4008, 322, 257, 2368, 4266, 786, 13, 50530
                    ],
                    "temperature": 0.0,
                    "avg_logprob": -0.2860786020755768,
                    "compression_ratio": 1.2363636493682861,
                    "no_speech_prob": 0.00985979475080967
                  },
                  ...
                ]
              }
  /audio/translations:
    post:
      operationId: createTranslation
      tags:
        - Audio
      summary: Translates audio into English.
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: "#/components/schemas/CreateTranslationRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                oneOf:
                  - $ref: "#/components/schemas/CreateTranslationResponseJson"
                  - $ref: "#/components/schemas/CreateTranslationResponseVerboseJson"
      x-oaiMeta:
        name: Create translation
        group: audio
        returns: The translated text.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/audio/translations \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: multipart/form-data" \
                -F file="@/path/to/file/german.m4a" \
                -F model="whisper-1"
            python: |
              from openai import OpenAI
              client = OpenAI()

              audio_file = open("speech.mp3", "rb")
              transcript = client.audio.translations.create(
                model="whisper-1",
                file=audio_file
              )
            node: |
              import fs from "fs";
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                  const translation = await openai.audio.translations.create({
                      file: fs.createReadStream("speech.mp3"),
                      model: "whisper-1",
                  });

                  console.log(translation.text);
              }
              main();
          response: |
            {
              "text": "Hello, my name is Wolfgang and I come from Germany. Where are you heading today?"
            }
  /batches:
    post:
      summary: Creates and executes a batch from an uploaded file of requests
      operationId: createBatch
      tags:
        - Batch
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - input_file_id
                - endpoint
                - completion_window
              properties:
                input_file_id:
                  type: string
                  description: >
                    The ID of an uploaded file that contains requests for the
                    new batch.


                    See [upload file](/docs/api-reference/files/create) for how
                    to upload a file.


                    Your input file must be formatted as a [JSONL
                    file](/docs/api-reference/batch/request-input), and must be
                    uploaded with the purpose `batch`. The file can contain up
                    to 50,000 requests, and can be up to 200 MB in size.
                endpoint:
                  type: string
                  enum:
                    - /v1/chat/completions
                    - /v1/embeddings
                    - /v1/completions
                  description: >-
                    The endpoint to be used for all requests in the batch.
                    Currently `/v1/chat/completions`, `/v1/embeddings`, and
                    `/v1/completions` are supported. Note that `/v1/embeddings`
                    batches are also restricted to a maximum of 50,000 embedding
                    inputs across all requests in the batch.
                completion_window:
                  type: string
                  enum:
                    - 24h
                  description: >-
                    The time frame within which the batch should be processed.
                    Currently only `24h` is supported.
                metadata:
                  $ref: "#/components/schemas/Metadata"
      responses:
        "200":
          description: Batch created successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Batch"
      x-oaiMeta:
        name: Create batch
        group: batch
        returns: The created [Batch](/docs/api-reference/batch/object) object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/batches \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                  "input_file_id": "file-abc123",
                  "endpoint": "/v1/chat/completions",
                  "completion_window": "24h"
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.batches.create(
                input_file_id="file-abc123",
                endpoint="/v1/chat/completions",
                completion_window="24h"
              )
            node: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const batch = await openai.batches.create({
                  input_file_id: "file-abc123",
                  endpoint: "/v1/chat/completions",
                  completion_window: "24h"
                });

                console.log(batch);
              }

              main();
          response: |
            {
              "id": "batch_abc123",
              "object": "batch",
              "endpoint": "/v1/chat/completions",
              "errors": null,
              "input_file_id": "file-abc123",
              "completion_window": "24h",
              "status": "validating",
              "output_file_id": null,
              "error_file_id": null,
              "created_at": 1711471533,
              "in_progress_at": null,
              "expires_at": null,
              "finalizing_at": null,
              "completed_at": null,
              "failed_at": null,
              "expired_at": null,
              "cancelling_at": null,
              "cancelled_at": null,
              "request_counts": {
                "total": 0,
                "completed": 0,
                "failed": 0
              },
              "metadata": {
                "customer_id": "user_123456789",
                "batch_description": "Nightly eval job",
              }
            }
    get:
      operationId: listBatches
      tags:
        - Batch
      summary: List your organization's batches.
      parameters:
        - in: query
          name: after
          required: false
          schema:
            type: string
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
      responses:
        "200":
          description: Batch listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListBatchesResponse"
      x-oaiMeta:
        name: List batch
        group: batch
        returns: A list of paginated [Batch](/docs/api-reference/batch/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/batches?limit=2 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.batches.list()
            node: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const list = await openai.batches.list();

                for await (const batch of list) {
                  console.log(batch);
                }
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "batch_abc123",
                  "object": "batch",
                  "endpoint": "/v1/chat/completions",
                  "errors": null,
                  "input_file_id": "file-abc123",
                  "completion_window": "24h",
                  "status": "completed",
                  "output_file_id": "file-cvaTdG",
                  "error_file_id": "file-HOWS94",
                  "created_at": 1711471533,
                  "in_progress_at": 1711471538,
                  "expires_at": 1711557933,
                  "finalizing_at": 1711493133,
                  "completed_at": 1711493163,
                  "failed_at": null,
                  "expired_at": null,
                  "cancelling_at": null,
                  "cancelled_at": null,
                  "request_counts": {
                    "total": 100,
                    "completed": 95,
                    "failed": 5
                  },
                  "metadata": {
                    "customer_id": "user_123456789",
                    "batch_description": "Nightly job",
                  }
                },
                { ... },
              ],
              "first_id": "batch_abc123",
              "last_id": "batch_abc456",
              "has_more": true
            }
  /batches/{batch_id}:
    get:
      operationId: retrieveBatch
      tags:
        - Batch
      summary: Retrieves a batch.
      parameters:
        - in: path
          name: batch_id
          required: true
          schema:
            type: string
          description: The ID of the batch to retrieve.
      responses:
        "200":
          description: Batch retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Batch"
      x-oaiMeta:
        name: Retrieve batch
        group: batch
        returns: >-
          The [Batch](/docs/api-reference/batch/object) object matching the
          specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/batches/batch_abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.batches.retrieve("batch_abc123")
            node: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const batch = await openai.batches.retrieve("batch_abc123");

                console.log(batch);
              }

              main();
          response: |
            {
              "id": "batch_abc123",
              "object": "batch",
              "endpoint": "/v1/completions",
              "errors": null,
              "input_file_id": "file-abc123",
              "completion_window": "24h",
              "status": "completed",
              "output_file_id": "file-cvaTdG",
              "error_file_id": "file-HOWS94",
              "created_at": 1711471533,
              "in_progress_at": 1711471538,
              "expires_at": 1711557933,
              "finalizing_at": 1711493133,
              "completed_at": 1711493163,
              "failed_at": null,
              "expired_at": null,
              "cancelling_at": null,
              "cancelled_at": null,
              "request_counts": {
                "total": 100,
                "completed": 95,
                "failed": 5
              },
              "metadata": {
                "customer_id": "user_123456789",
                "batch_description": "Nightly eval job",
              }
            }
  /batches/{batch_id}/cancel:
    post:
      operationId: cancelBatch
      tags:
        - Batch
      summary: >-
        Cancels an in-progress batch. The batch will be in status `cancelling`
        for up to 10 minutes, before changing to `cancelled`, where it will have
        partial results (if any) available in the output file.
      parameters:
        - in: path
          name: batch_id
          required: true
          schema:
            type: string
          description: The ID of the batch to cancel.
      responses:
        "200":
          description: Batch is cancelling. Returns the cancelling batch's details.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Batch"
      x-oaiMeta:
        name: Cancel batch
        group: batch
        returns: >-
          The [Batch](/docs/api-reference/batch/object) object matching the
          specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/batches/batch_abc123/cancel \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -X POST
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.batches.cancel("batch_abc123")
            node: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const batch = await openai.batches.cancel("batch_abc123");

                console.log(batch);
              }

              main();
          response: |
            {
              "id": "batch_abc123",
              "object": "batch",
              "endpoint": "/v1/chat/completions",
              "errors": null,
              "input_file_id": "file-abc123",
              "completion_window": "24h",
              "status": "cancelling",
              "output_file_id": null,
              "error_file_id": null,
              "created_at": 1711471533,
              "in_progress_at": 1711471538,
              "expires_at": 1711557933,
              "finalizing_at": null,
              "completed_at": null,
              "failed_at": null,
              "expired_at": null,
              "cancelling_at": 1711475133,
              "cancelled_at": null,
              "request_counts": {
                "total": 100,
                "completed": 23,
                "failed": 1
              },
              "metadata": {
                "customer_id": "user_123456789",
                "batch_description": "Nightly eval job",
              }
            }
  /chat/completions:
    post:
      operationId: createChatCompletion
      tags:
        - Chat
      summary: >
        Creates a model response for the given chat conversation. Learn more in
        the

        [text generation](/docs/guides/text-generation),
        [vision](/docs/guides/vision),

        and [audio](/docs/guides/audio) guides.


        Parameter support can differ depending on the model used to generate the

        response, particularly for newer reasoning models. Parameters that are
        only

        supported for reasoning models are noted below. For the current state
        of 

        unsupported parameters in reasoning models, 

        [refer to the reasoning guide](/docs/guides/reasoning).
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateChatCompletionRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionResponse"
            text/event-stream:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionStreamResponse"
      x-oaiMeta:
        name: Create chat completion
        group: chat
        returns: >
          Returns a [chat completion](/docs/api-reference/chat/object) object,
          or a streamed sequence of [chat completion
          chunk](/docs/api-reference/chat/streaming) objects if the request is
          streamed.
        path: create
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_chat_model_id",
                    "messages": [
                      {
                        "role": "developer",
                        "content": "You are a helpful assistant."
                      },
                      {
                        "role": "user",
                        "content": "Hello!"
                      }
                    ]
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                completion = client.chat.completions.create(
                  model="VAR_chat_model_id",
                  messages=[
                    {"role": "developer", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ]
                )

                print(completion.choices[0].message)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    messages: [{ role: "developer", content: "You are a helpful assistant." }],
                    model: "VAR_chat_model_id",
                    store: true,
                  });

                  console.log(completion.choices[0]);
                }

                main();
            response: |
              {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1677652288,
                "model": "gpt-4o-mini",
                "system_fingerprint": "fp_44709d6fcb",
                "choices": [{
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": "\n\nHello there, how may I assist you today?",
                  },
                  "logprobs": null,
                  "finish_reason": "stop"
                }],
                "service_tier": "default",
                "usage": {
                  "prompt_tokens": 9,
                  "completion_tokens": 12,
                  "total_tokens": 21,
                  "completion_tokens_details": {
                    "reasoning_tokens": 0,
                    "accepted_prediction_tokens": 0,
                    "rejected_prediction_tokens": 0
                  }
                }
              }
          - title: Image input
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "gpt-4o",
                    "messages": [
                      {
                        "role": "user",
                        "content": [
                          {
                            "type": "text",
                            "text": "What'\''s in this image?"
                          },
                          {
                            "type": "image_url",
                            "image_url": {
                              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                            }
                          }
                        ]
                      }
                    ],
                    "max_tokens": 300
                  }'
              python: |
                from openai import OpenAI

                client = OpenAI()

                response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": "What's in this image?"},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                                    }
                                },
                            ],
                        }
                    ],
                    max_tokens=300,
                )

                print(response.choices[0])
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const response = await openai.chat.completions.create({
                    model: "gpt-4o",
                    messages: [
                      {
                        role: "user",
                        content: [
                          { type: "text", text: "What's in this image?" },
                          {
                            type: "image_url",
                            image_url: {
                              "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                            },
                          }
                        ],
                      },
                    ],
                    store: true,
                  });
                  console.log(response.choices[0]);
                }
                main();
            response: |
              {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1677652288,
                "model": "gpt-4o-mini",
                "system_fingerprint": "fp_44709d6fcb",
                "choices": [{
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": "\n\nThis image shows a wooden boardwalk extending through a lush green marshland.",
                  },
                  "logprobs": null,
                  "finish_reason": "stop"
                }],
                "usage": {
                  "prompt_tokens": 9,
                  "completion_tokens": 12,
                  "total_tokens": 21,
                  "completion_tokens_details": {
                    "reasoning_tokens": 0,
                    "accepted_prediction_tokens": 0,
                    "rejected_prediction_tokens": 0
                  }
                }
              }
          - title: Streaming
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_chat_model_id",
                    "messages": [
                      {
                        "role": "developer",
                        "content": "You are a helpful assistant."
                      },
                      {
                        "role": "user",
                        "content": "Hello!"
                      }
                    ],
                    "stream": true
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                completion = client.chat.completions.create(
                  model="VAR_chat_model_id",
                  messages=[
                    {"role": "developer", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ],
                  stream=True
                )

                for chunk in completion:
                  print(chunk.choices[0].delta)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    model: "VAR_chat_model_id",
                    messages: [
                      {"role": "developer", "content": "You are a helpful assistant."},
                      {"role": "user", "content": "Hello!"}
                    ],
                    store: true,
                    stream: true,
                  });

                  for await (const chunk of completion) {
                    console.log(chunk.choices[0].delta.content);
                  }
                }

                main();
            response: >
              {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
              "system_fingerprint": "fp_44709d6fcb",
              "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}


              {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
              "system_fingerprint": "fp_44709d6fcb",
              "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}


              ....


              {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
              "system_fingerprint": "fp_44709d6fcb",
              "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}
          - title: Functions
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "gpt-4o",
                  "messages": [
                    {
                      "role": "user",
                      "content": "What'\''s the weather like in Boston today?"
                    }
                  ],
                  "tools": [
                    {
                      "type": "function",
                      "function": {
                        "name": "get_current_weather",
                        "description": "Get the current weather in a given location",
                        "parameters": {
                          "type": "object",
                          "properties": {
                            "location": {
                              "type": "string",
                              "description": "The city and state, e.g. San Francisco, CA"
                            },
                            "unit": {
                              "type": "string",
                              "enum": ["celsius", "fahrenheit"]
                            }
                          },
                          "required": ["location"]
                        }
                      }
                    }
                  ],
                  "tool_choice": "auto"
                }'
              python: >
                from openai import OpenAI

                client = OpenAI()


                tools = [
                  {
                    "type": "function",
                    "function": {
                      "name": "get_current_weather",
                      "description": "Get the current weather in a given location",
                      "parameters": {
                        "type": "object",
                        "properties": {
                          "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                          },
                          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                        },
                        "required": ["location"],
                      },
                    }
                  }
                ]

                messages = [{"role": "user", "content": "What's the weather like
                in Boston today?"}]

                completion = client.chat.completions.create(
                  model="VAR_chat_model_id",
                  messages=messages,
                  tools=tools,
                  tool_choice="auto"
                )


                print(completion)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const messages = [{"role": "user", "content": "What's the weather like in Boston today?"}];
                  const tools = [
                      {
                        "type": "function",
                        "function": {
                          "name": "get_current_weather",
                          "description": "Get the current weather in a given location",
                          "parameters": {
                            "type": "object",
                            "properties": {
                              "location": {
                                "type": "string",
                                "description": "The city and state, e.g. San Francisco, CA",
                              },
                              "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                            },
                            "required": ["location"],
                          },
                        }
                      }
                  ];

                  const response = await openai.chat.completions.create({
                    model: "gpt-4o",
                    messages: messages,
                    tools: tools,
                    tool_choice: "auto",
                    store: true,
                  });

                  console.log(response);
                }

                main();
            response: |
              {
                "id": "chatcmpl-abc123",
                "object": "chat.completion",
                "created": 1699896916,
                "model": "gpt-4o-mini",
                "choices": [
                  {
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": null,
                      "tool_calls": [
                        {
                          "id": "call_abc123",
                          "type": "function",
                          "function": {
                            "name": "get_current_weather",
                            "arguments": "{\n\"location\": \"Boston, MA\"\n}"
                          }
                        }
                      ]
                    },
                    "logprobs": null,
                    "finish_reason": "tool_calls"
                  }
                ],
                "usage": {
                  "prompt_tokens": 82,
                  "completion_tokens": 17,
                  "total_tokens": 99,
                  "completion_tokens_details": {
                    "reasoning_tokens": 0,
                    "accepted_prediction_tokens": 0,
                    "rejected_prediction_tokens": 0
                  }
                }
              }
          - title: Logprobs
            request:
              curl: |
                curl https://api.openai.com/v1/chat/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_chat_model_id",
                    "messages": [
                      {
                        "role": "user",
                        "content": "Hello!"
                      }
                    ],
                    "logprobs": true,
                    "top_logprobs": 2
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                completion = client.chat.completions.create(
                  model="VAR_chat_model_id",
                  messages=[
                    {"role": "user", "content": "Hello!"}
                  ],
                  logprobs=True,
                  top_logprobs=2
                )

                print(completion.choices[0].message)
                print(completion.choices[0].logprobs)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.chat.completions.create({
                    messages: [{ role: "user", content: "Hello!" }],
                    model: "VAR_chat_model_id",
                    store: true,
                    logprobs: true,
                    top_logprobs: 2,
                  });

                  console.log(completion.choices[0]);
                }

                main();
            response: |
              {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1702685778,
                "model": "gpt-4o-mini",
                "choices": [
                  {
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": "Hello! How can I assist you today?"
                    },
                    "logprobs": {
                      "content": [
                        {
                          "token": "Hello",
                          "logprob": -0.31725305,
                          "bytes": [72, 101, 108, 108, 111],
                          "top_logprobs": [
                            {
                              "token": "Hello",
                              "logprob": -0.31725305,
                              "bytes": [72, 101, 108, 108, 111]
                            },
                            {
                              "token": "Hi",
                              "logprob": -1.3190403,
                              "bytes": [72, 105]
                            }
                          ]
                        },
                        {
                          "token": "!",
                          "logprob": -0.02380986,
                          "bytes": [
                            33
                          ],
                          "top_logprobs": [
                            {
                              "token": "!",
                              "logprob": -0.02380986,
                              "bytes": [33]
                            },
                            {
                              "token": " there",
                              "logprob": -3.787621,
                              "bytes": [32, 116, 104, 101, 114, 101]
                            }
                          ]
                        },
                        {
                          "token": " How",
                          "logprob": -0.000054669687,
                          "bytes": [32, 72, 111, 119],
                          "top_logprobs": [
                            {
                              "token": " How",
                              "logprob": -0.000054669687,
                              "bytes": [32, 72, 111, 119]
                            },
                            {
                              "token": "<|end|>",
                              "logprob": -10.953937,
                              "bytes": null
                            }
                          ]
                        },
                        {
                          "token": " can",
                          "logprob": -0.015801601,
                          "bytes": [32, 99, 97, 110],
                          "top_logprobs": [
                            {
                              "token": " can",
                              "logprob": -0.015801601,
                              "bytes": [32, 99, 97, 110]
                            },
                            {
                              "token": " may",
                              "logprob": -4.161023,
                              "bytes": [32, 109, 97, 121]
                            }
                          ]
                        },
                        {
                          "token": " I",
                          "logprob": -3.7697225e-6,
                          "bytes": [
                            32,
                            73
                          ],
                          "top_logprobs": [
                            {
                              "token": " I",
                              "logprob": -3.7697225e-6,
                              "bytes": [32, 73]
                            },
                            {
                              "token": " assist",
                              "logprob": -13.596657,
                              "bytes": [32, 97, 115, 115, 105, 115, 116]
                            }
                          ]
                        },
                        {
                          "token": " assist",
                          "logprob": -0.04571125,
                          "bytes": [32, 97, 115, 115, 105, 115, 116],
                          "top_logprobs": [
                            {
                              "token": " assist",
                              "logprob": -0.04571125,
                              "bytes": [32, 97, 115, 115, 105, 115, 116]
                            },
                            {
                              "token": " help",
                              "logprob": -3.1089056,
                              "bytes": [32, 104, 101, 108, 112]
                            }
                          ]
                        },
                        {
                          "token": " you",
                          "logprob": -5.4385737e-6,
                          "bytes": [32, 121, 111, 117],
                          "top_logprobs": [
                            {
                              "token": " you",
                              "logprob": -5.4385737e-6,
                              "bytes": [32, 121, 111, 117]
                            },
                            {
                              "token": " today",
                              "logprob": -12.807695,
                              "bytes": [32, 116, 111, 100, 97, 121]
                            }
                          ]
                        },
                        {
                          "token": " today",
                          "logprob": -0.0040071653,
                          "bytes": [32, 116, 111, 100, 97, 121],
                          "top_logprobs": [
                            {
                              "token": " today",
                              "logprob": -0.0040071653,
                              "bytes": [32, 116, 111, 100, 97, 121]
                            },
                            {
                              "token": "?",
                              "logprob": -5.5247097,
                              "bytes": [63]
                            }
                          ]
                        },
                        {
                          "token": "?",
                          "logprob": -0.0008108172,
                          "bytes": [63],
                          "top_logprobs": [
                            {
                              "token": "?",
                              "logprob": -0.0008108172,
                              "bytes": [63]
                            },
                            {
                              "token": "?\n",
                              "logprob": -7.184561,
                              "bytes": [63, 10]
                            }
                          ]
                        }
                      ]
                    },
                    "finish_reason": "stop"
                  }
                ],
                "usage": {
                  "prompt_tokens": 9,
                  "completion_tokens": 9,
                  "total_tokens": 18,
                  "completion_tokens_details": {
                    "reasoning_tokens": 0,
                    "accepted_prediction_tokens": 0,
                    "rejected_prediction_tokens": 0
                  }
                },
                "system_fingerprint": null
              }
  /completions:
    post:
      operationId: createCompletion
      tags:
        - Completions
      summary: Creates a completion for the provided prompt and parameters.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateCompletionRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateCompletionResponse"
      x-oaiMeta:
        name: Create completion
        group: completions
        returns: >
          Returns a [completion](/docs/api-reference/completions/object) object,
          or a sequence of completion objects if the request is streamed.
        legacy: true
        examples:
          - title: No streaming
            request:
              curl: |
                curl https://api.openai.com/v1/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_completion_model_id",
                    "prompt": "Say this is a test",
                    "max_tokens": 7,
                    "temperature": 0
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                client.completions.create(
                  model="VAR_completion_model_id",
                  prompt="Say this is a test",
                  max_tokens=7,
                  temperature=0
                )
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const completion = await openai.completions.create({
                    model: "VAR_completion_model_id",
                    prompt: "Say this is a test.",
                    max_tokens: 7,
                    temperature: 0,
                  });

                  console.log(completion);
                }
                main();
            response: |
              {
                "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
                "object": "text_completion",
                "created": 1589478378,
                "model": "VAR_completion_model_id",
                "system_fingerprint": "fp_44709d6fcb",
                "choices": [
                  {
                    "text": "\n\nThis is indeed a test",
                    "index": 0,
                    "logprobs": null,
                    "finish_reason": "length"
                  }
                ],
                "usage": {
                  "prompt_tokens": 5,
                  "completion_tokens": 7,
                  "total_tokens": 12
                }
              }
          - title: Streaming
            request:
              curl: |
                curl https://api.openai.com/v1/completions \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "VAR_completion_model_id",
                    "prompt": "Say this is a test",
                    "max_tokens": 7,
                    "temperature": 0,
                    "stream": true
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                for chunk in client.completions.create(
                  model="VAR_completion_model_id",
                  prompt="Say this is a test",
                  max_tokens=7,
                  temperature=0,
                  stream=True
                ):
                  print(chunk.choices[0].text)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const stream = await openai.completions.create({
                    model: "VAR_completion_model_id",
                    prompt: "Say this is a test.",
                    stream: true,
                  });

                  for await (const chunk of stream) {
                    console.log(chunk.choices[0].text)
                  }
                }
                main();
            response: |
              {
                "id": "cmpl-7iA7iJjj8V2zOkCGvWF2hAkDWBQZe",
                "object": "text_completion",
                "created": 1690759702,
                "choices": [
                  {
                    "text": "This",
                    "index": 0,
                    "logprobs": null,
                    "finish_reason": null
                  }
                ],
                "model": "gpt-3.5-turbo-instruct"
                "system_fingerprint": "fp_44709d6fcb",
              }
  /embeddings:
    post:
      operationId: createEmbedding
      tags:
        - Embeddings
      summary: Creates an embedding vector representing the input text.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateEmbeddingRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateEmbeddingResponse"
      x-oaiMeta:
        name: Create embeddings
        group: embeddings
        returns: A list of [embedding](/docs/api-reference/embeddings/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/embeddings \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                  "input": "The food was delicious and the waiter...",
                  "model": "text-embedding-ada-002",
                  "encoding_format": "float"
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.embeddings.create(
                model="text-embedding-ada-002",
                input="The food was delicious and the waiter...",
                encoding_format="float"
              )
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const embedding = await openai.embeddings.create({
                  model: "text-embedding-ada-002",
                  input: "The quick brown fox jumped over the lazy dog",
                  encoding_format: "float",
                });

                console.log(embedding);
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "object": "embedding",
                  "embedding": [
                    0.0023064255,
                    -0.009327292,
                    .... (1536 floats total for ada-002)
                    -0.0028842222,
                  ],
                  "index": 0
                }
              ],
              "model": "text-embedding-ada-002",
              "usage": {
                "prompt_tokens": 8,
                "total_tokens": 8
              }
            }
  /files:
    get:
      operationId: listFiles
      tags:
        - Files
      summary: Returns a list of files.
      parameters:
        - in: query
          name: purpose
          required: false
          schema:
            type: string
          description: Only return files with the given purpose.
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 10,000, and the default is 10,000.
          required: false
          schema:
            type: integer
            default: 10000
        - name: order
          in: query
          description: >
            Sort order by the `created_at` timestamp of the objects. `asc` for
            ascending order and `desc` for descending order.
          schema:
            type: string
            default: desc
            enum:
              - asc
              - desc
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListFilesResponse"
      x-oaiMeta:
        name: List files
        group: files
        returns: A list of [File](/docs/api-reference/files/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/files \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.files.list()
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const list = await openai.files.list();

                for await (const file of list) {
                  console.log(file);
                }
              }

              main();
          response: |
            {
              "data": [
                {
                  "id": "file-abc123",
                  "object": "file",
                  "bytes": 175,
                  "created_at": 1613677385,
                  "filename": "salesOverview.pdf",
                  "purpose": "assistants",
                },
                {
                  "id": "file-abc123",
                  "object": "file",
                  "bytes": 140,
                  "created_at": 1613779121,
                  "filename": "puppy.jsonl",
                  "purpose": "fine-tune",
                }
              ],
              "object": "list"
            }
    post:
      operationId: createFile
      tags:
        - Files
      summary: >
        Upload a file that can be used across various endpoints. Individual
        files can be up to 512 MB, and the size of all files uploaded by one
        organization can be up to 100 GB.


        The Assistants API supports files up to 2 million tokens and of specific
        file types. See the [Assistants Tools guide](/docs/assistants/tools) for
        details.


        The Fine-tuning API only supports `.jsonl` files. The input also has
        certain required formats for fine-tuning
        [chat](/docs/api-reference/fine-tuning/chat-input) or
        [completions](/docs/api-reference/fine-tuning/completions-input) models.


        The Batch API only supports `.jsonl` files up to 200 MB in size. The
        input also has a specific required
        [format](/docs/api-reference/batch/request-input).


        Please [contact us](https://help.openai.com/) if you need to increase
        these storage limits.
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: "#/components/schemas/CreateFileRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/OpenAIFile"
      x-oaiMeta:
        name: Upload file
        group: files
        returns: The uploaded [File](/docs/api-reference/files/object) object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/files \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -F purpose="fine-tune" \
                -F file="@mydata.jsonl"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.files.create(
                file=open("mydata.jsonl", "rb"),
                purpose="fine-tune"
              )
            node.js: |-
              import fs from "fs";
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const file = await openai.files.create({
                  file: fs.createReadStream("mydata.jsonl"),
                  purpose: "fine-tune",
                });

                console.log(file);
              }

              main();
          response: |
            {
              "id": "file-abc123",
              "object": "file",
              "bytes": 120000,
              "created_at": 1677610602,
              "filename": "mydata.jsonl",
              "purpose": "fine-tune",
            }
  /files/{file_id}:
    delete:
      operationId: deleteFile
      tags:
        - Files
      summary: Delete a file.
      parameters:
        - in: path
          name: file_id
          required: true
          schema:
            type: string
          description: The ID of the file to use for this request.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteFileResponse"
      x-oaiMeta:
        name: Delete file
        group: files
        returns: Deletion status.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/files/file-abc123 \
                -X DELETE \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.files.delete("file-abc123")
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const file = await openai.files.del("file-abc123");

                console.log(file);
              }

              main();
          response: |
            {
              "id": "file-abc123",
              "object": "file",
              "deleted": true
            }
    get:
      operationId: retrieveFile
      tags:
        - Files
      summary: Returns information about a specific file.
      parameters:
        - in: path
          name: file_id
          required: true
          schema:
            type: string
          description: The ID of the file to use for this request.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/OpenAIFile"
      x-oaiMeta:
        name: Retrieve file
        group: files
        returns: >-
          The [File](/docs/api-reference/files/object) object matching the
          specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/files/file-abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.files.retrieve("file-abc123")
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const file = await openai.files.retrieve("file-abc123");

                console.log(file);
              }

              main();
          response: |
            {
              "id": "file-abc123",
              "object": "file",
              "bytes": 120000,
              "created_at": 1677610602,
              "filename": "mydata.jsonl",
              "purpose": "fine-tune",
            }
  /files/{file_id}/content:
    get:
      operationId: downloadFile
      tags:
        - Files
      summary: Returns the contents of the specified file.
      parameters:
        - in: path
          name: file_id
          required: true
          schema:
            type: string
          description: The ID of the file to use for this request.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                type: string
      x-oaiMeta:
        name: Retrieve file content
        group: files
        returns: The file content.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/files/file-abc123/content \
                -H "Authorization: Bearer $OPENAI_API_KEY" > file.jsonl
            python: |
              from openai import OpenAI
              client = OpenAI()

              content = client.files.content("file-abc123")
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const file = await openai.files.content("file-abc123");

                console.log(file);
              }

              main();
  /fine_tuning/jobs:
    post:
      operationId: createFineTuningJob
      tags:
        - Fine-tuning
      summary: >
        Creates a fine-tuning job which begins the process of creating a new
        model from a given dataset.


        Response includes details of the enqueued job including job status and
        the name of the fine-tuned models once complete.


        [Learn more about fine-tuning](/docs/guides/fine-tuning)
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateFineTuningJobRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/FineTuningJob"
      x-oaiMeta:
        name: Create fine-tuning job
        group: fine-tuning
        returns: A [fine-tuning.job](/docs/api-reference/fine-tuning/object) object.
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/fine_tuning/jobs \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "training_file": "file-BK7bzQj3FfZFXr7DbL6xJwfo",
                    "model": "gpt-4o-mini"
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                client.fine_tuning.jobs.create(
                  training_file="file-abc123",
                  model="gpt-4o-mini"
                )
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const fineTune = await openai.fineTuning.jobs.create({
                    training_file: "file-abc123"
                  });

                  console.log(fineTune);
                }

                main();
            response: |
              {
                "object": "fine_tuning.job",
                "id": "ftjob-abc123",
                "model": "gpt-4o-mini-2024-07-18",
                "created_at": 1721764800,
                "fine_tuned_model": null,
                "organization_id": "org-123",
                "result_files": [],
                "status": "queued",
                "validation_file": null,
                "training_file": "file-abc123",
                "method": {
                  "type": "supervised",
                  "supervised": {
                    "hyperparameters": {
                      "batch_size": "auto",
                      "learning_rate_multiplier": "auto",
                      "n_epochs": "auto",
                    }
                  }
                }
              }
          - title: Epochs
            request:
              curl: |
                curl https://api.openai.com/v1/fine_tuning/jobs \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "training_file": "file-abc123",
                    "model": "gpt-4o-mini",
                    "method": {
                      "type": "supervised",
                      "supervised": {
                        "hyperparameters": {
                          "n_epochs": 2
                        }
                      }
                    }
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                client.fine_tuning.jobs.create(
                  training_file="file-abc123",
                  model="gpt-4o-mini",
                  method={
                    "type": "supervised",
                    "supervised": {
                      "hyperparameters": {
                        "n_epochs": 2
                      }
                    }
                  }
                )
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const fineTune = await openai.fineTuning.jobs.create({
                    training_file: "file-abc123",
                    model: "gpt-4o-mini",
                    method: {
                      type: "supervised",
                      supervised: {
                        hyperparameters: {
                          n_epochs: 2
                        }
                      }
                    }
                  });

                  console.log(fineTune);
                }

                main();
            response: |
              {
                "object": "fine_tuning.job",
                "id": "ftjob-abc123",
                "model": "gpt-4o-mini-2024-07-18",
                "created_at": 1721764800,
                "fine_tuned_model": null,
                "organization_id": "org-123",
                "result_files": [],
                "status": "queued",
                "validation_file": null,
                "training_file": "file-abc123",
                "hyperparameters": {"n_epochs": 2},
                "method": {
                  "type": "supervised",
                  "supervised": {
                    "hyperparameters": {
                      "batch_size": "auto",
                      "learning_rate_multiplier": "auto",
                      "n_epochs": 2,
                    }
                  }
                }
              }
          - title: Validation file
            request:
              curl: |
                curl https://api.openai.com/v1/fine_tuning/jobs \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "training_file": "file-abc123",
                    "validation_file": "file-abc123",
                    "model": "gpt-4o-mini"
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                client.fine_tuning.jobs.create(
                  training_file="file-abc123",
                  validation_file="file-def456",
                  model="gpt-4o-mini"
                )
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const fineTune = await openai.fineTuning.jobs.create({
                    training_file: "file-abc123",
                    validation_file: "file-abc123"
                  });

                  console.log(fineTune);
                }

                main();
            response: |
              {
                "object": "fine_tuning.job",
                "id": "ftjob-abc123",
                "model": "gpt-4o-mini-2024-07-18",
                "created_at": 1721764800,
                "fine_tuned_model": null,
                "organization_id": "org-123",
                "result_files": [],
                "status": "queued",
                "validation_file": "file-abc123",
                "training_file": "file-abc123",
                "method": {
                  "type": "supervised",
                  "supervised": {
                    "hyperparameters": {
                      "batch_size": "auto",
                      "learning_rate_multiplier": "auto",
                      "n_epochs": "auto",
                    }
                  }
                }
              }
          - title: DPO
            request:
              curl: |
                curl https://api.openai.com/v1/fine_tuning/jobs \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "training_file": "file-abc123",
                    "validation_file": "file-abc123",
                    "model": "gpt-4o-mini",
                    "method": {
                      "type": "dpo",
                      "dpo": {
                        "hyperparameters": {
                          "beta": 0.1,
                        }
                      }
                    }
                  }'
            response: |
              {
                "object": "fine_tuning.job",
                "id": "ftjob-abc123",
                "model": "gpt-4o-mini-2024-07-18",
                "created_at": 1721764800,
                "fine_tuned_model": null,
                "organization_id": "org-123",
                "result_files": [],
                "status": "queued",
                "validation_file": "file-abc123",
                "training_file": "file-abc123",
                "method": {
                  "type": "dpo",
                  "dpo": {
                    "hyperparameters": {
                      "beta": 0.1,
                      "batch_size": "auto",
                      "learning_rate_multiplier": "auto",
                      "n_epochs": "auto",
                    }
                  }
                }
              }
          - title: W&B Integration
            request:
              curl: |
                curl https://api.openai.com/v1/fine_tuning/jobs \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "training_file": "file-abc123",
                    "validation_file": "file-abc123",
                    "model": "gpt-4o-mini",
                    "integrations": [
                      {
                        "type": "wandb",
                        "wandb": {
                          "project": "my-wandb-project",
                          "name": "ft-run-display-name"
                          "tags": [
                            "first-experiment", "v2"
                          ]
                        }
                      }
                    ]
                  }'
            response: |
              {
                "object": "fine_tuning.job",
                "id": "ftjob-abc123",
                "model": "gpt-4o-mini-2024-07-18",
                "created_at": 1721764800,
                "fine_tuned_model": null,
                "organization_id": "org-123",
                "result_files": [],
                "status": "queued",
                "validation_file": "file-abc123",
                "training_file": "file-abc123",
                "integrations": [
                  {
                    "type": "wandb",
                    "wandb": {
                      "project": "my-wandb-project",
                      "entity": None,
                      "run_id": "ftjob-abc123"
                    }
                  }
                ],
                "method": {
                  "type": "supervised",
                  "supervised": {
                    "hyperparameters": {
                      "batch_size": "auto",
                      "learning_rate_multiplier": "auto",
                      "n_epochs": "auto",
                    }
                  }
                }
              }
    get:
      operationId: listPaginatedFineTuningJobs
      tags:
        - Fine-tuning
      summary: |
        List your organization's fine-tuning jobs
      parameters:
        - name: after
          in: query
          description: Identifier for the last job from the previous pagination request.
          required: false
          schema:
            type: string
        - name: limit
          in: query
          description: Number of fine-tuning jobs to retrieve.
          required: false
          schema:
            type: integer
            default: 20
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListPaginatedFineTuningJobsResponse"
      x-oaiMeta:
        name: List fine-tuning jobs
        group: fine-tuning
        returns: >-
          A list of paginated [fine-tuning
          job](/docs/api-reference/fine-tuning/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/fine_tuning/jobs?limit=2 \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.fine_tuning.jobs.list()
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const list = await openai.fineTuning.jobs.list();

                for await (const fineTune of list) {
                  console.log(fineTune);
                }
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "object": "fine_tuning.job",
                  "id": "ftjob-abc123",
                  "model": "gpt-4o-mini-2024-07-18",
                  "created_at": 1721764800,
                  "fine_tuned_model": null,
                  "organization_id": "org-123",
                  "result_files": [],
                  "status": "queued",
                  "validation_file": null,
                  "training_file": "file-abc123"
                },
                { ... },
                { ... }
              ], "has_more": true
            }
  /fine_tuning/jobs/{fine_tuning_job_id}:
    get:
      operationId: retrieveFineTuningJob
      tags:
        - Fine-tuning
      summary: |
        Get info about a fine-tuning job.

        [Learn more about fine-tuning](/docs/guides/fine-tuning)
      parameters:
        - in: path
          name: fine_tuning_job_id
          required: true
          schema:
            type: string
            example: ft-AF1WoRqd3aJAHsqc9NY7iL8F
          description: |
            The ID of the fine-tuning job.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/FineTuningJob"
      x-oaiMeta:
        name: Retrieve fine-tuning job
        group: fine-tuning
        returns: >-
          The [fine-tuning](/docs/api-reference/fine-tuning/object) object with
          the given ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/fine_tuning/jobs/ft-AF1WoRqd3aJAHsqc9NY7iL8F
              \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.fine_tuning.jobs.retrieve("ftjob-abc123")
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const fineTune = await openai.fineTuning.jobs.retrieve("ftjob-abc123");

                console.log(fineTune);
              }

              main();
          response: |
            {
              "object": "fine_tuning.job",
              "id": "ftjob-abc123",
              "model": "davinci-002",
              "created_at": 1692661014,
              "finished_at": 1692661190,
              "fine_tuned_model": "ft:davinci-002:my-org:custom_suffix:7q8mpxmy",
              "organization_id": "org-123",
              "result_files": [
                  "file-abc123"
              ],
              "status": "succeeded",
              "validation_file": null,
              "training_file": "file-abc123",
              "hyperparameters": {
                  "n_epochs": 4,
                  "batch_size": 1,
                  "learning_rate_multiplier": 1.0
              },
              "trained_tokens": 5768,
              "integrations": [],
              "seed": 0,
              "estimated_finish": 0,
              "method": {
                "type": "supervised",
                "supervised": {
                  "hyperparameters": {
                    "n_epochs": 4,
                    "batch_size": 1,
                    "learning_rate_multiplier": 1.0
                  }
                }
              }
            }
  /fine_tuning/jobs/{fine_tuning_job_id}/cancel:
    post:
      operationId: cancelFineTuningJob
      tags:
        - Fine-tuning
      summary: |
        Immediately cancel a fine-tune job.
      parameters:
        - in: path
          name: fine_tuning_job_id
          required: true
          schema:
            type: string
            example: ft-AF1WoRqd3aJAHsqc9NY7iL8F
          description: |
            The ID of the fine-tuning job to cancel.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/FineTuningJob"
      x-oaiMeta:
        name: Cancel fine-tuning
        group: fine-tuning
        returns: >-
          The cancelled [fine-tuning](/docs/api-reference/fine-tuning/object)
          object.
        examples:
          request:
            curl: >
              curl -X POST
              https://api.openai.com/v1/fine_tuning/jobs/ftjob-abc123/cancel \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.fine_tuning.jobs.cancel("ftjob-abc123")
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const fineTune = await openai.fineTuning.jobs.cancel("ftjob-abc123");

                console.log(fineTune);
              }
              main();
          response: |
            {
              "object": "fine_tuning.job",
              "id": "ftjob-abc123",
              "model": "gpt-4o-mini-2024-07-18",
              "created_at": 1721764800,
              "fine_tuned_model": null,
              "organization_id": "org-123",
              "result_files": [],
              "status": "cancelled",
              "validation_file": "file-abc123",
              "training_file": "file-abc123"
            }
  /fine_tuning/jobs/{fine_tuning_job_id}/checkpoints:
    get:
      operationId: listFineTuningJobCheckpoints
      tags:
        - Fine-tuning
      summary: |
        List checkpoints for a fine-tuning job.
      parameters:
        - in: path
          name: fine_tuning_job_id
          required: true
          schema:
            type: string
            example: ft-AF1WoRqd3aJAHsqc9NY7iL8F
          description: |
            The ID of the fine-tuning job to get checkpoints for.
        - name: after
          in: query
          description: >-
            Identifier for the last checkpoint ID from the previous pagination
            request.
          required: false
          schema:
            type: string
        - name: limit
          in: query
          description: Number of checkpoints to retrieve.
          required: false
          schema:
            type: integer
            default: 10
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListFineTuningJobCheckpointsResponse"
      x-oaiMeta:
        name: List fine-tuning checkpoints
        group: fine-tuning
        returns: >-
          A list of fine-tuning [checkpoint
          objects](/docs/api-reference/fine-tuning/checkpoint-object) for a
          fine-tuning job.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/fine_tuning/jobs/ftjob-abc123/checkpoints
              \
                -H "Authorization: Bearer $OPENAI_API_KEY"
          response: |
            {
              "object": "list"
              "data": [
                {
                  "object": "fine_tuning.job.checkpoint",
                  "id": "ftckpt_zc4Q7MP6XxulcVzj4MZdwsAB",
                  "created_at": 1721764867,
                  "fine_tuned_model_checkpoint": "ft:gpt-4o-mini-2024-07-18:my-org:custom-suffix:96olL566:ckpt-step-2000",
                  "metrics": {
                    "full_valid_loss": 0.134,
                    "full_valid_mean_token_accuracy": 0.874
                  },
                  "fine_tuning_job_id": "ftjob-abc123",
                  "step_number": 2000,
                },
                {
                  "object": "fine_tuning.job.checkpoint",
                  "id": "ftckpt_enQCFmOTGj3syEpYVhBRLTSy",
                  "created_at": 1721764800,
                  "fine_tuned_model_checkpoint": "ft:gpt-4o-mini-2024-07-18:my-org:custom-suffix:7q8mpxmy:ckpt-step-1000",
                  "metrics": {
                    "full_valid_loss": 0.167,
                    "full_valid_mean_token_accuracy": 0.781
                  },
                  "fine_tuning_job_id": "ftjob-abc123",
                  "step_number": 1000,
                },
              ],
              "first_id": "ftckpt_zc4Q7MP6XxulcVzj4MZdwsAB",
              "last_id": "ftckpt_enQCFmOTGj3syEpYVhBRLTSy",
              "has_more": true
            }
  /fine_tuning/jobs/{fine_tuning_job_id}/events:
    get:
      operationId: listFineTuningEvents
      tags:
        - Fine-tuning
      summary: |
        Get status updates for a fine-tuning job.
      parameters:
        - in: path
          name: fine_tuning_job_id
          required: true
          schema:
            type: string
            example: ft-AF1WoRqd3aJAHsqc9NY7iL8F
          description: |
            The ID of the fine-tuning job to get events for.
        - name: after
          in: query
          description: Identifier for the last event from the previous pagination request.
          required: false
          schema:
            type: string
        - name: limit
          in: query
          description: Number of events to retrieve.
          required: false
          schema:
            type: integer
            default: 20
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListFineTuningJobEventsResponse"
      x-oaiMeta:
        name: List fine-tuning events
        group: fine-tuning
        returns: A list of fine-tuning event objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/fine_tuning/jobs/ftjob-abc123/events \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.fine_tuning.jobs.list_events(
                fine_tuning_job_id="ftjob-abc123",
                limit=2
              )
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const list = await openai.fineTuning.list_events(id="ftjob-abc123", limit=2);

                for await (const fineTune of list) {
                  console.log(fineTune);
                }
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "object": "fine_tuning.job.event",
                  "id": "ft-event-ddTJfwuMVpfLXseO0Am0Gqjm",
                  "created_at": 1721764800,
                  "level": "info",
                  "message": "Fine tuning job successfully completed",
                  "data": null,
                  "type": "message"
                },
                {
                  "object": "fine_tuning.job.event",
                  "id": "ft-event-tyiGuB72evQncpH87xe505Sv",
                  "created_at": 1721764800,
                  "level": "info",
                  "message": "New fine-tuned model created: ft:gpt-4o-mini:openai::7p4lURel",
                  "data": null,
                  "type": "message"
                }
              ],
              "has_more": true
            }
  /images/edits:
    post:
      operationId: createImageEdit
      tags:
        - Images
      summary: >-
        Creates an edited or extended image given an original image and a
        prompt.
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: "#/components/schemas/CreateImageEditRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ImagesResponse"
      x-oaiMeta:
        name: Create image edit
        group: images
        returns: Returns a list of [image](/docs/api-reference/images/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/images/edits \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -F image="@otter.png" \
                -F mask="@mask.png" \
                -F prompt="A cute baby sea otter wearing a beret" \
                -F n=2 \
                -F size="1024x1024"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.images.edit(
                image=open("otter.png", "rb"),
                mask=open("mask.png", "rb"),
                prompt="A cute baby sea otter wearing a beret",
                n=2,
                size="1024x1024"
              )
            node.js: |-
              import fs from "fs";
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const image = await openai.images.edit({
                  image: fs.createReadStream("otter.png"),
                  mask: fs.createReadStream("mask.png"),
                  prompt: "A cute baby sea otter wearing a beret",
                });

                console.log(image.data);
              }
              main();
          response: |
            {
              "created": 1589478378,
              "data": [
                {
                  "url": "https://..."
                },
                {
                  "url": "https://..."
                }
              ]
            }
  /images/generations:
    post:
      operationId: createImage
      tags:
        - Images
      summary: Creates an image given a prompt.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateImageRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ImagesResponse"
      x-oaiMeta:
        name: Create image
        group: images
        returns: Returns a list of [image](/docs/api-reference/images/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/images/generations \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "dall-e-3",
                  "prompt": "A cute baby sea otter",
                  "n": 1,
                  "size": "1024x1024"
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.images.generate(
                model="dall-e-3",
                prompt="A cute baby sea otter",
                n=1,
                size="1024x1024"
              )
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const image = await openai.images.generate({ model: "dall-e-3", prompt: "A cute baby sea otter" });

                console.log(image.data);
              }
              main();
          response: |
            {
              "created": 1589478378,
              "data": [
                {
                  "url": "https://..."
                },
                {
                  "url": "https://..."
                }
              ]
            }
  /images/variations:
    post:
      operationId: createImageVariation
      tags:
        - Images
      summary: Creates a variation of a given image.
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: "#/components/schemas/CreateImageVariationRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ImagesResponse"
      x-oaiMeta:
        name: Create image variation
        group: images
        returns: Returns a list of [image](/docs/api-reference/images/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/images/variations \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -F image="@otter.png" \
                -F n=2 \
                -F size="1024x1024"
            python: |
              from openai import OpenAI
              client = OpenAI()

              response = client.images.create_variation(
                image=open("image_edit_original.png", "rb"),
                n=2,
                size="1024x1024"
              )
            node.js: |-
              import fs from "fs";
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const image = await openai.images.createVariation({
                  image: fs.createReadStream("otter.png"),
                });

                console.log(image.data);
              }
              main();
          response: |
            {
              "created": 1589478378,
              "data": [
                {
                  "url": "https://..."
                },
                {
                  "url": "https://..."
                }
              ]
            }
  /models:
    get:
      operationId: listModels
      tags:
        - Models
      summary: >-
        Lists the currently available models, and provides basic information
        about each one such as the owner and availability.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListModelsResponse"
      x-oaiMeta:
        name: List models
        group: models
        returns: A list of [model](/docs/api-reference/models/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/models \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.models.list()
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const list = await openai.models.list();

                for await (const model of list) {
                  console.log(model);
                }
              }
              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "model-id-0",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "organization-owner"
                },
                {
                  "id": "model-id-1",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "organization-owner",
                },
                {
                  "id": "model-id-2",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "openai"
                },
              ],
              "object": "list"
            }
  /models/{model}:
    get:
      operationId: retrieveModel
      tags:
        - Models
      summary: >-
        Retrieves a model instance, providing basic information about the model
        such as the owner and permissioning.
      parameters:
        - in: path
          name: model
          required: true
          schema:
            type: string
            example: gpt-4o-mini
          description: The ID of the model to use for this request
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Model"
      x-oaiMeta:
        name: Retrieve model
        group: models
        returns: >-
          The [model](/docs/api-reference/models/object) object matching the
          specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/models/VAR_chat_model_id \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.models.retrieve("VAR_chat_model_id")
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const model = await openai.models.retrieve("VAR_chat_model_id");

                console.log(model);
              }

              main();
          response: |
            {
              "id": "VAR_chat_model_id",
              "object": "model",
              "created": 1686935002,
              "owned_by": "openai"
            }
    delete:
      operationId: deleteModel
      tags:
        - Models
      summary: >-
        Delete a fine-tuned model. You must have the Owner role in your
        organization to delete a model.
      parameters:
        - in: path
          name: model
          required: true
          schema:
            type: string
            example: ft:gpt-4o-mini:acemeco:suffix:abc123
          description: The model to delete
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteModelResponse"
      x-oaiMeta:
        name: Delete a fine-tuned model
        group: models
        returns: Deletion status.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/models/ft:gpt-4o-mini:acemeco:suffix:abc123
              \
                -X DELETE \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.models.delete("ft:gpt-4o-mini:acemeco:suffix:abc123")
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const model = await openai.models.del("ft:gpt-4o-mini:acemeco:suffix:abc123");

                console.log(model);
              }
              main();
          response: |
            {
              "id": "ft:gpt-4o-mini:acemeco:suffix:abc123",
              "object": "model",
              "deleted": true
            }
  /moderations:
    post:
      operationId: createModeration
      tags:
        - Moderations
      summary: |
        Classifies if text and/or image inputs are potentially harmful. Learn
        more in the [moderation guide](/docs/guides/moderation).
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateModerationRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateModerationResponse"
      x-oaiMeta:
        name: Create moderation
        group: moderations
        returns: A [moderation](/docs/api-reference/moderations/object) object.
        examples:
          - title: Single string
            request:
              curl: |
                curl https://api.openai.com/v1/moderations \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "input": "I want to kill them."
                  }'
              python: >
                from openai import OpenAI

                client = OpenAI()


                moderation = client.moderations.create(input="I want to kill
                them.")

                print(moderation)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const moderation = await openai.moderations.create({ input: "I want to kill them." });

                  console.log(moderation);
                }
                main();
            response: |
              {
                "id": "modr-AB8CjOTu2jiq12hp1AQPfeqFWaORR",
                "model": "text-moderation-007",
                "results": [
                  {
                    "flagged": true,
                    "categories": {
                      "sexual": false,
                      "hate": false,
                      "harassment": true,
                      "self-harm": false,
                      "sexual/minors": false,
                      "hate/threatening": false,
                      "violence/graphic": false,
                      "self-harm/intent": false,
                      "self-harm/instructions": false,
                      "harassment/threatening": true,
                      "violence": true
                    },
                    "category_scores": {
                      "sexual": 0.000011726012417057063,
                      "hate": 0.22706663608551025,
                      "harassment": 0.5215635299682617,
                      "self-harm": 2.227119921371923e-6,
                      "sexual/minors": 7.107352217872176e-8,
                      "hate/threatening": 0.023547329008579254,
                      "violence/graphic": 0.00003391829886822961,
                      "self-harm/intent": 1.646940972932498e-6,
                      "self-harm/instructions": 1.1198755256458526e-9,
                      "harassment/threatening": 0.5694745779037476,
                      "violence": 0.9971134662628174
                    }
                  }
                ]
              }
          - title: Image and text
            request:
              curl: |
                curl https://api.openai.com/v1/moderations \
                  -X POST \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -d '{
                    "model": "omni-moderation-latest",
                    "input": [
                      { "type": "text", "text": "...text to classify goes here..." },
                      {
                        "type": "image_url",
                        "image_url": {
                          "url": "https://example.com/image.png"
                        }
                      }
                    ]
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                response = client.moderations.create(
                    model="omni-moderation-latest",
                    input=[
                        {"type": "text", "text": "...text to classify goes here..."},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": "https://example.com/image.png",
                                # can also use base64 encoded image URLs
                                # "url": "data:image/jpeg;base64,abcdefg..."
                            }
                        },
                    ],
                )

                print(response)
              node.js: |
                import OpenAI from "openai";
                const openai = new OpenAI();

                const moderation = await openai.moderations.create({
                    model: "omni-moderation-latest",
                    input: [
                        { type: "text", text: "...text to classify goes here..." },
                        {
                            type: "image_url",
                            image_url: {
                                url: "https://example.com/image.png"
                                // can also use base64 encoded image URLs
                                // url: "data:image/jpeg;base64,abcdefg..."
                            }
                        }
                    ],
                });

                console.log(moderation);
            response: |
              {
                "id": "modr-0d9740456c391e43c445bf0f010940c7",
                "model": "omni-moderation-latest",
                "results": [
                  {
                    "flagged": true,
                    "categories": {
                      "harassment": true,
                      "harassment/threatening": true,
                      "sexual": false,
                      "hate": false,
                      "hate/threatening": false,
                      "illicit": false,
                      "illicit/violent": false,
                      "self-harm/intent": false,
                      "self-harm/instructions": false,
                      "self-harm": false,
                      "sexual/minors": false,
                      "violence": true,
                      "violence/graphic": true
                    },
                    "category_scores": {
                      "harassment": 0.8189693396524255,
                      "harassment/threatening": 0.804985420696006,
                      "sexual": 1.573112165348997e-6,
                      "hate": 0.007562942636942845,
                      "hate/threatening": 0.004208854591835476,
                      "illicit": 0.030535955153511665,
                      "illicit/violent": 0.008925306722380033,
                      "self-harm/intent": 0.00023023930975076432,
                      "self-harm/instructions": 0.0002293869201073356,
                      "self-harm": 0.012598046106750154,
                      "sexual/minors": 2.212566909570261e-8,
                      "violence": 0.9999992735124786,
                      "violence/graphic": 0.843064871157054
                    },
                    "category_applied_input_types": {
                      "harassment": [
                        "text"
                      ],
                      "harassment/threatening": [
                        "text"
                      ],
                      "sexual": [
                        "text",
                        "image"
                      ],
                      "hate": [
                        "text"
                      ],
                      "hate/threatening": [
                        "text"
                      ],
                      "illicit": [
                        "text"
                      ],
                      "illicit/violent": [
                        "text"
                      ],
                      "self-harm/intent": [
                        "text",
                        "image"
                      ],
                      "self-harm/instructions": [
                        "text",
                        "image"
                      ],
                      "self-harm": [
                        "text",
                        "image"
                      ],
                      "sexual/minors": [
                        "text"
                      ],
                      "violence": [
                        "text",
                        "image"
                      ],
                      "violence/graphic": [
                        "text",
                        "image"
                      ]
                    }
                  }
                ]
              }
  /organization/admin_api_keys:
    get:
      summary: List organization API keys
      operationId: admin-api-keys-list
      description: Retrieve a paginated list of organization admin API keys.
      parameters:
        - in: query
          name: after
          required: false
          schema:
            type: string
            nullable: true
            description: >-
              Return keys with IDs that come after this ID in the pagination
              order.
        - in: query
          name: order
          required: false
          schema:
            type: string
            enum:
              - asc
              - desc
            default: asc
            description: Order results by creation time, ascending or descending.
        - in: query
          name: limit
          required: false
          schema:
            type: integer
            default: 20
            description: Maximum number of keys to return.
      responses:
        "200":
          description: A list of organization API keys.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ApiKeyList"
      x-oaiMeta:
        name: List admin API keys
        group: administration
        returns: A list of admin API key objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/admin_api_keys?after=key_abc&limit=20
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
              "object": "list",
              "data": [
                {
                  "object": "organization.admin_api_key",
                  "id": "key_abc",
                  "name": "Main Admin Key",
                  "redacted_value": "sk-admin...def",
                  "created_at": 1711471533,
                  "owner": {
                    "type": "service_account",
                    "object": "organization.service_account",
                    "id": "sa_456",
                    "name": "My Service Account",
                    "created_at": 1711471533,
                    "role": "member"
                  }
                }
              ],
              "first_id": "key_abc",
              "last_id": "key_abc",
              "has_more": false
            }
    post:
      summary: Create an organization admin API key
      operationId: admin-api-keys-create
      description: Create a new admin-level API key for the organization.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - name
              properties:
                name:
                  type: string
                  example: New Admin Key
      responses:
        "200":
          description: The newly created admin API key.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AdminApiKey"
      x-oaiMeta:
        name: Create admin API key
        group: administration
        returns: The created admin API key object.
        examples:
          request:
            curl: >
              curl -X POST https://api.openai.com/v1/organization/admin_api_keys
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "name": "New Admin Key"
                }'
          response: |
            {
              "object": "organization.admin_api_key",
              "id": "key_xyz",
              "name": "New Admin Key",
              "redacted_value": "sk-admin...xyz",
              "created_at": 1711471533,
              "owner": {
                "type": "user",
                "object": "organization.user",
                "id": "user_123",
                "name": "John Doe",
                "created_at": 1711471533,
                "role": "owner"
              },
              "value": "sk-admin-1234abcd"
            }
  /organization/admin_api_keys/{key_id}:
    get:
      summary: Retrieve a single organization API key
      operationId: admin-api-keys-get
      description: Get details for a specific organization API key by its ID.
      parameters:
        - in: path
          name: key_id
          required: true
          schema:
            type: string
            description: The ID of the API key.
      responses:
        "200":
          description: Details of the requested API key.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/AdminApiKey"
      x-oaiMeta:
        name: Retrieve admin API key
        group: administration
        returns: The requested admin API key object.
        examples:
          request:
            curl: >
              curl https://api.openai.com/v1/organization/admin_api_keys/key_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
              "object": "organization.admin_api_key",
              "id": "key_abc",
              "name": "Main Admin Key",
              "redacted_value": "sk-admin...xyz",
              "created_at": 1711471533,
              "owner": {
                "type": "user",
                "object": "organization.user",
                "id": "user_123",
                "name": "John Doe",
                "created_at": 1711471533,
                "role": "owner"
              }
            }
    delete:
      summary: Delete an organization admin API key
      operationId: admin-api-keys-delete
      description: Delete the specified admin API key.
      parameters:
        - in: path
          name: key_id
          required: true
          schema:
            type: string
            description: The ID of the API key to be deleted.
      responses:
        "200":
          description: Confirmation that the API key was deleted.
          content:
            application/json:
              schema:
                type: object
                properties:
                  id:
                    type: string
                    example: key_abc
                  object:
                    type: string
                    example: organization.admin_api_key.deleted
                  deleted:
                    type: boolean
                    example: true
      x-oaiMeta:
        name: Delete admin API key
        group: administration
        returns: A confirmation object indicating the key was deleted.
        examples:
          request:
            curl: >
              curl -X DELETE
              https://api.openai.com/v1/organization/admin_api_keys/key_abc \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
              "id": "key_abc",
              "object": "organization.admin_api_key.deleted",
              "deleted": true
            }
  /organization/audit_logs:
    get:
      summary: List user actions and configuration changes within this organization.
      operationId: list-audit-logs
      tags:
        - Audit Logs
      parameters:
        - name: effective_at
          in: query
          description: >-
            Return only events whose `effective_at` (Unix seconds) is in this
            range.
          required: false
          schema:
            type: object
            properties:
              gt:
                type: integer
                description: >-
                  Return only events whose `effective_at` (Unix seconds) is
                  greater than this value.
              gte:
                type: integer
                description: >-
                  Return only events whose `effective_at` (Unix seconds) is
                  greater than or equal to this value.
              lt:
                type: integer
                description: >-
                  Return only events whose `effective_at` (Unix seconds) is less
                  than this value.
              lte:
                type: integer
                description: >-
                  Return only events whose `effective_at` (Unix seconds) is less
                  than or equal to this value.
        - name: project_ids[]
          in: query
          description: Return only events for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: event_types[]
          in: query
          description: >-
            Return only events with a `type` in one of these values. For
            example, `project.created`. For all options, see the documentation
            for the [audit log object](/docs/api-reference/audit-logs/object).
          required: false
          schema:
            type: array
            items:
              $ref: "#/components/schemas/AuditLogEventType"
        - name: actor_ids[]
          in: query
          description: >-
            Return only events performed by these actors. Can be a user ID, a
            service account ID, or an api key tracking ID.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: actor_emails[]
          in: query
          description: Return only events performed by users with these emails.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: resource_ids[]
          in: query
          description: >-
            Return only events performed on these targets. For example, a
            project ID updated.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, starting with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          schema:
            type: string
      responses:
        "200":
          description: Audit logs listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListAuditLogsResponse"
      x-oaiMeta:
        name: List audit logs
        group: audit-logs
        returns: >-
          A list of paginated [Audit Log](/docs/api-reference/audit-logs/object)
          objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/organization/audit_logs \
              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
              -H "Content-Type: application/json"
          response: |
            {
                "object": "list",
                "data": [
                    {
                        "id": "audit_log-xxx_yyyymmdd",
                        "type": "project.archived",
                        "effective_at": 1722461446,
                        "actor": {
                            "type": "api_key",
                            "api_key": {
                                "type": "user",
                                "user": {
                                    "id": "user-xxx",
                                    "email": "user@example.com"
                                }
                            }
                        },
                        "project.archived": {
                            "id": "proj_abc"
                        },
                    },
                    {
                        "id": "audit_log-yyy__20240101",
                        "type": "api_key.updated",
                        "effective_at": 1720804190,
                        "actor": {
                            "type": "session",
                            "session": {
                                "user": {
                                    "id": "user-xxx",
                                    "email": "user@example.com"
                                },
                                "ip_address": "127.0.0.1",
                                "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                                "ja3": "a497151ce4338a12c4418c44d375173e",
                                "ja4": "q13d0313h3_55b375c5d22e_c7319ce65786",
                                "ip_address_details": {
                                  "country": "US",
                                  "city": "San Francisco",
                                  "region": "California",
                                  "region_code": "CA",
                                  "asn": "1234",
                                  "latitude": "37.77490",
                                  "longitude": "-122.41940"
                                }
                            }
                        },
                        "api_key.updated": {
                            "id": "key_xxxx",
                            "data": {
                                "scopes": ["resource_2.operation_2"]
                            }
                        },
                    }
                ],
                "first_id": "audit_log-xxx__20240101",
                "last_id": "audit_log_yyy__20240101",
                "has_more": true
            }
  /organization/costs:
    get:
      summary: Get costs details for the organization.
      operationId: usage-costs
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently only `1d` is
            supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1d
            default: 1d
        - name: project_ids
          in: query
          description: Return only costs for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: group_by
          in: query
          description: >-
            Group the costs by the specified fields. Support fields include
            `project_id`, `line_item` and any combination of them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
                - line_item
        - name: limit
          in: query
          description: >
            A limit on the number of buckets to be returned. Limit can range
            between 1 and 180, and the default is 7.
          required: false
          schema:
            type: integer
            default: 7
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Costs data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Costs
        group: usage-costs
        returns: >-
          A list of paginated, time bucketed
          [Costs](/docs/api-reference/usage/costs_object) objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/costs?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.costs.result",
                                "amount": {
                                    "value": 0.06,
                                    "currency": "usd"
                                },
                                "line_item": null,
                                "project_id": null
                            }
                        ]
                    }
                ],
                "has_more": false,
                "next_page": null
            }
  /organization/invites:
    get:
      summary: Returns a list of invites in the organization.
      operationId: list-invites
      tags:
        - Invites
      parameters:
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          required: false
          schema:
            type: string
      responses:
        "200":
          description: Invites listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InviteListResponse"
      x-oaiMeta:
        name: List invites
        group: administration
        returns: A list of [Invite](/docs/api-reference/invite/object) objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/invites?after=invite-abc&limit=20
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
              "object": "list",
              "data": [
                {
                  "object": "organization.invite",
                  "id": "invite-abc",
                  "email": "user@example.com",
                  "role": "owner",
                  "status": "accepted",
                  "invited_at": 1711471533,
                  "expires_at": 1711471533,
                  "accepted_at": 1711471533
                }
              ],
              "first_id": "invite-abc",
              "last_id": "invite-abc",
              "has_more": false
            }
    post:
      summary: >-
        Create an invite for a user to the organization. The invite must be
        accepted by the user before they have access to the organization.
      operationId: inviteUser
      tags:
        - Invites
      requestBody:
        description: The invite request payload.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/InviteRequest"
      responses:
        "200":
          description: User invited successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Invite"
      x-oaiMeta:
        name: Create invite
        group: administration
        returns: The created [Invite](/docs/api-reference/invite/object) object.
        examples:
          request:
            curl: |
              curl -X POST https://api.openai.com/v1/organization/invites \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "email": "anotheruser@example.com",
                    "role": "reader",
                    "projects": [
                      {
                        "id": "project-xyz",
                        "role": "member"
                      },
                      {
                        "id": "project-abc",
                        "role": "owner"
                      }
                    ]
                }'
          response: |
            {
              "object": "organization.invite",
              "id": "invite-def",
              "email": "anotheruser@example.com",
              "role": "reader",
              "status": "pending",
              "invited_at": 1711471533,
              "expires_at": 1711471533,
              "accepted_at": null,
              "projects": [
                {
                  "id": "project-xyz",
                  "role": "member"
                },
                {
                  "id": "project-abc",
                  "role": "owner"
                }
              ]
            }
  /organization/invites/{invite_id}:
    get:
      summary: Retrieves an invite.
      operationId: retrieve-invite
      tags:
        - Invites
      parameters:
        - in: path
          name: invite_id
          required: true
          schema:
            type: string
          description: The ID of the invite to retrieve.
      responses:
        "200":
          description: Invite retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Invite"
      x-oaiMeta:
        name: Retrieve invite
        group: administration
        returns: >-
          The [Invite](/docs/api-reference/invite/object) object matching the
          specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/organization/invites/invite-abc \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.invite",
                "id": "invite-abc",
                "email": "user@example.com",
                "role": "owner",
                "status": "accepted",
                "invited_at": 1711471533,
                "expires_at": 1711471533,
                "accepted_at": 1711471533
            }
    delete:
      summary: >-
        Delete an invite. If the invite has already been accepted, it cannot be
        deleted.
      operationId: delete-invite
      tags:
        - Invites
      parameters:
        - in: path
          name: invite_id
          required: true
          schema:
            type: string
          description: The ID of the invite to delete.
      responses:
        "200":
          description: Invite deleted successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/InviteDeleteResponse"
      x-oaiMeta:
        name: Delete invite
        group: administration
        returns: Confirmation that the invite has been deleted
        examples:
          request:
            curl: >
              curl -X DELETE
              https://api.openai.com/v1/organization/invites/invite-abc \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.invite.deleted",
                "id": "invite-abc",
                "deleted": true
            }
  /organization/projects:
    get:
      summary: Returns a list of projects.
      operationId: list-projects
      tags:
        - Projects
      parameters:
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          required: false
          schema:
            type: string
        - name: include_archived
          in: query
          schema:
            type: boolean
            default: false
          description: >-
            If `true` returns all projects including those that have been
            `archived`. Archived projects are not included by default.
      responses:
        "200":
          description: Projects listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectListResponse"
      x-oaiMeta:
        name: List projects
        group: administration
        returns: A list of [Project](/docs/api-reference/projects/object) objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/projects?after=proj_abc&limit=20&include_archived=false
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "list",
                "data": [
                    {
                        "id": "proj_abc",
                        "object": "organization.project",
                        "name": "Project example",
                        "created_at": 1711471533,
                        "archived_at": null,
                        "status": "active"
                    }
                ],
                "first_id": "proj-abc",
                "last_id": "proj-xyz",
                "has_more": false
            }
    post:
      summary: >-
        Create a new project in the organization. Projects can be created and
        archived, but cannot be deleted.
      operationId: create-project
      tags:
        - Projects
      requestBody:
        description: The project create request payload.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ProjectCreateRequest"
      responses:
        "200":
          description: Project created successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Project"
      x-oaiMeta:
        name: Create project
        group: administration
        returns: The created [Project](/docs/api-reference/projects/object) object.
        examples:
          request:
            curl: |
              curl -X POST https://api.openai.com/v1/organization/projects \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "name": "Project ABC"
                }'
          response: |
            {
                "id": "proj_abc",
                "object": "organization.project",
                "name": "Project ABC",
                "created_at": 1711471533,
                "archived_at": null,
                "status": "active"
            }
  /organization/projects/{project_id}:
    get:
      summary: Retrieves a project.
      operationId: retrieve-project
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: Project retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Project"
      x-oaiMeta:
        name: Retrieve project
        group: administration
        description: Retrieve a project.
        returns: >-
          The [Project](/docs/api-reference/projects/object) object matching the
          specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/organization/projects/proj_abc \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "id": "proj_abc",
                "object": "organization.project",
                "name": "Project example",
                "created_at": 1711471533,
                "archived_at": null,
                "status": "active"
            }
    post:
      summary: Modifies a project in the organization.
      operationId: modify-project
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
      requestBody:
        description: The project update request payload.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ProjectUpdateRequest"
      responses:
        "200":
          description: Project updated successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Project"
        "400":
          description: Error response when updating the default project.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: Modify project
        group: administration
        returns: The updated [Project](/docs/api-reference/projects/object) object.
        examples:
          request:
            curl: >
              curl -X POST
              https://api.openai.com/v1/organization/projects/proj_abc \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "name": "Project DEF"
                }'
  /organization/projects/{project_id}/api_keys:
    get:
      summary: Returns a list of API keys in the project.
      operationId: list-project-api-keys
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          required: false
          schema:
            type: string
      responses:
        "200":
          description: Project API keys listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectApiKeyListResponse"
      x-oaiMeta:
        name: List project API keys
        group: administration
        returns: >-
          A list of [ProjectApiKey](/docs/api-reference/project-api-keys/object)
          objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/projects/proj_abc/api_keys?after=key_abc&limit=20
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "list",
                "data": [
                    {
                        "object": "organization.project.api_key",
                        "redacted_value": "sk-abc...def",
                        "name": "My API Key",
                        "created_at": 1711471533,
                        "id": "key_abc",
                        "owner": {
                            "type": "user",
                            "user": {
                                "object": "organization.project.user",
                                "id": "user_abc",
                                "name": "First Last",
                                "email": "user@example.com",
                                "role": "owner",
                                "added_at": 1711471533
                            }
                        }
                    }
                ],
                "first_id": "key_abc",
                "last_id": "key_xyz",
                "has_more": false
            }
  /organization/projects/{project_id}/api_keys/{key_id}:
    get:
      summary: Retrieves an API key in the project.
      operationId: retrieve-project-api-key
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: key_id
          in: path
          description: The ID of the API key.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: Project API key retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectApiKey"
      x-oaiMeta:
        name: Retrieve project API key
        group: administration
        returns: >-
          The [ProjectApiKey](/docs/api-reference/project-api-keys/object)
          object matching the specified ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/projects/proj_abc/api_keys/key_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.project.api_key",
                "redacted_value": "sk-abc...def",
                "name": "My API Key",
                "created_at": 1711471533,
                "id": "key_abc",
                "owner": {
                    "type": "user",
                    "user": {
                        "object": "organization.project.user",
                        "id": "user_abc",
                        "name": "First Last",
                        "email": "user@example.com",
                        "role": "owner",
                        "added_at": 1711471533
                    }
                }
            }
    delete:
      summary: Deletes an API key from the project.
      operationId: delete-project-api-key
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: key_id
          in: path
          description: The ID of the API key.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: Project API key deleted successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectApiKeyDeleteResponse"
        "400":
          description: Error response for various conditions.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: Delete project API key
        group: administration
        returns: >-
          Confirmation of the key's deletion or an error if the key belonged to
          a service account
        examples:
          request:
            curl: >
              curl -X DELETE
              https://api.openai.com/v1/organization/projects/proj_abc/api_keys/key_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.project.api_key.deleted",
                "id": "key_abc",
                "deleted": true
            }
  /organization/projects/{project_id}/archive:
    post:
      summary: >-
        Archives a project in the organization. Archived projects cannot be used
        or updated.
      operationId: archive-project
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: Project archived successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Project"
      x-oaiMeta:
        name: Archive project
        group: administration
        returns: The archived [Project](/docs/api-reference/projects/object) object.
        examples:
          request:
            curl: >
              curl -X POST
              https://api.openai.com/v1/organization/projects/proj_abc/archive \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "id": "proj_abc",
                "object": "organization.project",
                "name": "Project DEF",
                "created_at": 1711471533,
                "archived_at": 1711471533,
                "status": "archived"
            }
  /organization/projects/{project_id}/rate_limits:
    get:
      summary: Returns the rate limits per model for a project.
      operationId: list-project-rate-limits
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: limit
          in: query
          description: |
            A limit on the number of objects to be returned. The default is 100.
          required: false
          schema:
            type: integer
            default: 100
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          required: false
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, beginning with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          required: false
          schema:
            type: string
      responses:
        "200":
          description: Project rate limits listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectRateLimitListResponse"
      x-oaiMeta:
        name: List project rate limits
        group: administration
        returns: >-
          A list of
          [ProjectRateLimit](/docs/api-reference/project-rate-limits/object)
          objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/projects/proj_abc/rate_limits?after=rl_xxx&limit=20
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "list",
                "data": [
                    {
                      "object": "project.rate_limit",
                      "id": "rl-ada",
                      "model": "ada",
                      "max_requests_per_1_minute": 600,
                      "max_tokens_per_1_minute": 150000,
                      "max_images_per_1_minute": 10
                    }
                ],
                "first_id": "rl-ada",
                "last_id": "rl-ada",
                "has_more": false
            }
          error_response: |
            {
                "code": 404,
                "message": "The project {project_id} was not found"
            }
  /organization/projects/{project_id}/rate_limits/{rate_limit_id}:
    post:
      summary: Updates a project rate limit.
      operationId: update-project-rate-limits
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: rate_limit_id
          in: path
          description: The ID of the rate limit.
          required: true
          schema:
            type: string
      requestBody:
        description: The project rate limit update request payload.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ProjectRateLimitUpdateRequest"
      responses:
        "200":
          description: Project rate limit updated successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectRateLimit"
        "400":
          description: Error response for various conditions.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: Modify project rate limit
        group: administration
        returns: >-
          The updated
          [ProjectRateLimit](/docs/api-reference/project-rate-limits/object)
          object.
        examples:
          request:
            curl: >
              curl -X POST
              https://api.openai.com/v1/organization/projects/proj_abc/rate_limits/rl_xxx
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "max_requests_per_1_minute": 500
                }'
          response: |
            {
                "object": "project.rate_limit",
                "id": "rl-ada",
                "model": "ada",
                "max_requests_per_1_minute": 600,
                "max_tokens_per_1_minute": 150000,
                "max_images_per_1_minute": 10
              }
          error_response: |
            {
                "code": 404,
                "message": "The project {project_id} was not found"
            }
  /organization/projects/{project_id}/service_accounts:
    get:
      summary: Returns a list of service accounts in the project.
      operationId: list-project-service-accounts
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          required: false
          schema:
            type: string
      responses:
        "200":
          description: Project service accounts listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectServiceAccountListResponse"
        "400":
          description: Error response when project is archived.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: List project service accounts
        group: administration
        returns: >-
          A list of
          [ProjectServiceAccount](/docs/api-reference/project-service-accounts/object)
          objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/projects/proj_abc/service_accounts?after=custom_id&limit=20
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "list",
                "data": [
                    {
                        "object": "organization.project.service_account",
                        "id": "svc_acct_abc",
                        "name": "Service Account",
                        "role": "owner",
                        "created_at": 1711471533
                    }
                ],
                "first_id": "svc_acct_abc",
                "last_id": "svc_acct_xyz",
                "has_more": false
            }
    post:
      summary: >-
        Creates a new service account in the project. This also returns an
        unredacted API key for the service account.
      operationId: create-project-service-account
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
      requestBody:
        description: The project service account create request payload.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ProjectServiceAccountCreateRequest"
      responses:
        "200":
          description: Project service account created successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectServiceAccountCreateResponse"
        "400":
          description: Error response when project is archived.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: Create project service account
        group: administration
        returns: >-
          The created
          [ProjectServiceAccount](/docs/api-reference/project-service-accounts/object)
          object.
        examples:
          request:
            curl: >
              curl -X POST
              https://api.openai.com/v1/organization/projects/proj_abc/service_accounts
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "name": "Production App"
                }'
          response: |
            {
                "object": "organization.project.service_account",
                "id": "svc_acct_abc",
                "name": "Production App",
                "role": "member",
                "created_at": 1711471533,
                "api_key": {
                    "object": "organization.project.service_account.api_key",
                    "value": "sk-abcdefghijklmnop123",
                    "name": "Secret Key",
                    "created_at": 1711471533,
                    "id": "key_abc"
                }
            }
  /organization/projects/{project_id}/service_accounts/{service_account_id}:
    get:
      summary: Retrieves a service account in the project.
      operationId: retrieve-project-service-account
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: service_account_id
          in: path
          description: The ID of the service account.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: Project service account retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectServiceAccount"
      x-oaiMeta:
        name: Retrieve project service account
        group: administration
        returns: >-
          The
          [ProjectServiceAccount](/docs/api-reference/project-service-accounts/object)
          object matching the specified ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/projects/proj_abc/service_accounts/svc_acct_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.project.service_account",
                "id": "svc_acct_abc",
                "name": "Service Account",
                "role": "owner",
                "created_at": 1711471533
            }
    delete:
      summary: Deletes a service account from the project.
      operationId: delete-project-service-account
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: service_account_id
          in: path
          description: The ID of the service account.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: Project service account deleted successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectServiceAccountDeleteResponse"
      x-oaiMeta:
        name: Delete project service account
        group: administration
        returns: >-
          Confirmation of service account being deleted, or an error in case of
          an archived project, which has no service accounts
        examples:
          request:
            curl: >
              curl -X DELETE
              https://api.openai.com/v1/organization/projects/proj_abc/service_accounts/svc_acct_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.project.service_account.deleted",
                "id": "svc_acct_abc",
                "deleted": true
            }
  /organization/projects/{project_id}/users:
    get:
      summary: Returns a list of users in the project.
      operationId: list-project-users
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          required: false
          schema:
            type: string
      responses:
        "200":
          description: Project users listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectUserListResponse"
        "400":
          description: Error response when project is archived.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: List project users
        group: administration
        returns: >-
          A list of [ProjectUser](/docs/api-reference/project-users/object)
          objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/projects/proj_abc/users?after=user_abc&limit=20
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "list",
                "data": [
                    {
                        "object": "organization.project.user",
                        "id": "user_abc",
                        "name": "First Last",
                        "email": "user@example.com",
                        "role": "owner",
                        "added_at": 1711471533
                    }
                ],
                "first_id": "user-abc",
                "last_id": "user-xyz",
                "has_more": false
            }
    post:
      summary: >-
        Adds a user to the project. Users must already be members of the
        organization to be added to a project.
      operationId: create-project-user
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
      tags:
        - Projects
      requestBody:
        description: The project user create request payload.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ProjectUserCreateRequest"
      responses:
        "200":
          description: User added to project successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectUser"
        "400":
          description: Error response for various conditions.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: Create project user
        group: administration
        returns: >-
          The created [ProjectUser](/docs/api-reference/project-users/object)
          object.
        examples:
          request:
            curl: >
              curl -X POST
              https://api.openai.com/v1/organization/projects/proj_abc/users \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "user_id": "user_abc",
                    "role": "member"
                }'
          response: |
            {
                "object": "organization.project.user",
                "id": "user_abc",
                "email": "user@example.com",
                "role": "owner",
                "added_at": 1711471533
            }
  /organization/projects/{project_id}/users/{user_id}:
    get:
      summary: Retrieves a user in the project.
      operationId: retrieve-project-user
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: user_id
          in: path
          description: The ID of the user.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: Project user retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectUser"
      x-oaiMeta:
        name: Retrieve project user
        group: administration
        returns: >-
          The [ProjectUser](/docs/api-reference/project-users/object) object
          matching the specified ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/projects/proj_abc/users/user_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.project.user",
                "id": "user_abc",
                "name": "First Last",
                "email": "user@example.com",
                "role": "owner",
                "added_at": 1711471533
            }
    post:
      summary: Modifies a user's role in the project.
      operationId: modify-project-user
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: user_id
          in: path
          description: The ID of the user.
          required: true
          schema:
            type: string
      requestBody:
        description: The project user update request payload.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ProjectUserUpdateRequest"
      responses:
        "200":
          description: Project user's role updated successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectUser"
        "400":
          description: Error response for various conditions.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: Modify project user
        group: administration
        returns: >-
          The updated [ProjectUser](/docs/api-reference/project-users/object)
          object.
        examples:
          request:
            curl: >
              curl -X POST
              https://api.openai.com/v1/organization/projects/proj_abc/users/user_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "role": "owner"
                }'
          response: |
            {
                "object": "organization.project.user",
                "id": "user_abc",
                "name": "First Last",
                "email": "user@example.com",
                "role": "owner",
                "added_at": 1711471533
            }
    delete:
      summary: Deletes a user from the project.
      operationId: delete-project-user
      tags:
        - Projects
      parameters:
        - name: project_id
          in: path
          description: The ID of the project.
          required: true
          schema:
            type: string
        - name: user_id
          in: path
          description: The ID of the user.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: Project user deleted successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ProjectUserDeleteResponse"
        "400":
          description: Error response for various conditions.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      x-oaiMeta:
        name: Delete project user
        group: administration
        returns: >-
          Confirmation that project has been deleted or an error in case of an
          archived project, which has no users
        examples:
          request:
            curl: >
              curl -X DELETE
              https://api.openai.com/v1/organization/projects/proj_abc/users/user_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.project.user.deleted",
                "id": "user_abc",
                "deleted": true
            }
  /organization/usage/audio_speeches:
    get:
      summary: Get audio speeches usage details for the organization.
      operationId: usage-audio-speeches
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently `1m`, `1h` and `1d`
            are supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1m
              - 1h
              - 1d
            default: 1d
        - name: project_ids
          in: query
          description: Return only usage for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: user_ids
          in: query
          description: Return only usage for these users.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: api_key_ids
          in: query
          description: Return only usage for these API keys.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: models
          in: query
          description: Return only usage for these models.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: group_by
          in: query
          description: >-
            Group the usage data by the specified fields. Support fields include
            `project_id`, `user_id`, `api_key_id`, `model` or any combination of
            them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
                - user_id
                - api_key_id
                - model
        - name: limit
          in: query
          description: |
            Specifies the number of buckets to return.
            - `bucket_width=1d`: default: 7, max: 31
            - `bucket_width=1h`: default: 24, max: 168
            - `bucket_width=1m`: default: 60, max: 1440
          required: false
          schema:
            type: integer
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Usage data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Audio speeches
        group: usage-audio-speeches
        returns: >-
          A list of paginated, time bucketed [Audio speeches
          usage](/docs/api-reference/usage/audio_speeches_object) objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/usage/audio_speeches?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.usage.audio_speeches.result",
                                "characters": 45,
                                "num_model_requests": 1,
                                "project_id": null,
                                "user_id": null,
                                "api_key_id": null,
                                "model": null
                            }
                        ]
                    }
                ],
                "has_more": false,
                "next_page": null
            }
  /organization/usage/audio_transcriptions:
    get:
      summary: Get audio transcriptions usage details for the organization.
      operationId: usage-audio-transcriptions
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently `1m`, `1h` and `1d`
            are supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1m
              - 1h
              - 1d
            default: 1d
        - name: project_ids
          in: query
          description: Return only usage for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: user_ids
          in: query
          description: Return only usage for these users.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: api_key_ids
          in: query
          description: Return only usage for these API keys.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: models
          in: query
          description: Return only usage for these models.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: group_by
          in: query
          description: >-
            Group the usage data by the specified fields. Support fields include
            `project_id`, `user_id`, `api_key_id`, `model` or any combination of
            them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
                - user_id
                - api_key_id
                - model
        - name: limit
          in: query
          description: |
            Specifies the number of buckets to return.
            - `bucket_width=1d`: default: 7, max: 31
            - `bucket_width=1h`: default: 24, max: 168
            - `bucket_width=1m`: default: 60, max: 1440
          required: false
          schema:
            type: integer
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Usage data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Audio transcriptions
        group: usage-audio-transcriptions
        returns: >-
          A list of paginated, time bucketed [Audio transcriptions
          usage](/docs/api-reference/usage/audio_transcriptions_object) objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/usage/audio_transcriptions?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.usage.audio_transcriptions.result",
                                "seconds": 20,
                                "num_model_requests": 1,
                                "project_id": null,
                                "user_id": null,
                                "api_key_id": null,
                                "model": null
                            }
                        ]
                    }
                ],
                "has_more": false,
                "next_page": null
            }
  /organization/usage/code_interpreter_sessions:
    get:
      summary: Get code interpreter sessions usage details for the organization.
      operationId: usage-code-interpreter-sessions
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently `1m`, `1h` and `1d`
            are supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1m
              - 1h
              - 1d
            default: 1d
        - name: project_ids
          in: query
          description: Return only usage for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: group_by
          in: query
          description: >-
            Group the usage data by the specified fields. Support fields include
            `project_id`.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
        - name: limit
          in: query
          description: |
            Specifies the number of buckets to return.
            - `bucket_width=1d`: default: 7, max: 31
            - `bucket_width=1h`: default: 24, max: 168
            - `bucket_width=1m`: default: 60, max: 1440
          required: false
          schema:
            type: integer
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Usage data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Code interpreter sessions
        group: usage-code-interpreter-sessions
        returns: >-
          A list of paginated, time bucketed [Code interpreter sessions
          usage](/docs/api-reference/usage/code_interpreter_sessions_object)
          objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/usage/code_interpreter_sessions?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.usage.code_interpreter_sessions.result",
                                "num_sessions": 1,
                                "project_id": null
                            }
                        ]
                    }
                ],
                "has_more": false,
                "next_page": null
            }
  /organization/usage/completions:
    get:
      summary: Get completions usage details for the organization.
      operationId: usage-completions
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently `1m`, `1h` and `1d`
            are supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1m
              - 1h
              - 1d
            default: 1d
        - name: project_ids
          in: query
          description: Return only usage for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: user_ids
          in: query
          description: Return only usage for these users.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: api_key_ids
          in: query
          description: Return only usage for these API keys.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: models
          in: query
          description: Return only usage for these models.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: batch
          in: query
          description: >
            If `true`, return batch jobs only. If `false`, return non-batch jobs
            only. By default, return both.
          required: false
          schema:
            type: boolean
        - name: group_by
          in: query
          description: >-
            Group the usage data by the specified fields. Support fields include
            `project_id`, `user_id`, `api_key_id`, `model`, `batch` or any
            combination of them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
                - user_id
                - api_key_id
                - model
                - batch
        - name: limit
          in: query
          description: |
            Specifies the number of buckets to return.
            - `bucket_width=1d`: default: 7, max: 31
            - `bucket_width=1h`: default: 24, max: 168
            - `bucket_width=1m`: default: 60, max: 1440
          required: false
          schema:
            type: integer
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Usage data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Completions
        group: usage-completions
        returns: >-
          A list of paginated, time bucketed [Completions
          usage](/docs/api-reference/usage/completions_object) objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/usage/completions?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.usage.completions.result",
                                "input_tokens": 1000,
                                "output_tokens": 500,
                                "input_cached_tokens": 800,
                                "input_audio_tokens": 0,
                                "output_audio_tokens": 0,
                                "num_model_requests": 5,
                                "project_id": null,
                                "user_id": null,
                                "api_key_id": null,
                                "model": null,
                                "batch": null
                            }
                        ]
                    }
                ],
                "has_more": true,
                "next_page": "page_AAAAAGdGxdEiJdKOAAAAAGcqsYA="
            }
  /organization/usage/embeddings:
    get:
      summary: Get embeddings usage details for the organization.
      operationId: usage-embeddings
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently `1m`, `1h` and `1d`
            are supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1m
              - 1h
              - 1d
            default: 1d
        - name: project_ids
          in: query
          description: Return only usage for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: user_ids
          in: query
          description: Return only usage for these users.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: api_key_ids
          in: query
          description: Return only usage for these API keys.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: models
          in: query
          description: Return only usage for these models.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: group_by
          in: query
          description: >-
            Group the usage data by the specified fields. Support fields include
            `project_id`, `user_id`, `api_key_id`, `model` or any combination of
            them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
                - user_id
                - api_key_id
                - model
        - name: limit
          in: query
          description: |
            Specifies the number of buckets to return.
            - `bucket_width=1d`: default: 7, max: 31
            - `bucket_width=1h`: default: 24, max: 168
            - `bucket_width=1m`: default: 60, max: 1440
          required: false
          schema:
            type: integer
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Usage data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Embeddings
        group: usage-embeddings
        returns: >-
          A list of paginated, time bucketed [Embeddings
          usage](/docs/api-reference/usage/embeddings_object) objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/usage/embeddings?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.usage.embeddings.result",
                                "input_tokens": 16,
                                "num_model_requests": 2,
                                "project_id": null,
                                "user_id": null,
                                "api_key_id": null,
                                "model": null
                            }
                        ]
                    }
                ],
                "has_more": false,
                "next_page": null
            }
  /organization/usage/images:
    get:
      summary: Get images usage details for the organization.
      operationId: usage-images
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently `1m`, `1h` and `1d`
            are supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1m
              - 1h
              - 1d
            default: 1d
        - name: sources
          in: query
          description: >-
            Return only usages for these sources. Possible values are
            `image.generation`, `image.edit`, `image.variation` or any
            combination of them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - image.generation
                - image.edit
                - image.variation
        - name: sizes
          in: query
          description: >-
            Return only usages for these image sizes. Possible values are
            `256x256`, `512x512`, `1024x1024`, `1792x1792`, `1024x1792` or any
            combination of them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - 256x256
                - 512x512
                - 1024x1024
                - 1792x1792
                - 1024x1792
        - name: project_ids
          in: query
          description: Return only usage for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: user_ids
          in: query
          description: Return only usage for these users.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: api_key_ids
          in: query
          description: Return only usage for these API keys.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: models
          in: query
          description: Return only usage for these models.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: group_by
          in: query
          description: >-
            Group the usage data by the specified fields. Support fields include
            `project_id`, `user_id`, `api_key_id`, `model`, `size`, `source` or
            any combination of them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
                - user_id
                - api_key_id
                - model
                - size
                - source
        - name: limit
          in: query
          description: |
            Specifies the number of buckets to return.
            - `bucket_width=1d`: default: 7, max: 31
            - `bucket_width=1h`: default: 24, max: 168
            - `bucket_width=1m`: default: 60, max: 1440
          required: false
          schema:
            type: integer
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Usage data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Images
        group: usage-images
        returns: >-
          A list of paginated, time bucketed [Images
          usage](/docs/api-reference/usage/images_object) objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/usage/images?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.usage.images.result",
                                "images": 2,
                                "num_model_requests": 2,
                                "size": null,
                                "source": null,
                                "project_id": null,
                                "user_id": null,
                                "api_key_id": null,
                                "model": null
                            }
                        ]
                    }
                ],
                "has_more": false,
                "next_page": null
            }
  /organization/usage/moderations:
    get:
      summary: Get moderations usage details for the organization.
      operationId: usage-moderations
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently `1m`, `1h` and `1d`
            are supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1m
              - 1h
              - 1d
            default: 1d
        - name: project_ids
          in: query
          description: Return only usage for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: user_ids
          in: query
          description: Return only usage for these users.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: api_key_ids
          in: query
          description: Return only usage for these API keys.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: models
          in: query
          description: Return only usage for these models.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: group_by
          in: query
          description: >-
            Group the usage data by the specified fields. Support fields include
            `project_id`, `user_id`, `api_key_id`, `model` or any combination of
            them.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
                - user_id
                - api_key_id
                - model
        - name: limit
          in: query
          description: |
            Specifies the number of buckets to return.
            - `bucket_width=1d`: default: 7, max: 31
            - `bucket_width=1h`: default: 24, max: 168
            - `bucket_width=1m`: default: 60, max: 1440
          required: false
          schema:
            type: integer
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Usage data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Moderations
        group: usage-moderations
        returns: >-
          A list of paginated, time bucketed [Moderations
          usage](/docs/api-reference/usage/moderations_object) objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/usage/moderations?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.usage.moderations.result",
                                "input_tokens": 16,
                                "num_model_requests": 2,
                                "project_id": null,
                                "user_id": null,
                                "api_key_id": null,
                                "model": null
                            }
                        ]
                    }
                ],
                "has_more": false,
                "next_page": null
            }
  /organization/usage/vector_stores:
    get:
      summary: Get vector stores usage details for the organization.
      operationId: usage-vector-stores
      tags:
        - Usage
      parameters:
        - name: start_time
          in: query
          description: Start time (Unix seconds) of the query time range, inclusive.
          required: true
          schema:
            type: integer
        - name: end_time
          in: query
          description: End time (Unix seconds) of the query time range, exclusive.
          required: false
          schema:
            type: integer
        - name: bucket_width
          in: query
          description: >-
            Width of each time bucket in response. Currently `1m`, `1h` and `1d`
            are supported, default to `1d`.
          required: false
          schema:
            type: string
            enum:
              - 1m
              - 1h
              - 1d
            default: 1d
        - name: project_ids
          in: query
          description: Return only usage for these projects.
          required: false
          schema:
            type: array
            items:
              type: string
        - name: group_by
          in: query
          description: >-
            Group the usage data by the specified fields. Support fields include
            `project_id`.
          required: false
          schema:
            type: array
            items:
              type: string
              enum:
                - project_id
        - name: limit
          in: query
          description: |
            Specifies the number of buckets to return.
            - `bucket_width=1d`: default: 7, max: 31
            - `bucket_width=1h`: default: 24, max: 168
            - `bucket_width=1m`: default: 60, max: 1440
          required: false
          schema:
            type: integer
        - name: page
          in: query
          description: >-
            A cursor for use in pagination. Corresponding to the `next_page`
            field from the previous response.
          schema:
            type: string
      responses:
        "200":
          description: Usage data retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UsageResponse"
      x-oaiMeta:
        name: Vector stores
        group: usage-vector-stores
        returns: >-
          A list of paginated, time bucketed [Vector stores
          usage](/docs/api-reference/usage/vector_stores_object) objects.
        examples:
          request:
            curl: >
              curl
              "https://api.openai.com/v1/organization/usage/vector_stores?start_time=1730419200&limit=1"
              \

              -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \

              -H "Content-Type: application/json"
          response: |
            {
                "object": "page",
                "data": [
                    {
                        "object": "bucket",
                        "start_time": 1730419200,
                        "end_time": 1730505600,
                        "results": [
                            {
                                "object": "organization.usage.vector_stores.result",
                                "usage_bytes": 1024,
                                "project_id": null
                            }
                        ]
                    }
                ],
                "has_more": false,
                "next_page": null
            }
  /organization/users:
    get:
      summary: Lists all of the users in the organization.
      operationId: list-users
      tags:
        - Users
      parameters:
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          required: false
          schema:
            type: string
        - name: emails
          in: query
          description: Filter by the email address of users.
          required: false
          schema:
            type: array
            items:
              type: string
      responses:
        "200":
          description: Users listed successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UserListResponse"
      x-oaiMeta:
        name: List users
        group: administration
        returns: A list of [User](/docs/api-reference/users/object) objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/organization/users?after=user_abc&limit=20
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "list",
                "data": [
                    {
                        "object": "organization.user",
                        "id": "user_abc",
                        "name": "First Last",
                        "email": "user@example.com",
                        "role": "owner",
                        "added_at": 1711471533
                    }
                ],
                "first_id": "user-abc",
                "last_id": "user-xyz",
                "has_more": false
            }
  /organization/users/{user_id}:
    get:
      summary: Retrieves a user by their identifier.
      operationId: retrieve-user
      tags:
        - Users
      parameters:
        - name: user_id
          in: path
          description: The ID of the user.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: User retrieved successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/User"
      x-oaiMeta:
        name: Retrieve user
        group: administration
        returns: >-
          The [User](/docs/api-reference/users/object) object matching the
          specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/organization/users/user_abc \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.user",
                "id": "user_abc",
                "name": "First Last",
                "email": "user@example.com",
                "role": "owner",
                "added_at": 1711471533
            }
    post:
      summary: Modifies a user's role in the organization.
      operationId: modify-user
      tags:
        - Users
      parameters:
        - name: user_id
          in: path
          description: The ID of the user.
          required: true
          schema:
            type: string
      requestBody:
        description: The new user role to modify. This must be one of `owner` or `member`.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/UserRoleUpdateRequest"
      responses:
        "200":
          description: User role updated successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/User"
      x-oaiMeta:
        name: Modify user
        group: administration
        returns: The updated [User](/docs/api-reference/users/object) object.
        examples:
          request:
            curl: >
              curl -X POST https://api.openai.com/v1/organization/users/user_abc
              \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                    "role": "owner"
                }'
          response: |
            {
                "object": "organization.user",
                "id": "user_abc",
                "name": "First Last",
                "email": "user@example.com",
                "role": "owner",
                "added_at": 1711471533
            }
    delete:
      summary: Deletes a user from the organization.
      operationId: delete-user
      tags:
        - Users
      parameters:
        - name: user_id
          in: path
          description: The ID of the user.
          required: true
          schema:
            type: string
      responses:
        "200":
          description: User deleted successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UserDeleteResponse"
      x-oaiMeta:
        name: Delete user
        group: administration
        returns: Confirmation of the deleted user
        examples:
          request:
            curl: >
              curl -X DELETE
              https://api.openai.com/v1/organization/users/user_abc \
                -H "Authorization: Bearer $OPENAI_ADMIN_KEY" \
                -H "Content-Type: application/json"
          response: |
            {
                "object": "organization.user.deleted",
                "id": "user_abc",
                "deleted": true
            }
  /realtime/sessions:
    post:
      summary: >
        Create an ephemeral API token for use in client-side applications with
        the

        Realtime API. Can be configured with the same session parameters as the

        `session.update` client event.


        It responds with a session object, plus a `client_secret` key which
        contains

        a usable ephemeral API token that can be used to authenticate browser
        clients

        for the Realtime API.
      operationId: create-realtime-session
      tags:
        - Realtime
      requestBody:
        description: Create an ephemeral API key with the given session configuration.
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/RealtimeSessionCreateRequest"
      responses:
        "200":
          description: Session created successfully.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RealtimeSessionCreateResponse"
      x-oaiMeta:
        name: Create session
        group: realtime
        returns: The created Realtime session object, plus an ephemeral key
        examples:
          request:
            curl: |
              curl -X POST https://api.openai.com/v1/realtime/sessions \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                  "model": "gpt-4o-realtime-preview-2024-12-17",
                  "modalities": ["audio", "text"],
                  "instructions": "You are a friendly assistant."
                }'
          response: |
            {
              "id": "sess_001",
              "object": "realtime.session",
              "model": "gpt-4o-realtime-preview-2024-12-17",
              "modalities": ["audio", "text"],
              "instructions": "You are a friendly assistant.",
              "voice": "alloy",
              "input_audio_format": "pcm16",
              "output_audio_format": "pcm16",
              "input_audio_transcription": {
                  "model": "whisper-1"
              },
              "turn_detection": null,
              "tools": [],
              "tool_choice": "none",
              "temperature": 0.7,
              "max_response_output_tokens": 200,
              "client_secret": {
                "value": "ek_abc123", 
                "expires_at": 1234567890
              }
            }
  /threads:
    post:
      operationId: createThread
      tags:
        - Assistants
      summary: Create a thread.
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateThreadRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ThreadObject"
      x-oaiMeta:
        name: Create thread
        group: threads
        beta: true
        returns: A [thread](/docs/api-reference/threads) object.
        examples:
          - title: Empty
            request:
              curl: |
                curl https://api.openai.com/v1/threads \
                  -H "Content-Type: application/json" \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d ''
              python: |
                from openai import OpenAI
                client = OpenAI()

                empty_thread = client.beta.threads.create()
                print(empty_thread)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const emptyThread = await openai.beta.threads.create();

                  console.log(emptyThread);
                }

                main();
            response: |
              {
                "id": "thread_abc123",
                "object": "thread",
                "created_at": 1699012949,
                "metadata": {},
                "tool_resources": {}
              }
          - title: Messages
            request:
              curl: |
                curl https://api.openai.com/v1/threads \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2" \
                -d '{
                    "messages": [{
                      "role": "user",
                      "content": "Hello, what is AI?"
                    }, {
                      "role": "user",
                      "content": "How does AI work? Explain it in simple terms."
                    }]
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                message_thread = client.beta.threads.create(
                  messages=[
                    {
                      "role": "user",
                      "content": "Hello, what is AI?"
                    },
                    {
                      "role": "user",
                      "content": "How does AI work? Explain it in simple terms."
                    },
                  ]
                )

                print(message_thread)
              node.js: |-
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const messageThread = await openai.beta.threads.create({
                    messages: [
                      {
                        role: "user",
                        content: "Hello, what is AI?"
                      },
                      {
                        role: "user",
                        content: "How does AI work? Explain it in simple terms.",
                      },
                    ],
                  });

                  console.log(messageThread);
                }

                main();
            response: |
              {
                "id": "thread_abc123",
                "object": "thread",
                "created_at": 1699014083,
                "metadata": {},
                "tool_resources": {}
              }
  /threads/runs:
    post:
      operationId: createThreadAndRun
      tags:
        - Assistants
      summary: Create a thread and run it in one request.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateThreadAndRunRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RunObject"
      x-oaiMeta:
        name: Create thread and run
        group: threads
        beta: true
        returns: A [run](/docs/api-reference/runs/object) object.
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/threads/runs \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                      "assistant_id": "asst_abc123",
                      "thread": {
                        "messages": [
                          {"role": "user", "content": "Explain deep learning to a 5 year old."}
                        ]
                      }
                    }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                run = client.beta.threads.create_and_run(
                  assistant_id="asst_abc123",
                  thread={
                    "messages": [
                      {"role": "user", "content": "Explain deep learning to a 5 year old."}
                    ]
                  }
                )

                print(run)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const run = await openai.beta.threads.createAndRun({
                    assistant_id: "asst_abc123",
                    thread: {
                      messages: [
                        { role: "user", content: "Explain deep learning to a 5 year old." },
                      ],
                    },
                  });

                  console.log(run);
                }

                main();
            response: |
              {
                "id": "run_abc123",
                "object": "thread.run",
                "created_at": 1699076792,
                "assistant_id": "asst_abc123",
                "thread_id": "thread_abc123",
                "status": "queued",
                "started_at": null,
                "expires_at": 1699077392,
                "cancelled_at": null,
                "failed_at": null,
                "completed_at": null,
                "required_action": null,
                "last_error": null,
                "model": "gpt-4o",
                "instructions": "You are a helpful assistant.",
                "tools": [],
                "tool_resources": {},
                "metadata": {},
                "temperature": 1.0,
                "top_p": 1.0,
                "max_completion_tokens": null,
                "max_prompt_tokens": null,
                "truncation_strategy": {
                  "type": "auto",
                  "last_messages": null
                },
                "incomplete_details": null,
                "usage": null,
                "response_format": "auto",
                "tool_choice": "auto",
                "parallel_tool_calls": true
              }
          - title: Streaming
            request:
              curl: |
                curl https://api.openai.com/v1/threads/runs \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "assistant_id": "asst_123",
                    "thread": {
                      "messages": [
                        {"role": "user", "content": "Hello"}
                      ]
                    },
                    "stream": true
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                stream = client.beta.threads.create_and_run(
                  assistant_id="asst_123",
                  thread={
                    "messages": [
                      {"role": "user", "content": "Hello"}
                    ]
                  },
                  stream=True
                )

                for event in stream:
                  print(event)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const stream = await openai.beta.threads.createAndRun({
                      assistant_id: "asst_123",
                      thread: {
                        messages: [
                          { role: "user", content: "Hello" },
                        ],
                      },
                      stream: true
                  });

                  for await (const event of stream) {
                    console.log(event);
                  }
                }

                main();
            response: >
              event: thread.created

              data:
              {"id":"thread_123","object":"thread","created_at":1710348075,"metadata":{}}


              event: thread.run.created

              data:
              {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"tool_resources":{},"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}


              event: thread.run.queued

              data:
              {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"tool_resources":{},"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}


              event: thread.run.in_progress

              data:
              {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"tool_resources":{},"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}


              event: thread.run.step.created

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


              event: thread.run.step.in_progress

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


              event: thread.message.created

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],
              "metadata":{}}


              event: thread.message.in_progress

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],
              "metadata":{}}


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"Hello","annotations":[]}}]}}


              ...


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
              today"}}]}}


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"?"}}]}}


              event: thread.message.completed

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"completed","incomplete_details":null,"incomplete_at":null,"completed_at":1710348077,"role":"assistant","content":[{"type":"text","text":{"value":"Hello!
              How can I assist you today?","annotations":[]}}], "metadata":{}}


              event: thread.run.step.completed

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"completed","cancelled_at":null,"completed_at":1710348077,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31}}


              event: thread.run.completed

              {"id":"run_123","object":"thread.run","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","status":"completed","started_at":1713226836,"expires_at":null,"cancelled_at":null,"failed_at":null,"completed_at":1713226837,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":345,"completion_tokens":11,"total_tokens":356},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}


              event: done

              data: [DONE]
          - title: Streaming with Functions
            request:
              curl: |
                curl https://api.openai.com/v1/threads/runs \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "assistant_id": "asst_abc123",
                    "thread": {
                      "messages": [
                        {"role": "user", "content": "What is the weather like in San Francisco?"}
                      ]
                    },
                    "tools": [
                      {
                        "type": "function",
                        "function": {
                          "name": "get_current_weather",
                          "description": "Get the current weather in a given location",
                          "parameters": {
                            "type": "object",
                            "properties": {
                              "location": {
                                "type": "string",
                                "description": "The city and state, e.g. San Francisco, CA"
                              },
                              "unit": {
                                "type": "string",
                                "enum": ["celsius", "fahrenheit"]
                              }
                            },
                            "required": ["location"]
                          }
                        }
                      }
                    ],
                    "stream": true
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                tools = [
                  {
                    "type": "function",
                    "function": {
                      "name": "get_current_weather",
                      "description": "Get the current weather in a given location",
                      "parameters": {
                        "type": "object",
                        "properties": {
                          "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                          },
                          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                        },
                        "required": ["location"],
                      },
                    }
                  }
                ]

                stream = client.beta.threads.create_and_run(
                  thread={
                      "messages": [
                        {"role": "user", "content": "What is the weather like in San Francisco?"}
                      ]
                  },
                  assistant_id="asst_abc123",
                  tools=tools,
                  stream=True
                )

                for event in stream:
                  print(event)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                const tools = [
                    {
                      "type": "function",
                      "function": {
                        "name": "get_current_weather",
                        "description": "Get the current weather in a given location",
                        "parameters": {
                          "type": "object",
                          "properties": {
                            "location": {
                              "type": "string",
                              "description": "The city and state, e.g. San Francisco, CA",
                            },
                            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                          },
                          "required": ["location"],
                        },
                      }
                    }
                ];

                async function main() {
                  const stream = await openai.beta.threads.createAndRun({
                    assistant_id: "asst_123",
                    thread: {
                      messages: [
                        { role: "user", content: "What is the weather like in San Francisco?" },
                      ],
                    },
                    tools: tools,
                    stream: true
                  });

                  for await (const event of stream) {
                    console.log(event);
                  }
                }

                main();
            response: >
              event: thread.created

              data:
              {"id":"thread_123","object":"thread","created_at":1710351818,"metadata":{}}


              event: thread.run.created

              data:
              {"id":"run_123","object":"thread.run","created_at":1710351818,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710352418,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
              the current weather in a given
              location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
              city and state, e.g. San Francisco,
              CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.queued

              data:
              {"id":"run_123","object":"thread.run","created_at":1710351818,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710352418,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
              the current weather in a given
              location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
              city and state, e.g. San Francisco,
              CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.in_progress

              data:
              {"id":"run_123","object":"thread.run","created_at":1710351818,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":1710351818,"expires_at":1710352418,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
              the current weather in a given
              location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
              city and state, e.g. San Francisco,
              CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.step.created

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710351819,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"tool_calls","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710352418,"failed_at":null,"last_error":null,"step_details":{"type":"tool_calls","tool_calls":[]},"usage":null}


              event: thread.run.step.in_progress

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710351819,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"tool_calls","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710352418,"failed_at":null,"last_error":null,"step_details":{"type":"tool_calls","tool_calls":[]},"usage":null}


              event: thread.run.step.delta

              data:
              {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"id":"call_XXNp8YGaFrjrSjgqxtC8JJ1B","type":"function","function":{"name":"get_current_weather","arguments":"","output":null}}]}}}


              event: thread.run.step.delta

              data:
              {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"type":"function","function":{"arguments":"{\""}}]}}}


              event: thread.run.step.delta

              data:
              {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"type":"function","function":{"arguments":"location"}}]}}}


              ...


              event: thread.run.step.delta

              data:
              {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"type":"function","function":{"arguments":"ahrenheit"}}]}}}


              event: thread.run.step.delta

              data:
              {"id":"step_001","object":"thread.run.step.delta","delta":{"step_details":{"type":"tool_calls","tool_calls":[{"index":0,"type":"function","function":{"arguments":"\"}"}}]}}}


              event: thread.run.requires_action

              data:
              {"id":"run_123","object":"thread.run","created_at":1710351818,"assistant_id":"asst_123","thread_id":"thread_123","status":"requires_action","started_at":1710351818,"expires_at":1710352418,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":{"type":"submit_tool_outputs","submit_tool_outputs":{"tool_calls":[{"id":"call_XXNp8YGaFrjrSjgqxtC8JJ1B","type":"function","function":{"name":"get_current_weather","arguments":"{\"location\":\"San
              Francisco,
              CA\",\"unit\":\"fahrenheit\"}"}}]}},"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
              the current weather in a given
              location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
              city and state, e.g. San Francisco,
              CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":345,"completion_tokens":11,"total_tokens":356},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: done

              data: [DONE]
  /threads/{thread_id}:
    get:
      operationId: getThread
      tags:
        - Assistants
      summary: Retrieves a thread.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the thread to retrieve.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ThreadObject"
      x-oaiMeta:
        name: Retrieve thread
        group: threads
        beta: true
        returns: >-
          The [thread](/docs/api-reference/threads/object) object matching the
          specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/threads/thread_abc123 \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              my_thread = client.beta.threads.retrieve("thread_abc123")
              print(my_thread)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const myThread = await openai.beta.threads.retrieve(
                  "thread_abc123"
                );

                console.log(myThread);
              }

              main();
          response: |
            {
              "id": "thread_abc123",
              "object": "thread",
              "created_at": 1699014083,
              "metadata": {},
              "tool_resources": {
                "code_interpreter": {
                  "file_ids": []
                }
              }
            }
    post:
      operationId: modifyThread
      tags:
        - Assistants
      summary: Modifies a thread.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the thread to modify. Only the `metadata` can be modified.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ModifyThreadRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ThreadObject"
      x-oaiMeta:
        name: Modify thread
        group: threads
        beta: true
        returns: >-
          The modified [thread](/docs/api-reference/threads/object) object
          matching the specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/threads/thread_abc123 \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2" \
                -d '{
                    "metadata": {
                      "modified": "true",
                      "user": "abc123"
                    }
                  }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              my_updated_thread = client.beta.threads.update(
                "thread_abc123",
                metadata={
                  "modified": "true",
                  "user": "abc123"
                }
              )
              print(my_updated_thread)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const updatedThread = await openai.beta.threads.update(
                  "thread_abc123",
                  {
                    metadata: { modified: "true", user: "abc123" },
                  }
                );

                console.log(updatedThread);
              }

              main();
          response: |
            {
              "id": "thread_abc123",
              "object": "thread",
              "created_at": 1699014083,
              "metadata": {
                "modified": "true",
                "user": "abc123"
              },
              "tool_resources": {}
            }
    delete:
      operationId: deleteThread
      tags:
        - Assistants
      summary: Delete a thread.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the thread to delete.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteThreadResponse"
      x-oaiMeta:
        name: Delete thread
        group: threads
        beta: true
        returns: Deletion status
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/threads/thread_abc123 \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2" \
                -X DELETE
            python: |
              from openai import OpenAI
              client = OpenAI()

              response = client.beta.threads.delete("thread_abc123")
              print(response)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const response = await openai.beta.threads.del("thread_abc123");

                console.log(response);
              }
              main();
          response: |
            {
              "id": "thread_abc123",
              "object": "thread.deleted",
              "deleted": true
            }
  /threads/{thread_id}/messages:
    get:
      operationId: listMessages
      tags:
        - Assistants
      summary: Returns a list of messages for a given thread.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: >-
            The ID of the [thread](/docs/api-reference/threads) the messages
            belong to.
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: order
          in: query
          description: >
            Sort order by the `created_at` timestamp of the objects. `asc` for
            ascending order and `desc` for descending order.
          schema:
            type: string
            default: desc
            enum:
              - asc
              - desc
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, starting with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          schema:
            type: string
        - name: run_id
          in: query
          description: |
            Filter messages by the run ID that generated them.
          schema:
            type: string
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListMessagesResponse"
      x-oaiMeta:
        name: List messages
        group: threads
        beta: true
        returns: A list of [message](/docs/api-reference/messages) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/threads/thread_abc123/messages \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2"
            python: >
              from openai import OpenAI

              client = OpenAI()


              thread_messages =
              client.beta.threads.messages.list("thread_abc123")

              print(thread_messages.data)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const threadMessages = await openai.beta.threads.messages.list(
                  "thread_abc123"
                );

                console.log(threadMessages.data);
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "msg_abc123",
                  "object": "thread.message",
                  "created_at": 1699016383,
                  "assistant_id": null,
                  "thread_id": "thread_abc123",
                  "run_id": null,
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": {
                        "value": "How does AI work? Explain it in simple terms.",
                        "annotations": []
                      }
                    }
                  ],
                  "attachments": [],
                  "metadata": {}
                },
                {
                  "id": "msg_abc456",
                  "object": "thread.message",
                  "created_at": 1699016383,
                  "assistant_id": null,
                  "thread_id": "thread_abc123",
                  "run_id": null,
                  "role": "user",
                  "content": [
                    {
                      "type": "text",
                      "text": {
                        "value": "Hello, what is AI?",
                        "annotations": []
                      }
                    }
                  ],
                  "attachments": [],
                  "metadata": {}
                }
              ],
              "first_id": "msg_abc123",
              "last_id": "msg_abc456",
              "has_more": false
            }
    post:
      operationId: createMessage
      tags:
        - Assistants
      summary: Create a message.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: >-
            The ID of the [thread](/docs/api-reference/threads) to create a
            message for.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateMessageRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/MessageObject"
      x-oaiMeta:
        name: Create message
        group: threads
        beta: true
        returns: A [message](/docs/api-reference/messages/object) object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/threads/thread_abc123/messages \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2" \
                -d '{
                    "role": "user",
                    "content": "How does AI work? Explain it in simple terms."
                  }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              thread_message = client.beta.threads.messages.create(
                "thread_abc123",
                role="user",
                content="How does AI work? Explain it in simple terms.",
              )
              print(thread_message)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const threadMessages = await openai.beta.threads.messages.create(
                  "thread_abc123",
                  { role: "user", content: "How does AI work? Explain it in simple terms." }
                );

                console.log(threadMessages);
              }

              main();
          response: |
            {
              "id": "msg_abc123",
              "object": "thread.message",
              "created_at": 1713226573,
              "assistant_id": null,
              "thread_id": "thread_abc123",
              "run_id": null,
              "role": "user",
              "content": [
                {
                  "type": "text",
                  "text": {
                    "value": "How does AI work? Explain it in simple terms.",
                    "annotations": []
                  }
                }
              ],
              "attachments": [],
              "metadata": {}
            }
  /threads/{thread_id}/messages/{message_id}:
    get:
      operationId: getMessage
      tags:
        - Assistants
      summary: Retrieve a message.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: >-
            The ID of the [thread](/docs/api-reference/threads) to which this
            message belongs.
        - in: path
          name: message_id
          required: true
          schema:
            type: string
          description: The ID of the message to retrieve.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/MessageObject"
      x-oaiMeta:
        name: Retrieve message
        group: threads
        beta: true
        returns: >-
          The [message](/docs/api-reference/messages/object) object matching the
          specified ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/threads/thread_abc123/messages/msg_abc123
              \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              message = client.beta.threads.messages.retrieve(
                message_id="msg_abc123",
                thread_id="thread_abc123",
              )
              print(message)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const message = await openai.beta.threads.messages.retrieve(
                  "thread_abc123",
                  "msg_abc123"
                );

                console.log(message);
              }

              main();
          response: |
            {
              "id": "msg_abc123",
              "object": "thread.message",
              "created_at": 1699017614,
              "assistant_id": null,
              "thread_id": "thread_abc123",
              "run_id": null,
              "role": "user",
              "content": [
                {
                  "type": "text",
                  "text": {
                    "value": "How does AI work? Explain it in simple terms.",
                    "annotations": []
                  }
                }
              ],
              "attachments": [],
              "metadata": {}
            }
    post:
      operationId: modifyMessage
      tags:
        - Assistants
      summary: Modifies a message.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the thread to which this message belongs.
        - in: path
          name: message_id
          required: true
          schema:
            type: string
          description: The ID of the message to modify.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ModifyMessageRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/MessageObject"
      x-oaiMeta:
        name: Modify message
        group: threads
        beta: true
        returns: The modified [message](/docs/api-reference/messages/object) object.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/threads/thread_abc123/messages/msg_abc123
              \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2" \
                -d '{
                    "metadata": {
                      "modified": "true",
                      "user": "abc123"
                    }
                  }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              message = client.beta.threads.messages.update(
                message_id="msg_abc12",
                thread_id="thread_abc123",
                metadata={
                  "modified": "true",
                  "user": "abc123",
                },
              )
              print(message)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const message = await openai.beta.threads.messages.update(
                  "thread_abc123",
                  "msg_abc123",
                  {
                    metadata: {
                      modified: "true",
                      user: "abc123",
                    },
                  }
                }'
          response: |
            {
              "id": "msg_abc123",
              "object": "thread.message",
              "created_at": 1699017614,
              "assistant_id": null,
              "thread_id": "thread_abc123",
              "run_id": null,
              "role": "user",
              "content": [
                {
                  "type": "text",
                  "text": {
                    "value": "How does AI work? Explain it in simple terms.",
                    "annotations": []
                  }
                }
              ],
              "file_ids": [],
              "metadata": {
                "modified": "true",
                "user": "abc123"
              }
            }
    delete:
      operationId: deleteMessage
      tags:
        - Assistants
      summary: Deletes a message.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the thread to which this message belongs.
        - in: path
          name: message_id
          required: true
          schema:
            type: string
          description: The ID of the message to delete.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteMessageResponse"
      x-oaiMeta:
        name: Delete message
        group: threads
        beta: true
        returns: Deletion status
        examples:
          request:
            curl: >
              curl -X DELETE
              https://api.openai.com/v1/threads/thread_abc123/messages/msg_abc123
              \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              deleted_message = client.beta.threads.messages.delete(
                message_id="msg_abc12",
                thread_id="thread_abc123",
              )
              print(deleted_message)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const deletedMessage = await openai.beta.threads.messages.del(
                  "thread_abc123",
                  "msg_abc123"
                );

                console.log(deletedMessage);
              }
          response: |
            {
              "id": "msg_abc123",
              "object": "thread.message.deleted",
              "deleted": true
            }
  /threads/{thread_id}/runs:
    get:
      operationId: listRuns
      tags:
        - Assistants
      summary: Returns a list of runs belonging to a thread.
      parameters:
        - name: thread_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the thread the run belongs to.
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: order
          in: query
          description: >
            Sort order by the `created_at` timestamp of the objects. `asc` for
            ascending order and `desc` for descending order.
          schema:
            type: string
            default: desc
            enum:
              - asc
              - desc
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, starting with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          schema:
            type: string
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListRunsResponse"
      x-oaiMeta:
        name: List runs
        group: threads
        beta: true
        returns: A list of [run](/docs/api-reference/runs/object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/threads/thread_abc123/runs \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              runs = client.beta.threads.runs.list(
                "thread_abc123"
              )

              print(runs)
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const runs = await openai.beta.threads.runs.list(
                  "thread_abc123"
                );

                console.log(runs);
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "run_abc123",
                  "object": "thread.run",
                  "created_at": 1699075072,
                  "assistant_id": "asst_abc123",
                  "thread_id": "thread_abc123",
                  "status": "completed",
                  "started_at": 1699075072,
                  "expires_at": null,
                  "cancelled_at": null,
                  "failed_at": null,
                  "completed_at": 1699075073,
                  "last_error": null,
                  "model": "gpt-4o",
                  "instructions": null,
                  "incomplete_details": null,
                  "tools": [
                    {
                      "type": "code_interpreter"
                    }
                  ],
                  "tool_resources": {
                    "code_interpreter": {
                      "file_ids": [
                        "file-abc123",
                        "file-abc456"
                      ]
                    }
                  },
                  "metadata": {},
                  "usage": {
                    "prompt_tokens": 123,
                    "completion_tokens": 456,
                    "total_tokens": 579
                  },
                  "temperature": 1.0,
                  "top_p": 1.0,
                  "max_prompt_tokens": 1000,
                  "max_completion_tokens": 1000,
                  "truncation_strategy": {
                    "type": "auto",
                    "last_messages": null
                  },
                  "response_format": "auto",
                  "tool_choice": "auto",
                  "parallel_tool_calls": true
                },
                {
                  "id": "run_abc456",
                  "object": "thread.run",
                  "created_at": 1699063290,
                  "assistant_id": "asst_abc123",
                  "thread_id": "thread_abc123",
                  "status": "completed",
                  "started_at": 1699063290,
                  "expires_at": null,
                  "cancelled_at": null,
                  "failed_at": null,
                  "completed_at": 1699063291,
                  "last_error": null,
                  "model": "gpt-4o",
                  "instructions": null,
                  "incomplete_details": null,
                  "tools": [
                    {
                      "type": "code_interpreter"
                    }
                  ],
                  "tool_resources": {
                    "code_interpreter": {
                      "file_ids": [
                        "file-abc123",
                        "file-abc456"
                      ]
                    }
                  },
                  "metadata": {},
                  "usage": {
                    "prompt_tokens": 123,
                    "completion_tokens": 456,
                    "total_tokens": 579
                  },
                  "temperature": 1.0,
                  "top_p": 1.0,
                  "max_prompt_tokens": 1000,
                  "max_completion_tokens": 1000,
                  "truncation_strategy": {
                    "type": "auto",
                    "last_messages": null
                  },
                  "response_format": "auto",
                  "tool_choice": "auto",
                  "parallel_tool_calls": true
                }
              ],
              "first_id": "run_abc123",
              "last_id": "run_abc456",
              "has_more": false
            }
    post:
      operationId: createRun
      tags:
        - Assistants
      summary: Create a run.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the thread to run.
        - name: include[]
          in: query
          description: >
            A list of additional fields to include in the response. Currently
            the only supported value is
            `step_details.tool_calls[*].file_search.results[*].content` to fetch
            the file search result content.


            See the [file search tool
            documentation](/docs/assistants/tools/file-search#customizing-file-search-settings)
            for more information.
          schema:
            type: array
            items:
              type: string
              enum:
                - step_details.tool_calls[*].file_search.results[*].content
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateRunRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RunObject"
      x-oaiMeta:
        name: Create run
        group: threads
        beta: true
        returns: A [run](/docs/api-reference/runs/object) object.
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/threads/thread_abc123/runs \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "assistant_id": "asst_abc123"
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                run = client.beta.threads.runs.create(
                  thread_id="thread_abc123",
                  assistant_id="asst_abc123"
                )

                print(run)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const run = await openai.beta.threads.runs.create(
                    "thread_abc123",
                    { assistant_id: "asst_abc123" }
                  );

                  console.log(run);
                }

                main();
            response: |
              {
                "id": "run_abc123",
                "object": "thread.run",
                "created_at": 1699063290,
                "assistant_id": "asst_abc123",
                "thread_id": "thread_abc123",
                "status": "queued",
                "started_at": 1699063290,
                "expires_at": null,
                "cancelled_at": null,
                "failed_at": null,
                "completed_at": 1699063291,
                "last_error": null,
                "model": "gpt-4o",
                "instructions": null,
                "incomplete_details": null,
                "tools": [
                  {
                    "type": "code_interpreter"
                  }
                ],
                "metadata": {},
                "usage": null,
                "temperature": 1.0,
                "top_p": 1.0,
                "max_prompt_tokens": 1000,
                "max_completion_tokens": 1000,
                "truncation_strategy": {
                  "type": "auto",
                  "last_messages": null
                },
                "response_format": "auto",
                "tool_choice": "auto",
                "parallel_tool_calls": true
              }
          - title: Streaming
            request:
              curl: |
                curl https://api.openai.com/v1/threads/thread_123/runs \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "assistant_id": "asst_123",
                    "stream": true
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                stream = client.beta.threads.runs.create(
                  thread_id="thread_123",
                  assistant_id="asst_123",
                  stream=True
                )

                for event in stream:
                  print(event)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const stream = await openai.beta.threads.runs.create(
                    "thread_123",
                    { assistant_id: "asst_123", stream: true }
                  );

                  for await (const event of stream) {
                    console.log(event);
                  }
                }

                main();
            response: >
              event: thread.run.created

              data:
              {"id":"run_123","object":"thread.run","created_at":1710330640,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710331240,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.queued

              data:
              {"id":"run_123","object":"thread.run","created_at":1710330640,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710331240,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.in_progress

              data:
              {"id":"run_123","object":"thread.run","created_at":1710330640,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":1710330641,"expires_at":1710331240,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.step.created

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710330641,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710331240,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


              event: thread.run.step.in_progress

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710330641,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710331240,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


              event: thread.message.created

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710330641,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


              event: thread.message.in_progress

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710330641,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"Hello","annotations":[]}}]}}


              ...


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
              today"}}]}}


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"?"}}]}}


              event: thread.message.completed

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710330641,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"completed","incomplete_details":null,"incomplete_at":null,"completed_at":1710330642,"role":"assistant","content":[{"type":"text","text":{"value":"Hello!
              How can I assist you today?","annotations":[]}}],"metadata":{}}


              event: thread.run.step.completed

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710330641,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"completed","cancelled_at":null,"completed_at":1710330642,"expires_at":1710331240,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31}}


              event: thread.run.completed

              data:
              {"id":"run_123","object":"thread.run","created_at":1710330640,"assistant_id":"asst_123","thread_id":"thread_123","status":"completed","started_at":1710330641,"expires_at":null,"cancelled_at":null,"failed_at":null,"completed_at":1710330642,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: done

              data: [DONE]
          - title: Streaming with Functions
            request:
              curl: |
                curl https://api.openai.com/v1/threads/thread_abc123/runs \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "assistant_id": "asst_abc123",
                    "tools": [
                      {
                        "type": "function",
                        "function": {
                          "name": "get_current_weather",
                          "description": "Get the current weather in a given location",
                          "parameters": {
                            "type": "object",
                            "properties": {
                              "location": {
                                "type": "string",
                                "description": "The city and state, e.g. San Francisco, CA"
                              },
                              "unit": {
                                "type": "string",
                                "enum": ["celsius", "fahrenheit"]
                              }
                            },
                            "required": ["location"]
                          }
                        }
                      }
                    ],
                    "stream": true
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                tools = [
                  {
                    "type": "function",
                    "function": {
                      "name": "get_current_weather",
                      "description": "Get the current weather in a given location",
                      "parameters": {
                        "type": "object",
                        "properties": {
                          "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA",
                          },
                          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                        },
                        "required": ["location"],
                      },
                    }
                  }
                ]

                stream = client.beta.threads.runs.create(
                  thread_id="thread_abc123",
                  assistant_id="asst_abc123",
                  tools=tools,
                  stream=True
                )

                for event in stream:
                  print(event)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                const tools = [
                    {
                      "type": "function",
                      "function": {
                        "name": "get_current_weather",
                        "description": "Get the current weather in a given location",
                        "parameters": {
                          "type": "object",
                          "properties": {
                            "location": {
                              "type": "string",
                              "description": "The city and state, e.g. San Francisco, CA",
                            },
                            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                          },
                          "required": ["location"],
                        },
                      }
                    }
                ];

                async function main() {
                  const stream = await openai.beta.threads.runs.create(
                    "thread_abc123",
                    {
                      assistant_id: "asst_abc123",
                      tools: tools,
                      stream: true
                    }
                  );

                  for await (const event of stream) {
                    console.log(event);
                  }
                }

                main();
            response: >
              event: thread.run.created

              data:
              {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.queued

              data:
              {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":null,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.in_progress

              data:
              {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":1710348075,"expires_at":1710348675,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.step.created

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


              event: thread.run.step.in_progress

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":null}


              event: thread.message.created

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


              event: thread.message.in_progress

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"Hello","annotations":[]}}]}}


              ...


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
              today"}}]}}


              event: thread.message.delta

              data:
              {"id":"msg_001","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"?"}}]}}


              event: thread.message.completed

              data:
              {"id":"msg_001","object":"thread.message","created_at":1710348076,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"completed","incomplete_details":null,"incomplete_at":null,"completed_at":1710348077,"role":"assistant","content":[{"type":"text","text":{"value":"Hello!
              How can I assist you today?","annotations":[]}}],"metadata":{}}


              event: thread.run.step.completed

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710348076,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"completed","cancelled_at":null,"completed_at":1710348077,"expires_at":1710348675,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_001"}},"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31}}


              event: thread.run.completed

              data:
              {"id":"run_123","object":"thread.run","created_at":1710348075,"assistant_id":"asst_123","thread_id":"thread_123","status":"completed","started_at":1710348075,"expires_at":null,"cancelled_at":null,"failed_at":null,"completed_at":1710348077,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: done

              data: [DONE]
  /threads/{thread_id}/runs/{run_id}:
    get:
      operationId: getRun
      tags:
        - Assistants
      summary: Retrieves a run.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the [thread](/docs/api-reference/threads) that was run.
        - in: path
          name: run_id
          required: true
          schema:
            type: string
          description: The ID of the run to retrieve.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RunObject"
      x-oaiMeta:
        name: Retrieve run
        group: threads
        beta: true
        returns: >-
          The [run](/docs/api-reference/runs/object) object matching the
          specified ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/threads/thread_abc123/runs/run_abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              run = client.beta.threads.runs.retrieve(
                thread_id="thread_abc123",
                run_id="run_abc123"
              )

              print(run)
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const run = await openai.beta.threads.runs.retrieve(
                  "thread_abc123",
                  "run_abc123"
                );

                console.log(run);
              }

              main();
          response: |
            {
              "id": "run_abc123",
              "object": "thread.run",
              "created_at": 1699075072,
              "assistant_id": "asst_abc123",
              "thread_id": "thread_abc123",
              "status": "completed",
              "started_at": 1699075072,
              "expires_at": null,
              "cancelled_at": null,
              "failed_at": null,
              "completed_at": 1699075073,
              "last_error": null,
              "model": "gpt-4o",
              "instructions": null,
              "incomplete_details": null,
              "tools": [
                {
                  "type": "code_interpreter"
                }
              ],
              "metadata": {},
              "usage": {
                "prompt_tokens": 123,
                "completion_tokens": 456,
                "total_tokens": 579
              },
              "temperature": 1.0,
              "top_p": 1.0,
              "max_prompt_tokens": 1000,
              "max_completion_tokens": 1000,
              "truncation_strategy": {
                "type": "auto",
                "last_messages": null
              },
              "response_format": "auto",
              "tool_choice": "auto",
              "parallel_tool_calls": true
            }
    post:
      operationId: modifyRun
      tags:
        - Assistants
      summary: Modifies a run.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the [thread](/docs/api-reference/threads) that was run.
        - in: path
          name: run_id
          required: true
          schema:
            type: string
          description: The ID of the run to modify.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/ModifyRunRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RunObject"
      x-oaiMeta:
        name: Modify run
        group: threads
        beta: true
        returns: >-
          The modified [run](/docs/api-reference/runs/object) object matching
          the specified ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/threads/thread_abc123/runs/run_abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2" \
                -d '{
                  "metadata": {
                    "user_id": "user_abc123"
                  }
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              run = client.beta.threads.runs.update(
                thread_id="thread_abc123",
                run_id="run_abc123",
                metadata={"user_id": "user_abc123"},
              )

              print(run)
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const run = await openai.beta.threads.runs.update(
                  "thread_abc123",
                  "run_abc123",
                  {
                    metadata: {
                      user_id: "user_abc123",
                    },
                  }
                );

                console.log(run);
              }

              main();
          response: |
            {
              "id": "run_abc123",
              "object": "thread.run",
              "created_at": 1699075072,
              "assistant_id": "asst_abc123",
              "thread_id": "thread_abc123",
              "status": "completed",
              "started_at": 1699075072,
              "expires_at": null,
              "cancelled_at": null,
              "failed_at": null,
              "completed_at": 1699075073,
              "last_error": null,
              "model": "gpt-4o",
              "instructions": null,
              "incomplete_details": null,
              "tools": [
                {
                  "type": "code_interpreter"
                }
              ],
              "tool_resources": {
                "code_interpreter": {
                  "file_ids": [
                    "file-abc123",
                    "file-abc456"
                  ]
                }
              },
              "metadata": {
                "user_id": "user_abc123"
              },
              "usage": {
                "prompt_tokens": 123,
                "completion_tokens": 456,
                "total_tokens": 579
              },
              "temperature": 1.0,
              "top_p": 1.0,
              "max_prompt_tokens": 1000,
              "max_completion_tokens": 1000,
              "truncation_strategy": {
                "type": "auto",
                "last_messages": null
              },
              "response_format": "auto",
              "tool_choice": "auto",
              "parallel_tool_calls": true
            }
  /threads/{thread_id}/runs/{run_id}/cancel:
    post:
      operationId: cancelRun
      tags:
        - Assistants
      summary: Cancels a run that is `in_progress`.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the thread to which this run belongs.
        - in: path
          name: run_id
          required: true
          schema:
            type: string
          description: The ID of the run to cancel.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RunObject"
      x-oaiMeta:
        name: Cancel a run
        group: threads
        beta: true
        returns: >-
          The modified [run](/docs/api-reference/runs/object) object matching
          the specified ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/threads/thread_abc123/runs/run_abc123/cancel
              \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "OpenAI-Beta: assistants=v2" \
                -X POST
            python: |
              from openai import OpenAI
              client = OpenAI()

              run = client.beta.threads.runs.cancel(
                thread_id="thread_abc123",
                run_id="run_abc123"
              )

              print(run)
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const run = await openai.beta.threads.runs.cancel(
                  "thread_abc123",
                  "run_abc123"
                );

                console.log(run);
              }

              main();
          response: |
            {
              "id": "run_abc123",
              "object": "thread.run",
              "created_at": 1699076126,
              "assistant_id": "asst_abc123",
              "thread_id": "thread_abc123",
              "status": "cancelling",
              "started_at": 1699076126,
              "expires_at": 1699076726,
              "cancelled_at": null,
              "failed_at": null,
              "completed_at": null,
              "last_error": null,
              "model": "gpt-4o",
              "instructions": "You summarize books.",
              "tools": [
                {
                  "type": "file_search"
                }
              ],
              "tool_resources": {
                "file_search": {
                  "vector_store_ids": ["vs_123"]
                }
              },
              "metadata": {},
              "usage": null,
              "temperature": 1.0,
              "top_p": 1.0,
              "response_format": "auto",
              "tool_choice": "auto",
              "parallel_tool_calls": true
            }
  /threads/{thread_id}/runs/{run_id}/steps:
    get:
      operationId: listRunSteps
      tags:
        - Assistants
      summary: Returns a list of run steps belonging to a run.
      parameters:
        - name: thread_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the thread the run and run steps belong to.
        - name: run_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the run the run steps belong to.
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: order
          in: query
          description: >
            Sort order by the `created_at` timestamp of the objects. `asc` for
            ascending order and `desc` for descending order.
          schema:
            type: string
            default: desc
            enum:
              - asc
              - desc
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, starting with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          schema:
            type: string
        - name: include[]
          in: query
          description: >
            A list of additional fields to include in the response. Currently
            the only supported value is
            `step_details.tool_calls[*].file_search.results[*].content` to fetch
            the file search result content.


            See the [file search tool
            documentation](/docs/assistants/tools/file-search#customizing-file-search-settings)
            for more information.
          schema:
            type: array
            items:
              type: string
              enum:
                - step_details.tool_calls[*].file_search.results[*].content
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListRunStepsResponse"
      x-oaiMeta:
        name: List run steps
        group: threads
        beta: true
        returns: >-
          A list of [run step](/docs/api-reference/run-steps/step-object)
          objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/threads/thread_abc123/runs/run_abc123/steps
              \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              run_steps = client.beta.threads.runs.steps.list(
                  thread_id="thread_abc123",
                  run_id="run_abc123"
              )

              print(run_steps)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const runStep = await openai.beta.threads.runs.steps.list(
                  "thread_abc123",
                  "run_abc123"
                );
                console.log(runStep);
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "step_abc123",
                  "object": "thread.run.step",
                  "created_at": 1699063291,
                  "run_id": "run_abc123",
                  "assistant_id": "asst_abc123",
                  "thread_id": "thread_abc123",
                  "type": "message_creation",
                  "status": "completed",
                  "cancelled_at": null,
                  "completed_at": 1699063291,
                  "expired_at": null,
                  "failed_at": null,
                  "last_error": null,
                  "step_details": {
                    "type": "message_creation",
                    "message_creation": {
                      "message_id": "msg_abc123"
                    }
                  },
                  "usage": {
                    "prompt_tokens": 123,
                    "completion_tokens": 456,
                    "total_tokens": 579
                  }
                }
              ],
              "first_id": "step_abc123",
              "last_id": "step_abc456",
              "has_more": false
            }
  /threads/{thread_id}/runs/{run_id}/steps/{step_id}:
    get:
      operationId: getRunStep
      tags:
        - Assistants
      summary: Retrieves a run step.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: The ID of the thread to which the run and run step belongs.
        - in: path
          name: run_id
          required: true
          schema:
            type: string
          description: The ID of the run to which the run step belongs.
        - in: path
          name: step_id
          required: true
          schema:
            type: string
          description: The ID of the run step to retrieve.
        - name: include[]
          in: query
          description: >
            A list of additional fields to include in the response. Currently
            the only supported value is
            `step_details.tool_calls[*].file_search.results[*].content` to fetch
            the file search result content.


            See the [file search tool
            documentation](/docs/assistants/tools/file-search#customizing-file-search-settings)
            for more information.
          schema:
            type: array
            items:
              type: string
              enum:
                - step_details.tool_calls[*].file_search.results[*].content
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RunStepObject"
      x-oaiMeta:
        name: Retrieve run step
        group: threads
        beta: true
        returns: >-
          The [run step](/docs/api-reference/run-steps/step-object) object
          matching the specified ID.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/threads/thread_abc123/runs/run_abc123/steps/step_abc123
              \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              run_step = client.beta.threads.runs.steps.retrieve(
                  thread_id="thread_abc123",
                  run_id="run_abc123",
                  step_id="step_abc123"
              )

              print(run_step)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const runStep = await openai.beta.threads.runs.steps.retrieve(
                  "thread_abc123",
                  "run_abc123",
                  "step_abc123"
                );
                console.log(runStep);
              }

              main();
          response: |
            {
              "id": "step_abc123",
              "object": "thread.run.step",
              "created_at": 1699063291,
              "run_id": "run_abc123",
              "assistant_id": "asst_abc123",
              "thread_id": "thread_abc123",
              "type": "message_creation",
              "status": "completed",
              "cancelled_at": null,
              "completed_at": 1699063291,
              "expired_at": null,
              "failed_at": null,
              "last_error": null,
              "step_details": {
                "type": "message_creation",
                "message_creation": {
                  "message_id": "msg_abc123"
                }
              },
              "usage": {
                "prompt_tokens": 123,
                "completion_tokens": 456,
                "total_tokens": 579
              }
            }
  /threads/{thread_id}/runs/{run_id}/submit_tool_outputs:
    post:
      operationId: submitToolOuputsToRun
      tags:
        - Assistants
      summary: >
        When a run has the `status: "requires_action"` and
        `required_action.type` is `submit_tool_outputs`, this endpoint can be
        used to submit the outputs from the tool calls once they're all
        completed. All outputs must be submitted in a single request.
      parameters:
        - in: path
          name: thread_id
          required: true
          schema:
            type: string
          description: >-
            The ID of the [thread](/docs/api-reference/threads) to which this
            run belongs.
        - in: path
          name: run_id
          required: true
          schema:
            type: string
          description: The ID of the run that requires the tool output submission.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/SubmitToolOutputsRunRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/RunObject"
      x-oaiMeta:
        name: Submit tool outputs to run
        group: threads
        beta: true
        returns: >-
          The modified [run](/docs/api-reference/runs/object) object matching
          the specified ID.
        examples:
          - title: Default
            request:
              curl: >
                curl
                https://api.openai.com/v1/threads/thread_123/runs/run_123/submit_tool_outputs
                \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "tool_outputs": [
                      {
                        "tool_call_id": "call_001",
                        "output": "70 degrees and sunny."
                      }
                    ]
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                run = client.beta.threads.runs.submit_tool_outputs(
                  thread_id="thread_123",
                  run_id="run_123",
                  tool_outputs=[
                    {
                      "tool_call_id": "call_001",
                      "output": "70 degrees and sunny."
                    }
                  ]
                )

                print(run)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const run = await openai.beta.threads.runs.submitToolOutputs(
                    "thread_123",
                    "run_123",
                    {
                      tool_outputs: [
                        {
                          tool_call_id: "call_001",
                          output: "70 degrees and sunny.",
                        },
                      ],
                    }
                  );

                  console.log(run);
                }

                main();
            response: |
              {
                "id": "run_123",
                "object": "thread.run",
                "created_at": 1699075592,
                "assistant_id": "asst_123",
                "thread_id": "thread_123",
                "status": "queued",
                "started_at": 1699075592,
                "expires_at": 1699076192,
                "cancelled_at": null,
                "failed_at": null,
                "completed_at": null,
                "last_error": null,
                "model": "gpt-4o",
                "instructions": null,
                "tools": [
                  {
                    "type": "function",
                    "function": {
                      "name": "get_current_weather",
                      "description": "Get the current weather in a given location",
                      "parameters": {
                        "type": "object",
                        "properties": {
                          "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA"
                          },
                          "unit": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"]
                          }
                        },
                        "required": ["location"]
                      }
                    }
                  }
                ],
                "metadata": {},
                "usage": null,
                "temperature": 1.0,
                "top_p": 1.0,
                "max_prompt_tokens": 1000,
                "max_completion_tokens": 1000,
                "truncation_strategy": {
                  "type": "auto",
                  "last_messages": null
                },
                "response_format": "auto",
                "tool_choice": "auto",
                "parallel_tool_calls": true
              }
          - title: Streaming
            request:
              curl: >
                curl
                https://api.openai.com/v1/threads/thread_123/runs/run_123/submit_tool_outputs
                \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "tool_outputs": [
                      {
                        "tool_call_id": "call_001",
                        "output": "70 degrees and sunny."
                      }
                    ],
                    "stream": true
                  }'
              python: |
                from openai import OpenAI
                client = OpenAI()

                stream = client.beta.threads.runs.submit_tool_outputs(
                  thread_id="thread_123",
                  run_id="run_123",
                  tool_outputs=[
                    {
                      "tool_call_id": "call_001",
                      "output": "70 degrees and sunny."
                    }
                  ],
                  stream=True
                )

                for event in stream:
                  print(event)
              node.js: |
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const stream = await openai.beta.threads.runs.submitToolOutputs(
                    "thread_123",
                    "run_123",
                    {
                      tool_outputs: [
                        {
                          tool_call_id: "call_001",
                          output: "70 degrees and sunny.",
                        },
                      ],
                    }
                  );

                  for await (const event of stream) {
                    console.log(event);
                  }
                }

                main();
            response: >
              event: thread.run.step.completed

              data:
              {"id":"step_001","object":"thread.run.step","created_at":1710352449,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"tool_calls","status":"completed","cancelled_at":null,"completed_at":1710352475,"expires_at":1710353047,"failed_at":null,"last_error":null,"step_details":{"type":"tool_calls","tool_calls":[{"id":"call_iWr0kQ2EaYMaxNdl0v3KYkx7","type":"function","function":{"name":"get_current_weather","arguments":"{\"location\":\"San
              Francisco, CA\",\"unit\":\"fahrenheit\"}","output":"70 degrees and
              sunny."}}]},"usage":{"prompt_tokens":291,"completion_tokens":24,"total_tokens":315}}


              event: thread.run.queued

              data:
              {"id":"run_123","object":"thread.run","created_at":1710352447,"assistant_id":"asst_123","thread_id":"thread_123","status":"queued","started_at":1710352448,"expires_at":1710353047,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
              the current weather in a given
              location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
              city and state, e.g. San Francisco,
              CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.in_progress

              data:
              {"id":"run_123","object":"thread.run","created_at":1710352447,"assistant_id":"asst_123","thread_id":"thread_123","status":"in_progress","started_at":1710352475,"expires_at":1710353047,"cancelled_at":null,"failed_at":null,"completed_at":null,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
              the current weather in a given
              location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
              city and state, e.g. San Francisco,
              CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":null,"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: thread.run.step.created

              data:
              {"id":"step_002","object":"thread.run.step","created_at":1710352476,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710353047,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_002"}},"usage":null}


              event: thread.run.step.in_progress

              data:
              {"id":"step_002","object":"thread.run.step","created_at":1710352476,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"in_progress","cancelled_at":null,"completed_at":null,"expires_at":1710353047,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_002"}},"usage":null}


              event: thread.message.created

              data:
              {"id":"msg_002","object":"thread.message","created_at":1710352476,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


              event: thread.message.in_progress

              data:
              {"id":"msg_002","object":"thread.message","created_at":1710352476,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"in_progress","incomplete_details":null,"incomplete_at":null,"completed_at":null,"role":"assistant","content":[],"metadata":{}}


              event: thread.message.delta

              data:
              {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"The","annotations":[]}}]}}


              event: thread.message.delta

              data:
              {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
              current"}}]}}


              event: thread.message.delta

              data:
              {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
              weather"}}]}}


              ...


              event: thread.message.delta

              data:
              {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"
              sunny"}}]}}


              event: thread.message.delta

              data:
              {"id":"msg_002","object":"thread.message.delta","delta":{"content":[{"index":0,"type":"text","text":{"value":"."}}]}}


              event: thread.message.completed

              data:
              {"id":"msg_002","object":"thread.message","created_at":1710352476,"assistant_id":"asst_123","thread_id":"thread_123","run_id":"run_123","status":"completed","incomplete_details":null,"incomplete_at":null,"completed_at":1710352477,"role":"assistant","content":[{"type":"text","text":{"value":"The
              current weather in San Francisco, CA is 70 degrees Fahrenheit and
              sunny.","annotations":[]}}],"metadata":{}}


              event: thread.run.step.completed

              data:
              {"id":"step_002","object":"thread.run.step","created_at":1710352476,"run_id":"run_123","assistant_id":"asst_123","thread_id":"thread_123","type":"message_creation","status":"completed","cancelled_at":null,"completed_at":1710352477,"expires_at":1710353047,"failed_at":null,"last_error":null,"step_details":{"type":"message_creation","message_creation":{"message_id":"msg_002"}},"usage":{"prompt_tokens":329,"completion_tokens":18,"total_tokens":347}}


              event: thread.run.completed

              data:
              {"id":"run_123","object":"thread.run","created_at":1710352447,"assistant_id":"asst_123","thread_id":"thread_123","status":"completed","started_at":1710352475,"expires_at":null,"cancelled_at":null,"failed_at":null,"completed_at":1710352477,"required_action":null,"last_error":null,"model":"gpt-4o","instructions":null,"tools":[{"type":"function","function":{"name":"get_current_weather","description":"Get
              the current weather in a given
              location","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The
              city and state, e.g. San Francisco,
              CA"},"unit":{"type":"string","enum":["celsius","fahrenheit"]}},"required":["location"]}}}],"metadata":{},"temperature":1.0,"top_p":1.0,"max_completion_tokens":null,"max_prompt_tokens":null,"truncation_strategy":{"type":"auto","last_messages":null},"incomplete_details":null,"usage":{"prompt_tokens":20,"completion_tokens":11,"total_tokens":31},"response_format":"auto","tool_choice":"auto","parallel_tool_calls":true}}


              event: done

              data: [DONE]
  /uploads:
    post:
      operationId: createUpload
      tags:
        - Uploads
      summary: >
        Creates an intermediate [Upload](/docs/api-reference/uploads/object)
        object that you can add [Parts](/docs/api-reference/uploads/part-object)
        to. Currently, an Upload can accept at most 8 GB in total and expires
        after an hour after you create it.


        Once you complete the Upload, we will create a
        [File](/docs/api-reference/files/object) object that contains all the
        parts you uploaded. This File is usable in the rest of our platform as a
        regular File object.


        For certain `purpose`s, the correct `mime_type` must be specified.
        Please refer to documentation for the supported MIME types for your use
        case:

        - [Assistants](/docs/assistants/tools/file-search#supported-files)


        For guidance on the proper filename extensions for each purpose, please
        follow the documentation on [creating a
        File](/docs/api-reference/files/create).
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateUploadRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Upload"
      x-oaiMeta:
        name: Create upload
        group: uploads
        returns: >-
          The [Upload](/docs/api-reference/uploads/object) object with status
          `pending`.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/uploads \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "purpose": "fine-tune",
                  "filename": "training_examples.jsonl",
                  "bytes": 2147483648,
                  "mime_type": "text/jsonl"
                }'
          response: |
            {
              "id": "upload_abc123",
              "object": "upload",
              "bytes": 2147483648,
              "created_at": 1719184911,
              "filename": "training_examples.jsonl",
              "purpose": "fine-tune",
              "status": "pending",
              "expires_at": 1719127296
            }
  /uploads/{upload_id}/cancel:
    post:
      operationId: cancelUpload
      tags:
        - Uploads
      summary: |
        Cancels the Upload. No Parts may be added after an Upload is cancelled.
      parameters:
        - in: path
          name: upload_id
          required: true
          schema:
            type: string
            example: upload_abc123
          description: |
            The ID of the Upload.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Upload"
      x-oaiMeta:
        name: Cancel upload
        group: uploads
        returns: >-
          The [Upload](/docs/api-reference/uploads/object) object with status
          `cancelled`.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/uploads/upload_abc123/cancel
          response: |
            {
              "id": "upload_abc123",
              "object": "upload",
              "bytes": 2147483648,
              "created_at": 1719184911,
              "filename": "training_examples.jsonl",
              "purpose": "fine-tune",
              "status": "cancelled",
              "expires_at": 1719127296
            }
  /uploads/{upload_id}/complete:
    post:
      operationId: completeUpload
      tags:
        - Uploads
      summary: >
        Completes the [Upload](/docs/api-reference/uploads/object). 


        Within the returned Upload object, there is a nested
        [File](/docs/api-reference/files/object) object that is ready to use in
        the rest of the platform.


        You can specify the order of the Parts by passing in an ordered list of
        the Part IDs.


        The number of bytes uploaded upon completion must match the number of
        bytes initially specified when creating the Upload object. No Parts may
        be added after an Upload is completed.
      parameters:
        - in: path
          name: upload_id
          required: true
          schema:
            type: string
            example: upload_abc123
          description: |
            The ID of the Upload.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CompleteUploadRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Upload"
      x-oaiMeta:
        name: Complete upload
        group: uploads
        returns: >-
          The [Upload](/docs/api-reference/uploads/object) object with status
          `completed` with an additional `file` property containing the created
          usable File object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/uploads/upload_abc123/complete
                -d '{
                  "part_ids": ["part_def456", "part_ghi789"]
                }'
          response: |
            {
              "id": "upload_abc123",
              "object": "upload",
              "bytes": 2147483648,
              "created_at": 1719184911,
              "filename": "training_examples.jsonl",
              "purpose": "fine-tune",
              "status": "completed",
              "expires_at": 1719127296,
              "file": {
                "id": "file-xyz321",
                "object": "file",
                "bytes": 2147483648,
                "created_at": 1719186911,
                "filename": "training_examples.jsonl",
                "purpose": "fine-tune",
              }
            }
  /uploads/{upload_id}/parts:
    post:
      operationId: addUploadPart
      tags:
        - Uploads
      summary: >
        Adds a [Part](/docs/api-reference/uploads/part-object) to an
        [Upload](/docs/api-reference/uploads/object) object. A Part represents a
        chunk of bytes from the file you are trying to upload. 


        Each Part can be at most 64 MB, and you can add Parts until you hit the
        Upload maximum of 8 GB.


        It is possible to add multiple Parts in parallel. You can decide the
        intended order of the Parts when you [complete the
        Upload](/docs/api-reference/uploads/complete).
      parameters:
        - in: path
          name: upload_id
          required: true
          schema:
            type: string
            example: upload_abc123
          description: |
            The ID of the Upload.
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: "#/components/schemas/AddUploadPartRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/UploadPart"
      x-oaiMeta:
        name: Add upload part
        group: uploads
        returns: The upload [Part](/docs/api-reference/uploads/part-object) object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/uploads/upload_abc123/parts
                -F data="aHR0cHM6Ly9hcGkub3BlbmFpLmNvbS92MS91cGxvYWRz..."
          response: |
            {
              "id": "part_def456",
              "object": "upload.part",
              "created_at": 1719185911,
              "upload_id": "upload_abc123"
            }
  /vector_stores:
    get:
      operationId: listVectorStores
      tags:
        - Vector stores
      summary: Returns a list of vector stores.
      parameters:
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: order
          in: query
          description: >
            Sort order by the `created_at` timestamp of the objects. `asc` for
            ascending order and `desc` for descending order.
          schema:
            type: string
            default: desc
            enum:
              - asc
              - desc
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, starting with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          schema:
            type: string
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListVectorStoresResponse"
      x-oaiMeta:
        name: List vector stores
        group: vector_stores
        beta: true
        returns: >-
          A list of [vector store](/docs/api-reference/vector-stores/object)
          objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/vector_stores \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              vector_stores = client.beta.vector_stores.list()
              print(vector_stores)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const vectorStores = await openai.beta.vectorStores.list();
                console.log(vectorStores);
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "vs_abc123",
                  "object": "vector_store",
                  "created_at": 1699061776,
                  "name": "Support FAQ",
                  "bytes": 139920,
                  "file_counts": {
                    "in_progress": 0,
                    "completed": 3,
                    "failed": 0,
                    "cancelled": 0,
                    "total": 3
                  }
                },
                {
                  "id": "vs_abc456",
                  "object": "vector_store",
                  "created_at": 1699061776,
                  "name": "Support FAQ v2",
                  "bytes": 139920,
                  "file_counts": {
                    "in_progress": 0,
                    "completed": 3,
                    "failed": 0,
                    "cancelled": 0,
                    "total": 3
                  }
                }
              ],
              "first_id": "vs_abc123",
              "last_id": "vs_abc456",
              "has_more": false
            }
    post:
      operationId: createVectorStore
      tags:
        - Vector stores
      summary: Create a vector store.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateVectorStoreRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VectorStoreObject"
      x-oaiMeta:
        name: Create vector store
        group: vector_stores
        beta: true
        returns: A [vector store](/docs/api-reference/vector-stores/object) object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/vector_stores \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
                -d '{
                  "name": "Support FAQ"
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              vector_store = client.beta.vector_stores.create(
                name="Support FAQ"
              )
              print(vector_store)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const vectorStore = await openai.beta.vectorStores.create({
                  name: "Support FAQ"
                });
                console.log(vectorStore);
              }

              main();
          response: |
            {
              "id": "vs_abc123",
              "object": "vector_store",
              "created_at": 1699061776,
              "name": "Support FAQ",
              "bytes": 139920,
              "file_counts": {
                "in_progress": 0,
                "completed": 3,
                "failed": 0,
                "cancelled": 0,
                "total": 3
              }
            }
  /vector_stores/{vector_store_id}:
    get:
      operationId: getVectorStore
      tags:
        - Vector stores
      summary: Retrieves a vector store.
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
          description: The ID of the vector store to retrieve.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VectorStoreObject"
      x-oaiMeta:
        name: Retrieve vector store
        group: vector_stores
        beta: true
        returns: >-
          The [vector store](/docs/api-reference/vector-stores/object) object
          matching the specified ID.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/vector_stores/vs_abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              vector_store = client.beta.vector_stores.retrieve(
                vector_store_id="vs_abc123"
              )
              print(vector_store)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const vectorStore = await openai.beta.vectorStores.retrieve(
                  "vs_abc123"
                );
                console.log(vectorStore);
              }

              main();
          response: |
            {
              "id": "vs_abc123",
              "object": "vector_store",
              "created_at": 1699061776
            }
    post:
      operationId: modifyVectorStore
      tags:
        - Vector stores
      summary: Modifies a vector store.
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
          description: The ID of the vector store to modify.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/UpdateVectorStoreRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VectorStoreObject"
      x-oaiMeta:
        name: Modify vector store
        group: vector_stores
        beta: true
        returns: >-
          The modified [vector store](/docs/api-reference/vector-stores/object)
          object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/vector_stores/vs_abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
                -d '{
                  "name": "Support FAQ"
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              vector_store = client.beta.vector_stores.update(
                vector_store_id="vs_abc123",
                name="Support FAQ"
              )
              print(vector_store)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const vectorStore = await openai.beta.vectorStores.update(
                  "vs_abc123",
                  {
                    name: "Support FAQ"
                  }
                );
                console.log(vectorStore);
              }

              main();
          response: |
            {
              "id": "vs_abc123",
              "object": "vector_store",
              "created_at": 1699061776,
              "name": "Support FAQ",
              "bytes": 139920,
              "file_counts": {
                "in_progress": 0,
                "completed": 3,
                "failed": 0,
                "cancelled": 0,
                "total": 3
              }
            }
    delete:
      operationId: deleteVectorStore
      tags:
        - Vector stores
      summary: Delete a vector store.
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
          description: The ID of the vector store to delete.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteVectorStoreResponse"
      x-oaiMeta:
        name: Delete vector store
        group: vector_stores
        beta: true
        returns: Deletion status
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/vector_stores/vs_abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2" \
                -X DELETE
            python: |
              from openai import OpenAI
              client = OpenAI()

              deleted_vector_store = client.beta.vector_stores.delete(
                vector_store_id="vs_abc123"
              )
              print(deleted_vector_store)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const deletedVectorStore = await openai.beta.vectorStores.del(
                  "vs_abc123"
                );
                console.log(deletedVectorStore);
              }

              main();
          response: |
            {
              id: "vs_abc123",
              object: "vector_store.deleted",
              deleted: true
            }
  /vector_stores/{vector_store_id}/file_batches:
    post:
      operationId: createVectorStoreFileBatch
      tags:
        - Vector stores
      summary: Create a vector store file batch.
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
            example: vs_abc123
          description: |
            The ID of the vector store for which to create a File Batch.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateVectorStoreFileBatchRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VectorStoreFileBatchObject"
      x-oaiMeta:
        name: Create vector store file batch
        group: vector_stores
        beta: true
        returns: >-
          A [vector store file
          batch](/docs/api-reference/vector-stores-file-batches/batch-object)
          object.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/vector_stores/vs_abc123/file_batches \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "file_ids": ["file-abc123", "file-abc456"]
                  }'
            python: >
              from openai import OpenAI

              client = OpenAI()


              vector_store_file_batch =
              client.beta.vector_stores.file_batches.create(
                vector_store_id="vs_abc123",
                file_ids=["file-abc123", "file-abc456"]
              )

              print(vector_store_file_batch)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const myVectorStoreFileBatch = await openai.beta.vectorStores.fileBatches.create(
                  "vs_abc123",
                  {
                    file_ids: ["file-abc123", "file-abc456"]
                  }
                );
                console.log(myVectorStoreFileBatch);
              }

              main();
          response: |
            {
              "id": "vsfb_abc123",
              "object": "vector_store.file_batch",
              "created_at": 1699061776,
              "vector_store_id": "vs_abc123",
              "status": "in_progress",
              "file_counts": {
                "in_progress": 1,
                "completed": 1,
                "failed": 0,
                "cancelled": 0,
                "total": 0,
              }
            }
  /vector_stores/{vector_store_id}/file_batches/{batch_id}:
    get:
      operationId: getVectorStoreFileBatch
      tags:
        - Vector stores
      summary: Retrieves a vector store file batch.
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
            example: vs_abc123
          description: The ID of the vector store that the file batch belongs to.
        - in: path
          name: batch_id
          required: true
          schema:
            type: string
            example: vsfb_abc123
          description: The ID of the file batch being retrieved.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VectorStoreFileBatchObject"
      x-oaiMeta:
        name: Retrieve vector store file batch
        group: vector_stores
        beta: true
        returns: >-
          The [vector store file
          batch](/docs/api-reference/vector-stores-file-batches/batch-object)
          object.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/vector_stores/vs_abc123/files_batches/vsfb_abc123
              \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: >
              from openai import OpenAI

              client = OpenAI()


              vector_store_file_batch =
              client.beta.vector_stores.file_batches.retrieve(
                vector_store_id="vs_abc123",
                batch_id="vsfb_abc123"
              )

              print(vector_store_file_batch)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const vectorStoreFileBatch = await openai.beta.vectorStores.fileBatches.retrieve(
                  "vs_abc123",
                  "vsfb_abc123"
                );
                console.log(vectorStoreFileBatch);
              }

              main();
          response: |
            {
              "id": "vsfb_abc123",
              "object": "vector_store.file_batch",
              "created_at": 1699061776,
              "vector_store_id": "vs_abc123",
              "status": "in_progress",
              "file_counts": {
                "in_progress": 1,
                "completed": 1,
                "failed": 0,
                "cancelled": 0,
                "total": 0,
              }
            }
  /vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel:
    post:
      operationId: cancelVectorStoreFileBatch
      tags:
        - Vector stores
      summary: >-
        Cancel a vector store file batch. This attempts to cancel the processing
        of files in this batch as soon as possible.
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
          description: The ID of the vector store that the file batch belongs to.
        - in: path
          name: batch_id
          required: true
          schema:
            type: string
          description: The ID of the file batch to cancel.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VectorStoreFileBatchObject"
      x-oaiMeta:
        name: Cancel vector store file batch
        group: vector_stores
        beta: true
        returns: The modified vector store file batch object.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/vector_stores/vs_abc123/files_batches/vsfb_abc123/cancel
              \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2" \
                -X POST
            python: >
              from openai import OpenAI

              client = OpenAI()


              deleted_vector_store_file_batch =
              client.beta.vector_stores.file_batches.cancel(
                  vector_store_id="vs_abc123",
                  file_batch_id="vsfb_abc123"
              )

              print(deleted_vector_store_file_batch)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const deletedVectorStoreFileBatch = await openai.vector_stores.fileBatches.cancel(
                  "vs_abc123",
                  "vsfb_abc123"
                );
                console.log(deletedVectorStoreFileBatch);
              }

              main();
          response: |
            {
              "id": "vsfb_abc123",
              "object": "vector_store.file_batch",
              "created_at": 1699061776,
              "vector_store_id": "vs_abc123",
              "status": "in_progress",
              "file_counts": {
                "in_progress": 12,
                "completed": 3,
                "failed": 0,
                "cancelled": 0,
                "total": 15,
              }
            }
  /vector_stores/{vector_store_id}/file_batches/{batch_id}/files:
    get:
      operationId: listFilesInVectorStoreBatch
      tags:
        - Vector stores
      summary: Returns a list of vector store files in a batch.
      parameters:
        - name: vector_store_id
          in: path
          description: The ID of the vector store that the files belong to.
          required: true
          schema:
            type: string
        - name: batch_id
          in: path
          description: The ID of the file batch that the files belong to.
          required: true
          schema:
            type: string
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: order
          in: query
          description: >
            Sort order by the `created_at` timestamp of the objects. `asc` for
            ascending order and `desc` for descending order.
          schema:
            type: string
            default: desc
            enum:
              - asc
              - desc
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, starting with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          schema:
            type: string
        - name: filter
          in: query
          description: >-
            Filter by file status. One of `in_progress`, `completed`, `failed`,
            `cancelled`.
          schema:
            type: string
            enum:
              - in_progress
              - completed
              - failed
              - cancelled
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListVectorStoreFilesResponse"
      x-oaiMeta:
        name: List vector store files in a batch
        group: vector_stores
        beta: true
        returns: >-
          A list of [vector store
          file](/docs/api-reference/vector-stores-files/file-object) objects.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/vector_stores/vs_abc123/files_batches/vsfb_abc123/files
              \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: >
              from openai import OpenAI

              client = OpenAI()


              vector_store_files =
              client.beta.vector_stores.file_batches.list_files(
                vector_store_id="vs_abc123",
                batch_id="vsfb_abc123"
              )

              print(vector_store_files)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const vectorStoreFiles = await openai.beta.vectorStores.fileBatches.listFiles(
                  "vs_abc123",
                  "vsfb_abc123"
                );
                console.log(vectorStoreFiles);
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "file-abc123",
                  "object": "vector_store.file",
                  "created_at": 1699061776,
                  "vector_store_id": "vs_abc123"
                },
                {
                  "id": "file-abc456",
                  "object": "vector_store.file",
                  "created_at": 1699061776,
                  "vector_store_id": "vs_abc123"
                }
              ],
              "first_id": "file-abc123",
              "last_id": "file-abc456",
              "has_more": false
            }
  /vector_stores/{vector_store_id}/files:
    get:
      operationId: listVectorStoreFiles
      tags:
        - Vector stores
      summary: Returns a list of vector store files.
      parameters:
        - name: vector_store_id
          in: path
          description: The ID of the vector store that the files belong to.
          required: true
          schema:
            type: string
        - name: limit
          in: query
          description: >
            A limit on the number of objects to be returned. Limit can range
            between 1 and 100, and the default is 20.
          required: false
          schema:
            type: integer
            default: 20
        - name: order
          in: query
          description: >
            Sort order by the `created_at` timestamp of the objects. `asc` for
            ascending order and `desc` for descending order.
          schema:
            type: string
            default: desc
            enum:
              - asc
              - desc
        - name: after
          in: query
          description: >
            A cursor for use in pagination. `after` is an object ID that defines
            your place in the list. For instance, if you make a list request and
            receive 100 objects, ending with obj_foo, your subsequent call can
            include after=obj_foo in order to fetch the next page of the list.
          schema:
            type: string
        - name: before
          in: query
          description: >
            A cursor for use in pagination. `before` is an object ID that
            defines your place in the list. For instance, if you make a list
            request and receive 100 objects, starting with obj_foo, your
            subsequent call can include before=obj_foo in order to fetch the
            previous page of the list.
          schema:
            type: string
        - name: filter
          in: query
          description: >-
            Filter by file status. One of `in_progress`, `completed`, `failed`,
            `cancelled`.
          schema:
            type: string
            enum:
              - in_progress
              - completed
              - failed
              - cancelled
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListVectorStoreFilesResponse"
      x-oaiMeta:
        name: List vector store files
        group: vector_stores
        beta: true
        returns: >-
          A list of [vector store
          file](/docs/api-reference/vector-stores-files/file-object) objects.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/vector_stores/vs_abc123/files \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              vector_store_files = client.beta.vector_stores.files.list(
                vector_store_id="vs_abc123"
              )
              print(vector_store_files)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const vectorStoreFiles = await openai.beta.vectorStores.files.list(
                  "vs_abc123"
                );
                console.log(vectorStoreFiles);
              }

              main();
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "file-abc123",
                  "object": "vector_store.file",
                  "created_at": 1699061776,
                  "vector_store_id": "vs_abc123"
                },
                {
                  "id": "file-abc456",
                  "object": "vector_store.file",
                  "created_at": 1699061776,
                  "vector_store_id": "vs_abc123"
                }
              ],
              "first_id": "file-abc123",
              "last_id": "file-abc456",
              "has_more": false
            }
    post:
      operationId: createVectorStoreFile
      tags:
        - Vector stores
      summary: >-
        Create a vector store file by attaching a
        [File](/docs/api-reference/files) to a [vector
        store](/docs/api-reference/vector-stores/object).
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
            example: vs_abc123
          description: |
            The ID of the vector store for which to create a File.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateVectorStoreFileRequest"
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VectorStoreFileObject"
      x-oaiMeta:
        name: Create vector store file
        group: vector_stores
        beta: true
        returns: >-
          A [vector store
          file](/docs/api-reference/vector-stores-files/file-object) object.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/vector_stores/vs_abc123/files \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -H "OpenAI-Beta: assistants=v2" \
                  -d '{
                    "file_id": "file-abc123"
                  }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              vector_store_file = client.beta.vector_stores.files.create(
                vector_store_id="vs_abc123",
                file_id="file-abc123"
              )
              print(vector_store_file)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const myVectorStoreFile = await openai.beta.vectorStores.files.create(
                  "vs_abc123",
                  {
                    file_id: "file-abc123"
                  }
                );
                console.log(myVectorStoreFile);
              }

              main();
          response: |
            {
              "id": "file-abc123",
              "object": "vector_store.file",
              "created_at": 1699061776,
              "usage_bytes": 1234,
              "vector_store_id": "vs_abcd",
              "status": "completed",
              "last_error": null
            }
  /vector_stores/{vector_store_id}/files/{file_id}:
    get:
      operationId: getVectorStoreFile
      tags:
        - Vector stores
      summary: Retrieves a vector store file.
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
            example: vs_abc123
          description: The ID of the vector store that the file belongs to.
        - in: path
          name: file_id
          required: true
          schema:
            type: string
            example: file-abc123
          description: The ID of the file being retrieved.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/VectorStoreFileObject"
      x-oaiMeta:
        name: Retrieve vector store file
        group: vector_stores
        beta: true
        returns: >-
          The [vector store
          file](/docs/api-reference/vector-stores-files/file-object) object.
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/vector_stores/vs_abc123/files/file-abc123
              \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2"
            python: |
              from openai import OpenAI
              client = OpenAI()

              vector_store_file = client.beta.vector_stores.files.retrieve(
                vector_store_id="vs_abc123",
                file_id="file-abc123"
              )
              print(vector_store_file)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const vectorStoreFile = await openai.beta.vectorStores.files.retrieve(
                  "vs_abc123",
                  "file-abc123"
                );
                console.log(vectorStoreFile);
              }

              main();
          response: |
            {
              "id": "file-abc123",
              "object": "vector_store.file",
              "created_at": 1699061776,
              "vector_store_id": "vs_abcd",
              "status": "completed",
              "last_error": null
            }
    delete:
      operationId: deleteVectorStoreFile
      tags:
        - Vector stores
      summary: >-
        Delete a vector store file. This will remove the file from the vector
        store but the file itself will not be deleted. To delete the file, use
        the [delete file](/docs/api-reference/files/delete) endpoint.
      parameters:
        - in: path
          name: vector_store_id
          required: true
          schema:
            type: string
          description: The ID of the vector store that the file belongs to.
        - in: path
          name: file_id
          required: true
          schema:
            type: string
          description: The ID of the file to delete.
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteVectorStoreFileResponse"
      x-oaiMeta:
        name: Delete vector store file
        group: vector_stores
        beta: true
        returns: Deletion status
        examples:
          request:
            curl: >
              curl
              https://api.openai.com/v1/vector_stores/vs_abc123/files/file-abc123
              \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -H "OpenAI-Beta: assistants=v2" \
                -X DELETE
            python: >
              from openai import OpenAI

              client = OpenAI()


              deleted_vector_store_file =
              client.beta.vector_stores.files.delete(
                  vector_store_id="vs_abc123",
                  file_id="file-abc123"
              )

              print(deleted_vector_store_file)
            node.js: |
              import OpenAI from "openai";
              const openai = new OpenAI();

              async function main() {
                const deletedVectorStoreFile = await openai.beta.vectorStores.files.del(
                  "vs_abc123",
                  "file-abc123"
                );
                console.log(deletedVectorStoreFile);
              }

              main();
          response: |
            {
              id: "file-abc123",
              object: "vector_store.file.deleted",
              deleted: true
            }
components:
  schemas:
    AddUploadPartRequest:
      type: object
      additionalProperties: false
      properties:
        data:
          description: |
            The chunk of bytes for this Part.
          type: string
          format: binary
      required:
        - data
    AdminApiKey:
      type: object
      properties:
        object:
          type: string
          example: organization.admin_api_key
        id:
          type: string
          example: key_abc
        name:
          type: string
          example: Administration Key
        redacted_value:
          type: string
          example: sk-admin...def
        value:
          type: string
          example: sk-admin-1234abcd
        created_at:
          type: integer
          format: int64
          example: 1711471533
        owner:
          type: object
          properties:
            type:
              type: string
              example: service_account
            id:
              type: string
              example: sa_456
            name:
              type: string
              example: My Service Account
            created_at:
              type: integer
              format: int64
              example: 1711471533
            role:
              type: string
              example: member
    ApiKeyList:
      type: object
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/AdminApiKey"
        has_more:
          type: boolean
          example: false
        first_id:
          type: string
          example: key_abc
        last_id:
          type: string
          example: key_xyz
    AssistantObject:
      type: object
      title: Assistant
      description: Represents an `assistant` that can call the model and use tools.
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `assistant`.
          type: string
          enum:
            - assistant
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the assistant was created.
          type: integer
        name:
          description: |
            The name of the assistant. The maximum length is 256 characters.
          type: string
          maxLength: 256
          nullable: true
        description:
          description: >
            The description of the assistant. The maximum length is 512
            characters.
          type: string
          maxLength: 512
          nullable: true
        model:
          description: >
            ID of the model to use. You can use the [List
            models](/docs/api-reference/models/list) API to see all of your
            available models, or see our [Model overview](/docs/models) for
            descriptions of them.
          type: string
        instructions:
          description: >
            The system instructions that the assistant uses. The maximum length
            is 256,000 characters.
          type: string
          maxLength: 256000
          nullable: true
        tools:
          description: >
            A list of tool enabled on the assistant. There can be a maximum of
            128 tools per assistant. Tools can be of types `code_interpreter`,
            `file_search`, or `function`.
          default: []
          type: array
          maxItems: 128
          items:
            oneOf:
              - $ref: "#/components/schemas/AssistantToolsCode"
              - $ref: "#/components/schemas/AssistantToolsFileSearch"
              - $ref: "#/components/schemas/AssistantToolsFunction"
            x-oaiExpandable: true
        tool_resources:
          type: object
          description: >
            A set of resources that are used by the assistant's tools. The
            resources are specific to the type of tool. For example, the
            `code_interpreter` tool requires a list of file IDs, while the
            `file_search` tool requires a list of vector store IDs.
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: >
                    A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter`` tool. There can be a
                    maximum of 20 files associated with the tool.
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: >
                    The ID of the [vector
                    store](/docs/api-reference/vector-stores/object) attached to
                    this assistant. There can be a maximum of 1 vector store
                    attached to the assistant.
                  maxItems: 1
                  items:
                    type: string
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
        temperature:
          description: >
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: >
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with
            top_p probability mass. So 0.1 means only the tokens comprising the
            top 10% probability mass are considered.


            We generally recommend altering this or temperature but not both.
        response_format:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiResponseFormatOption"
            - nullable: true
      required:
        - id
        - object
        - created_at
        - name
        - description
        - model
        - instructions
        - tools
        - metadata
      x-oaiMeta:
        name: The assistant object
        beta: true
        example: |
          {
            "id": "asst_abc123",
            "object": "assistant",
            "created_at": 1698984975,
            "name": "Math Tutor",
            "description": null,
            "model": "gpt-4o",
            "instructions": "You are a personal math tutor. When asked a question, write and run Python code to answer the question.",
            "tools": [
              {
                "type": "code_interpreter"
              }
            ],
            "metadata": {},
            "top_p": 1.0,
            "temperature": 1.0,
            "response_format": "auto"
          }
    AssistantStreamEvent:
      description: >
        Represents an event emitted when streaming a Run.


        Each event in a server-sent events stream has an `event` and `data`
        property:


        ```

        event: thread.created

        data: {"id": "thread_123", "object": "thread", ...}

        ```


        We emit events whenever a new object is created, transitions to a new
        state, or is being

        streamed in parts (deltas). For example, we emit `thread.run.created`
        when a new run

        is created, `thread.run.completed` when a run completes, and so on. When
        an Assistant chooses

        to create a message during a run, we emit a `thread.message.created
        event`, a

        `thread.message.in_progress` event, many `thread.message.delta` events,
        and finally a

        `thread.message.completed` event.


        We may add additional events over time, so we recommend handling unknown
        events gracefully

        in your code. See the [Assistants API
        quickstart](/docs/assistants/overview) to learn how to

        integrate the Assistants API with streaming.
      oneOf:
        - $ref: "#/components/schemas/ThreadStreamEvent"
        - $ref: "#/components/schemas/RunStreamEvent"
        - $ref: "#/components/schemas/RunStepStreamEvent"
        - $ref: "#/components/schemas/MessageStreamEvent"
        - $ref: "#/components/schemas/ErrorEvent"
        - $ref: "#/components/schemas/DoneEvent"
      x-oaiMeta:
        name: Assistant stream events
        beta: true
    AssistantSupportedModels:
      type: string
      enum:
        - o3-mini
        - o3-mini-2025-01-31
        - o1
        - o1-2024-12-17
        - gpt-4o
        - gpt-4o-2024-11-20
        - gpt-4o-2024-08-06
        - gpt-4o-2024-05-13
        - gpt-4o-mini
        - gpt-4o-mini-2024-07-18
        - gpt-4-turbo
        - gpt-4-turbo-2024-04-09
        - gpt-4-0125-preview
        - gpt-4-turbo-preview
        - gpt-4-1106-preview
        - gpt-4-vision-preview
        - gpt-4
        - gpt-4-0314
        - gpt-4-0613
        - gpt-4-32k
        - gpt-4-32k-0314
        - gpt-4-32k-0613
        - gpt-3.5-turbo
        - gpt-3.5-turbo-16k
        - gpt-3.5-turbo-0613
        - gpt-3.5-turbo-1106
        - gpt-3.5-turbo-0125
        - gpt-3.5-turbo-16k-0613
    AssistantToolsCode:
      type: object
      title: Code interpreter tool
      properties:
        type:
          type: string
          description: "The type of tool being defined: `code_interpreter`"
          enum:
            - code_interpreter
          x-stainless-const: true
      required:
        - type
    AssistantToolsFileSearch:
      type: object
      title: FileSearch tool
      properties:
        type:
          type: string
          description: "The type of tool being defined: `file_search`"
          enum:
            - file_search
          x-stainless-const: true
        file_search:
          type: object
          description: Overrides for the file search tool.
          properties:
            max_num_results:
              type: integer
              minimum: 1
              maximum: 50
              description: >
                The maximum number of results the file search tool should
                output. The default is 20 for `gpt-4*` models and 5 for
                `gpt-3.5-turbo`. This number should be between 1 and 50
                inclusive.


                Note that the file search tool may output fewer than
                `max_num_results` results. See the [file search tool
                documentation](/docs/assistants/tools/file-search#customizing-file-search-settings)
                for more information.
            ranking_options:
              $ref: "#/components/schemas/FileSearchRankingOptions"
      required:
        - type
    AssistantToolsFileSearchTypeOnly:
      type: object
      title: FileSearch tool
      properties:
        type:
          type: string
          description: "The type of tool being defined: `file_search`"
          enum:
            - file_search
          x-stainless-const: true
      required:
        - type
    AssistantToolsFunction:
      type: object
      title: Function tool
      properties:
        type:
          type: string
          description: "The type of tool being defined: `function`"
          enum:
            - function
          x-stainless-const: true
        function:
          $ref: "#/components/schemas/FunctionObject"
      required:
        - type
        - function
    AssistantsApiResponseFormatOption:
      description: >
        Specifies the format that the model must output. Compatible with
        [GPT-4o](/docs/models#gpt-4o), [GPT-4
        Turbo](/docs/models#gpt-4-turbo-and-gpt-4), and all GPT-3.5 Turbo models
        since `gpt-3.5-turbo-1106`.


        Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
        Structured Outputs which ensures the model will match your supplied JSON
        schema. Learn more in the [Structured Outputs
        guide](/docs/guides/structured-outputs).


        Setting to `{ "type": "json_object" }` enables JSON mode, which ensures
        the message the model generates is valid JSON.


        **Important:** when using JSON mode, you **must** also instruct the
        model to produce JSON yourself via a system or user message. Without
        this, the model may generate an unending stream of whitespace until the
        generation reaches the token limit, resulting in a long-running and
        seemingly "stuck" request. Also note that the message content may be
        partially cut off if `finish_reason="length"`, which indicates the
        generation exceeded `max_tokens` or the conversation exceeded the max
        context length.
      oneOf:
        - type: string
          description: |
            `auto` is the default value
          enum:
            - auto
          x-stainless-const: true
        - $ref: "#/components/schemas/ResponseFormatText"
        - $ref: "#/components/schemas/ResponseFormatJsonObject"
        - $ref: "#/components/schemas/ResponseFormatJsonSchema"
      x-oaiExpandable: true
    AssistantsApiToolChoiceOption:
      description: >
        Controls which (if any) tool is called by the model.

        `none` means the model will not call any tools and instead generates a
        message.

        `auto` is the default value and means the model can pick between
        generating a message or calling one or more tools.

        `required` means the model must call one or more tools before responding
        to the user.

        Specifying a particular tool like `{"type": "file_search"}` or `{"type":
        "function", "function": {"name": "my_function"}}` forces the model to
        call that tool.
      oneOf:
        - type: string
          description: >
            `none` means the model will not call any tools and instead generates
            a message. `auto` means the model can pick between generating a
            message or calling one or more tools. `required` means the model
            must call one or more tools before responding to the user.
          enum:
            - none
            - auto
            - required
        - $ref: "#/components/schemas/AssistantsNamedToolChoice"
      x-oaiExpandable: true
    AssistantsNamedToolChoice:
      type: object
      description: >-
        Specifies a tool the model should use. Use to force the model to call a
        specific tool.
      properties:
        type:
          type: string
          enum:
            - function
            - code_interpreter
            - file_search
          description: >-
            The type of the tool. If type is `function`, the function name must
            be set
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
          required:
            - name
      required:
        - type
    AudioResponseFormat:
      description: >
        The format of the output, in one of these options: `json`, `text`,
        `srt`, `verbose_json`, or `vtt`.
      type: string
      enum:
        - json
        - text
        - srt
        - verbose_json
        - vtt
      default: json
    AuditLog:
      type: object
      description: A log of a user action or configuration change within this organization.
      properties:
        id:
          type: string
          description: The ID of this log.
        type:
          $ref: "#/components/schemas/AuditLogEventType"
        effective_at:
          type: integer
          description: The Unix timestamp (in seconds) of the event.
        project:
          type: object
          description: >-
            The project that the action was scoped to. Absent for actions not
            scoped to projects.
          properties:
            id:
              type: string
              description: The project ID.
            name:
              type: string
              description: The project title.
        actor:
          $ref: "#/components/schemas/AuditLogActor"
        api_key.created:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The tracking ID of the API key.
            data:
              type: object
              description: The payload used to create the API key.
              properties:
                scopes:
                  type: array
                  items:
                    type: string
                  description: >-
                    A list of scopes allowed for the API key, e.g.
                    `["api.model.request"]`
        api_key.updated:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The tracking ID of the API key.
            changes_requested:
              type: object
              description: The payload used to update the API key.
              properties:
                scopes:
                  type: array
                  items:
                    type: string
                  description: >-
                    A list of scopes allowed for the API key, e.g.
                    `["api.model.request"]`
        api_key.deleted:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The tracking ID of the API key.
        invite.sent:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The ID of the invite.
            data:
              type: object
              description: The payload used to create the invite.
              properties:
                email:
                  type: string
                  description: The email invited to the organization.
                role:
                  type: string
                  description: >-
                    The role the email was invited to be. Is either `owner` or
                    `member`.
        invite.accepted:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The ID of the invite.
        invite.deleted:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The ID of the invite.
        login.failed:
          type: object
          description: The details for events with this `type`.
          properties:
            error_code:
              type: string
              description: The error code of the failure.
            error_message:
              type: string
              description: The error message of the failure.
        logout.failed:
          type: object
          description: The details for events with this `type`.
          properties:
            error_code:
              type: string
              description: The error code of the failure.
            error_message:
              type: string
              description: The error message of the failure.
        organization.updated:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The organization ID.
            changes_requested:
              type: object
              description: The payload used to update the organization settings.
              properties:
                title:
                  type: string
                  description: The organization title.
                description:
                  type: string
                  description: The organization description.
                name:
                  type: string
                  description: The organization name.
                settings:
                  type: object
                  properties:
                    threads_ui_visibility:
                      type: string
                      description: >-
                        Visibility of the threads page which shows messages
                        created with the Assistants API and Playground. One of
                        `ANY_ROLE`, `OWNERS`, or `NONE`.
                    usage_dashboard_visibility:
                      type: string
                      description: >-
                        Visibility of the usage dashboard which shows activity
                        and costs for your organization. One of `ANY_ROLE` or
                        `OWNERS`.
        project.created:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The project ID.
            data:
              type: object
              description: The payload used to create the project.
              properties:
                name:
                  type: string
                  description: The project name.
                title:
                  type: string
                  description: The title of the project as seen on the dashboard.
        project.updated:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The project ID.
            changes_requested:
              type: object
              description: The payload used to update the project.
              properties:
                title:
                  type: string
                  description: The title of the project as seen on the dashboard.
        project.archived:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The project ID.
        rate_limit.updated:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The rate limit ID
            changes_requested:
              type: object
              description: The payload used to update the rate limits.
              properties:
                max_requests_per_1_minute:
                  type: integer
                  description: The maximum requests per minute.
                max_tokens_per_1_minute:
                  type: integer
                  description: The maximum tokens per minute.
                max_images_per_1_minute:
                  type: integer
                  description: >-
                    The maximum images per minute. Only relevant for certain
                    models.
                max_audio_megabytes_per_1_minute:
                  type: integer
                  description: >-
                    The maximum audio megabytes per minute. Only relevant for
                    certain models.
                max_requests_per_1_day:
                  type: integer
                  description: >-
                    The maximum requests per day. Only relevant for certain
                    models.
                batch_1_day_max_input_tokens:
                  type: integer
                  description: >-
                    The maximum batch input tokens per day. Only relevant for
                    certain models.
        rate_limit.deleted:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The rate limit ID
        service_account.created:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The service account ID.
            data:
              type: object
              description: The payload used to create the service account.
              properties:
                role:
                  type: string
                  description: >-
                    The role of the service account. Is either `owner` or
                    `member`.
        service_account.updated:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The service account ID.
            changes_requested:
              type: object
              description: The payload used to updated the service account.
              properties:
                role:
                  type: string
                  description: >-
                    The role of the service account. Is either `owner` or
                    `member`.
        service_account.deleted:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The service account ID.
        user.added:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The user ID.
            data:
              type: object
              description: The payload used to add the user to the project.
              properties:
                role:
                  type: string
                  description: The role of the user. Is either `owner` or `member`.
        user.updated:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The project ID.
            changes_requested:
              type: object
              description: The payload used to update the user.
              properties:
                role:
                  type: string
                  description: The role of the user. Is either `owner` or `member`.
        user.deleted:
          type: object
          description: The details for events with this `type`.
          properties:
            id:
              type: string
              description: The user ID.
      required:
        - id
        - type
        - effective_at
        - actor
      x-oaiMeta:
        name: The audit log object
        example: |
          {
              "id": "req_xxx_20240101",
              "type": "api_key.created",
              "effective_at": 1720804090,
              "actor": {
                  "type": "session",
                  "session": {
                      "user": {
                          "id": "user-xxx",
                          "email": "user@example.com"
                      },
                      "ip_address": "127.0.0.1",
                      "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                  }
              },
              "api_key.created": {
                  "id": "key_xxxx",
                  "data": {
                      "scopes": ["resource.operation"]
                  }
              }
          }
    AuditLogActor:
      type: object
      description: The actor who performed the audit logged action.
      properties:
        type:
          type: string
          description: The type of actor. Is either `session` or `api_key`.
          enum:
            - session
            - api_key
        session:
          $ref: "#/components/schemas/AuditLogActorSession"
        api_key:
          $ref: "#/components/schemas/AuditLogActorApiKey"
    AuditLogActorApiKey:
      type: object
      description: The API Key used to perform the audit logged action.
      properties:
        id:
          type: string
          description: The tracking id of the API key.
        type:
          type: string
          description: The type of API key. Can be either `user` or `service_account`.
          enum:
            - user
            - service_account
        user:
          $ref: "#/components/schemas/AuditLogActorUser"
        service_account:
          $ref: "#/components/schemas/AuditLogActorServiceAccount"
    AuditLogActorServiceAccount:
      type: object
      description: The service account that performed the audit logged action.
      properties:
        id:
          type: string
          description: The service account id.
    AuditLogActorSession:
      type: object
      description: The session in which the audit logged action was performed.
      properties:
        user:
          $ref: "#/components/schemas/AuditLogActorUser"
        ip_address:
          type: string
          description: The IP address from which the action was performed.
    AuditLogActorUser:
      type: object
      description: The user who performed the audit logged action.
      properties:
        id:
          type: string
          description: The user id.
        email:
          type: string
          description: The user email.
    AuditLogEventType:
      type: string
      description: The event type.
      x-oaiExpandable: true
      enum:
        - api_key.created
        - api_key.updated
        - api_key.deleted
        - invite.sent
        - invite.accepted
        - invite.deleted
        - login.succeeded
        - login.failed
        - logout.succeeded
        - logout.failed
        - organization.updated
        - project.created
        - project.updated
        - project.archived
        - service_account.created
        - service_account.updated
        - service_account.deleted
        - rate_limit.updated
        - rate_limit.deleted
        - user.added
        - user.updated
        - user.deleted
    AutoChunkingStrategyRequestParam:
      type: object
      title: Auto Chunking Strategy
      description: >-
        The default strategy. This strategy currently uses a
        `max_chunk_size_tokens` of `800` and `chunk_overlap_tokens` of `400`.
      additionalProperties: false
      properties:
        type:
          type: string
          description: Always `auto`.
          enum:
            - auto
          x-stainless-const: true
      required:
        - type
    Batch:
      type: object
      properties:
        id:
          type: string
        object:
          type: string
          enum:
            - batch
          description: The object type, which is always `batch`.
          x-stainless-const: true
        endpoint:
          type: string
          description: The OpenAI API endpoint used by the batch.
        errors:
          type: object
          properties:
            object:
              type: string
              description: The object type, which is always `list`.
            data:
              type: array
              items:
                type: object
                properties:
                  code:
                    type: string
                    description: An error code identifying the error type.
                  message:
                    type: string
                    description: >-
                      A human-readable message providing more details about the
                      error.
                  param:
                    type: string
                    description: >-
                      The name of the parameter that caused the error, if
                      applicable.
                    nullable: true
                  line:
                    type: integer
                    description: >-
                      The line number of the input file where the error
                      occurred, if applicable.
                    nullable: true
        input_file_id:
          type: string
          description: The ID of the input file for the batch.
        completion_window:
          type: string
          description: The time frame within which the batch should be processed.
        status:
          type: string
          description: The current status of the batch.
          enum:
            - validating
            - failed
            - in_progress
            - finalizing
            - completed
            - expired
            - cancelling
            - cancelled
        output_file_id:
          type: string
          description: >-
            The ID of the file containing the outputs of successfully executed
            requests.
        error_file_id:
          type: string
          description: The ID of the file containing the outputs of requests with errors.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch was created.
        in_progress_at:
          type: integer
          description: >-
            The Unix timestamp (in seconds) for when the batch started
            processing.
        expires_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch will expire.
        finalizing_at:
          type: integer
          description: >-
            The Unix timestamp (in seconds) for when the batch started
            finalizing.
        completed_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch was completed.
        failed_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch failed.
        expired_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch expired.
        cancelling_at:
          type: integer
          description: >-
            The Unix timestamp (in seconds) for when the batch started
            cancelling.
        cancelled_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the batch was cancelled.
        request_counts:
          type: object
          properties:
            total:
              type: integer
              description: Total number of requests in the batch.
            completed:
              type: integer
              description: Number of requests that have been completed successfully.
            failed:
              type: integer
              description: Number of requests that have failed.
          required:
            - total
            - completed
            - failed
          description: The request counts for different statuses within the batch.
        metadata:
          $ref: "#/components/schemas/Metadata"
      required:
        - id
        - object
        - endpoint
        - input_file_id
        - completion_window
        - status
        - created_at
      x-oaiMeta:
        name: The batch object
        example: |
          {
            "id": "batch_abc123",
            "object": "batch",
            "endpoint": "/v1/completions",
            "errors": null,
            "input_file_id": "file-abc123",
            "completion_window": "24h",
            "status": "completed",
            "output_file_id": "file-cvaTdG",
            "error_file_id": "file-HOWS94",
            "created_at": 1711471533,
            "in_progress_at": 1711471538,
            "expires_at": 1711557933,
            "finalizing_at": 1711493133,
            "completed_at": 1711493163,
            "failed_at": null,
            "expired_at": null,
            "cancelling_at": null,
            "cancelled_at": null,
            "request_counts": {
              "total": 100,
              "completed": 95,
              "failed": 5
            },
            "metadata": {
              "customer_id": "user_123456789",
              "batch_description": "Nightly eval job",
            }
          }
    BatchRequestInput:
      type: object
      description: The per-line object of the batch input file
      properties:
        custom_id:
          type: string
          description: >-
            A developer-provided per-request id that will be used to match
            outputs to inputs. Must be unique for each request in a batch.
        method:
          type: string
          enum:
            - POST
          description: >-
            The HTTP method to be used for the request. Currently only `POST` is
            supported.
          x-stainless-const: true
        url:
          type: string
          description: >-
            The OpenAI API relative URL to be used for the request. Currently
            `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions` are
            supported.
      x-oaiMeta:
        name: The request input object
        example: >
          {"custom_id": "request-1", "method": "POST", "url":
          "/v1/chat/completions", "body": {"model": "gpt-4o-mini", "messages":
          [{"role": "system", "content": "You are a helpful assistant."},
          {"role": "user", "content": "What is 2+2?"}]}}
    BatchRequestOutput:
      type: object
      description: The per-line object of the batch output and error files
      properties:
        id:
          type: string
        custom_id:
          type: string
          description: >-
            A developer-provided per-request id that will be used to match
            outputs to inputs.
        response:
          type: object
          nullable: true
          properties:
            status_code:
              type: integer
              description: The HTTP status code of the response
            request_id:
              type: string
              description: >-
                An unique identifier for the OpenAI API request. Please include
                this request ID when contacting support.
            body:
              type: object
              x-oaiTypeLabel: map
              description: The JSON body of the response
        error:
          type: object
          nullable: true
          description: >-
            For requests that failed with a non-HTTP error, this will contain
            more information on the cause of the failure.
          properties:
            code:
              type: string
              description: A machine-readable error code.
            message:
              type: string
              description: A human-readable error message.
      x-oaiMeta:
        name: The request output object
        example: >
          {"id": "batch_req_wnaDys", "custom_id": "request-2", "response":
          {"status_code": 200, "request_id": "req_c187b3", "body": {"id":
          "chatcmpl-9758Iw", "object": "chat.completion", "created": 1711475054,
          "model": "gpt-4o-mini", "choices": [{"index": 0, "message": {"role":
          "assistant", "content": "2 + 2 equals 4."}, "finish_reason": "stop"}],
          "usage": {"prompt_tokens": 24, "completion_tokens": 15,
          "total_tokens": 39}, "system_fingerprint": null}}, "error": null}
    CancelUploadRequest:
      type: object
      additionalProperties: false
    ChatCompletionFunctionCallOption:
      type: object
      description: >
        Specifying a particular function via `{"name": "my_function"}` forces
        the model to call that function.
      properties:
        name:
          type: string
          description: The name of the function to call.
      required:
        - name
    ChatCompletionMessageToolCall:
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          description: The function that the model called.
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the
                model in JSON format. Note that the model does not always
                generate valid JSON, and may hallucinate parameters not defined
                by your function schema. Validate the arguments in your code
                before calling your function.
          required:
            - name
            - arguments
      required:
        - id
        - type
        - function
    ChatCompletionMessageToolCallChunk:
      type: object
      properties:
        index:
          type: integer
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: >-
                The arguments to call the function with, as generated by the
                model in JSON format. Note that the model does not always
                generate valid JSON, and may hallucinate parameters not defined
                by your function schema. Validate the arguments in your code
                before calling your function.
      required:
        - index
    ChatCompletionMessageToolCalls:
      type: array
      description: The tool calls generated by the model, such as function calls.
      items:
        $ref: "#/components/schemas/ChatCompletionMessageToolCall"
    ChatCompletionModalities:
      type: array
      nullable: true
      description: >
        Output types that you would like the model to generate for this request.

        Most models are capable of generating text, which is the default:


        `["text"]`


        The `gpt-4o-audio-preview` model can also be used to [generate
        audio](/docs/guides/audio). To

        request that this model generate both text and audio responses, you can

        use:


        `["text", "audio"]`
      items:
        type: string
        enum:
          - text
          - audio
    ChatCompletionNamedToolChoice:
      type: object
      description: >-
        Specifies a tool the model should use. Use to force the model to call a
        specific function.
      properties:
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
          required:
            - name
      required:
        - type
        - function
    ChatCompletionRequestAssistantMessage:
      type: object
      title: Assistant message
      description: |
        Messages sent by the model in response to user messages.
      properties:
        content:
          x-oaiExpandable: true
          nullable: true
          oneOf:
            - type: string
              description: The contents of the assistant message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. Can be one or
                more of type `text`, or exactly one of type `refusal`.
              title: Array of content parts
              items:
                $ref: >-
                  #/components/schemas/ChatCompletionRequestAssistantMessageContentPart
              minItems: 1
          description: >
            The contents of the assistant message. Required unless `tool_calls`
            or `function_call` is specified.
        refusal:
          nullable: true
          type: string
          description: The refusal message by the assistant.
        role:
          type: string
          enum:
            - assistant
          description: The role of the messages author, in this case `assistant`.
          x-stainless-const: true
        name:
          type: string
          description: >-
            An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
        audio:
          type: object
          nullable: true
          x-oaiExpandable: true
          description: |
            Data about a previous audio response from the model. 
            [Learn more](/docs/guides/audio).
          required:
            - id
          properties:
            id:
              type: string
              description: |
                Unique identifier for a previous audio response from the model.
        tool_calls:
          $ref: "#/components/schemas/ChatCompletionMessageToolCalls"
      required:
        - role
    ChatCompletionRequestAssistantMessageContentPart:
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartRefusal"
      x-oaiExpandable: true
    ChatCompletionRequestDeveloperMessage:
      type: object
      title: Developer message
      description: >
        Developer-provided instructions that the model should follow, regardless
        of

        messages sent by the user. With o1 models and newer, `developer`
        messages

        replace the previous `system` messages.
      properties:
        content:
          description: The contents of the developer message.
          oneOf:
            - type: string
              description: The contents of the developer message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. For developer
                messages, only type `text` is supported.
              title: Array of content parts
              items:
                $ref: >-
                  #/components/schemas/ChatCompletionRequestMessageContentPartText
              minItems: 1
        role:
          type: string
          enum:
            - developer
          description: The role of the messages author, in this case `developer`.
          x-stainless-const: true
        name:
          type: string
          description: >-
            An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
        - content
        - role
    ChatCompletionRequestMessage:
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionRequestDeveloperMessage"
        - $ref: "#/components/schemas/ChatCompletionRequestSystemMessage"
        - $ref: "#/components/schemas/ChatCompletionRequestUserMessage"
        - $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage"
        - $ref: "#/components/schemas/ChatCompletionRequestToolMessage"
      x-oaiExpandable: true
    ChatCompletionRequestMessageContentPartAudio:
      type: object
      title: Audio content part
      description: |
        Learn about [audio inputs](/docs/guides/audio).
      properties:
        type:
          type: string
          enum:
            - input_audio
          description: The type of the content part. Always `input_audio`.
          x-stainless-const: true
        input_audio:
          type: object
          properties:
            data:
              type: string
              description: Base64 encoded audio data.
            format:
              type: string
              enum:
                - wav
                - mp3
              description: >
                The format of the encoded audio data. Currently supports "wav"
                and "mp3".
          required:
            - data
            - format
      required:
        - type
        - input_audio
    ChatCompletionRequestMessageContentPartImage:
      type: object
      title: Image content part
      description: |
        Learn about [image inputs](/docs/guides/vision).
      properties:
        type:
          type: string
          enum:
            - image_url
          description: The type of the content part.
          x-stainless-const: true
        image_url:
          type: object
          properties:
            url:
              type: string
              description: Either a URL of the image or the base64 encoded image data.
              format: uri
            detail:
              type: string
              description: >-
                Specifies the detail level of the image. Learn more in the
                [Vision
                guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).
              enum:
                - auto
                - low
                - high
              default: auto
          required:
            - url
      required:
        - type
        - image_url
    ChatCompletionRequestMessageContentPartRefusal:
      type: object
      title: Refusal content part
      properties:
        type:
          type: string
          enum:
            - refusal
          description: The type of the content part.
          x-stainless-const: true
        refusal:
          type: string
          description: The refusal message generated by the model.
      required:
        - type
        - refusal
    ChatCompletionRequestMessageContentPartText:
      type: object
      title: Text content part
      description: |
        Learn about [text inputs](/docs/guides/text-generation).
      properties:
        type:
          type: string
          enum:
            - text
          description: The type of the content part.
          x-stainless-const: true
        text:
          type: string
          description: The text content.
      required:
        - type
        - text
    ChatCompletionRequestSystemMessage:
      type: object
      title: System message
      description: >
        Developer-provided instructions that the model should follow, regardless
        of

        messages sent by the user. With o1 models and newer, use `developer`
        messages

        for this purpose instead.
      properties:
        content:
          description: The contents of the system message.
          oneOf:
            - type: string
              description: The contents of the system message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. For system
                messages, only type `text` is supported.
              title: Array of content parts
              items:
                $ref: >-
                  #/components/schemas/ChatCompletionRequestSystemMessageContentPart
              minItems: 1
        role:
          type: string
          enum:
            - system
          description: The role of the messages author, in this case `system`.
          x-stainless-const: true
        name:
          type: string
          description: >-
            An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
        - content
        - role
    ChatCompletionRequestSystemMessageContentPart:
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
      x-oaiExpandable: true
    ChatCompletionRequestToolMessage:
      type: object
      title: Tool message
      properties:
        role:
          type: string
          enum:
            - tool
          description: The role of the messages author, in this case `tool`.
          x-stainless-const: true
        content:
          oneOf:
            - type: string
              description: The contents of the tool message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. For tool
                messages, only type `text` is supported.
              title: Array of content parts
              items:
                $ref: >-
                  #/components/schemas/ChatCompletionRequestToolMessageContentPart
              minItems: 1
          description: The contents of the tool message.
        tool_call_id:
          type: string
          description: Tool call that this message is responding to.
      required:
        - role
        - content
        - tool_call_id
    ChatCompletionRequestToolMessageContentPart:
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
      x-oaiExpandable: true
    ChatCompletionRequestUserMessage:
      type: object
      title: User message
      description: |
        Messages sent by an end user, containing prompts or additional context
        information.
      properties:
        content:
          description: |
            The contents of the user message.
          oneOf:
            - type: string
              description: The text contents of the message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type. Supported options
                differ based on the [model](/docs/models) being used to generate
                the response. Can contain text, image, or audio inputs.
              title: Array of content parts
              items:
                $ref: >-
                  #/components/schemas/ChatCompletionRequestUserMessageContentPart
              minItems: 1
          x-oaiExpandable: true
        role:
          type: string
          enum:
            - user
          description: The role of the messages author, in this case `user`.
          x-stainless-const: true
        name:
          type: string
          description: >-
            An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
        - content
        - role
    ChatCompletionRequestUserMessageContentPart:
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartImage"
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartAudio"
      x-oaiExpandable: true
    ChatCompletionResponseMessage:
      type: object
      description: A chat completion message generated by the model.
      properties:
        content:
          type: string
          description: The contents of the message.
          nullable: true
        refusal:
          type: string
          description: The refusal message generated by the model.
          nullable: true
        tool_calls:
          $ref: "#/components/schemas/ChatCompletionMessageToolCalls"
        role:
          type: string
          enum:
            - assistant
          description: The role of the author of this message.
          x-stainless-const: true
        audio:
          type: object
          nullable: true
          description: >
            If the audio output modality is requested, this object contains data

            about the audio response from the model. [Learn
            more](/docs/guides/audio).
          x-oaiExpandable: true
          required:
            - id
            - expires_at
            - data
            - transcript
          properties:
            id:
              type: string
              description: Unique identifier for this audio response.
            expires_at:
              type: integer
              description: >
                The Unix timestamp (in seconds) for when this audio response
                will

                no longer be accessible on the server for use in multi-turn

                conversations.
            data:
              type: string
              description: |
                Base64 encoded audio bytes generated by the model, in the format
                specified in the request.
            transcript:
              type: string
              description: Transcript of the audio generated by the model.
      required:
        - role
        - content
        - refusal
    ChatCompletionRole:
      type: string
      description: The role of the author of a message
      enum:
        - developer
        - system
        - user
        - assistant
        - tool
        - function
    ChatCompletionStreamOptions:
      description: >
        Options for streaming response. Only set this when you set `stream:
        true`.
      type: object
      nullable: true
      default: null
      properties:
        include_usage:
          type: boolean
          description: >
            If set, an additional chunk will be streamed before the `data:
            [DONE]` message. The `usage` field on this chunk shows the token
            usage statistics for the entire request, and the `choices` field
            will always be an empty array. All other chunks will also include a
            `usage` field, but with a null value.
    ChatCompletionStreamResponseDelta:
      type: object
      description: A chat completion delta generated by streamed model responses.
      properties:
        content:
          type: string
          description: The contents of the chunk message.
          nullable: true
        tool_calls:
          type: array
          items:
            $ref: "#/components/schemas/ChatCompletionMessageToolCallChunk"
        role:
          type: string
          enum:
            - developer
            - system
            - user
            - assistant
            - tool
          description: The role of the author of this message.
        refusal:
          type: string
          description: The refusal message generated by the model.
          nullable: true
    ChatCompletionTokenLogprob:
      type: object
      properties:
        token:
          description: The token.
          type: string
        logprob:
          description: >-
            The log probability of this token, if it is within the top 20 most
            likely tokens. Otherwise, the value `-9999.0` is used to signify
            that the token is very unlikely.
          type: number
        bytes:
          description: >-
            A list of integers representing the UTF-8 bytes representation of
            the token. Useful in instances where characters are represented by
            multiple tokens and their byte representations must be combined to
            generate the correct text representation. Can be `null` if there is
            no bytes representation for the token.
          type: array
          items:
            type: integer
          nullable: true
        top_logprobs:
          description: >-
            List of the most likely tokens and their log probability, at this
            token position. In rare cases, there may be fewer than the number of
            requested `top_logprobs` returned.
          type: array
          items:
            type: object
            properties:
              token:
                description: The token.
                type: string
              logprob:
                description: >-
                  The log probability of this token, if it is within the top 20
                  most likely tokens. Otherwise, the value `-9999.0` is used to
                  signify that the token is very unlikely.
                type: number
              bytes:
                description: >-
                  A list of integers representing the UTF-8 bytes representation
                  of the token. Useful in instances where characters are
                  represented by multiple tokens and their byte representations
                  must be combined to generate the correct text representation.
                  Can be `null` if there is no bytes representation for the
                  token.
                type: array
                items:
                  type: integer
                nullable: true
            required:
              - token
              - logprob
              - bytes
      required:
        - token
        - logprob
        - bytes
        - top_logprobs
    ChatCompletionTool:
      type: object
      properties:
        type:
          type: string
          enum:
            - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          $ref: "#/components/schemas/FunctionObject"
      required:
        - type
        - function
    ChatCompletionToolChoiceOption:
      description: >
        Controls which (if any) tool is called by the model.

        `none` means the model will not call any tool and instead generates a
        message.

        `auto` means the model can pick between generating a message or calling
        one or more tools.

        `required` means the model must call one or more tools.

        Specifying a particular tool via `{"type": "function", "function":
        {"name": "my_function"}}` forces the model to call that tool.


        `none` is the default when no tools are present. `auto` is the default
        if tools are present.
      oneOf:
        - type: string
          description: >
            `none` means the model will not call any tool and instead generates
            a message. `auto` means the model can pick between generating a
            message or calling one or more tools. `required` means the model
            must call one or more tools.
          enum:
            - none
            - auto
            - required
        - $ref: "#/components/schemas/ChatCompletionNamedToolChoice"
      x-oaiExpandable: true
    ChunkingStrategyRequestParam:
      type: object
      description: >-
        The chunking strategy used to chunk the file(s). If not set, will use
        the `auto` strategy.
      oneOf:
        - $ref: "#/components/schemas/AutoChunkingStrategyRequestParam"
        - $ref: "#/components/schemas/StaticChunkingStrategyRequestParam"
      x-oaiExpandable: true
    CompleteUploadRequest:
      type: object
      additionalProperties: false
      properties:
        part_ids:
          type: array
          description: |
            The ordered list of Part IDs.
          items:
            type: string
        md5:
          description: >
            The optional md5 checksum for the file contents to verify if the
            bytes uploaded matches what you expect.
          type: string
      required:
        - part_ids
    CompletionUsage:
      type: object
      description: Usage statistics for the completion request.
      properties:
        completion_tokens:
          type: integer
          default: 0
          description: Number of tokens in the generated completion.
        prompt_tokens:
          type: integer
          default: 0
          description: Number of tokens in the prompt.
        total_tokens:
          type: integer
          default: 0
          description: Total number of tokens used in the request (prompt + completion).
        completion_tokens_details:
          type: object
          description: Breakdown of tokens used in a completion.
          properties:
            accepted_prediction_tokens:
              type: integer
              default: 0
              description: |
                When using Predicted Outputs, the number of tokens in the
                prediction that appeared in the completion.
            audio_tokens:
              type: integer
              default: 0
              description: Audio input tokens generated by the model.
            reasoning_tokens:
              type: integer
              default: 0
              description: Tokens generated by the model for reasoning.
            rejected_prediction_tokens:
              type: integer
              default: 0
              description: >
                When using Predicted Outputs, the number of tokens in the

                prediction that did not appear in the completion. However, like

                reasoning tokens, these tokens are still counted in the total

                completion tokens for purposes of billing, output, and context
                window

                limits.
        prompt_tokens_details:
          type: object
          description: Breakdown of tokens used in the prompt.
          properties:
            audio_tokens:
              type: integer
              default: 0
              description: Audio input tokens present in the prompt.
            cached_tokens:
              type: integer
              default: 0
              description: Cached tokens present in the prompt.
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
    CostsResult:
      type: object
      description: The aggregated costs details of the specific time bucket.
      properties:
        object:
          type: string
          enum:
            - organization.costs.result
          x-stainless-const: true
        amount:
          type: object
          description: The monetary value in its associated currency.
          properties:
            value:
              type: number
              description: The numeric value of the cost.
            currency:
              type: string
              description: Lowercase ISO-4217 currency e.g. "usd"
        line_item:
          type: string
          nullable: true
          description: >-
            When `group_by=line_item`, this field provides the line item of the
            grouped costs result.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped costs result.
      required:
        - object
      x-oaiMeta:
        name: Costs object
        example: |
          {
              "object": "organization.costs.result",
              "amount": {
                "value": 0.06,
                "currency": "usd"
              },
              "line_item": "Image models",
              "project_id": "proj_abc"
          }
    CreateAssistantRequest:
      type: object
      additionalProperties: false
      properties:
        model:
          description: >
            ID of the model to use. You can use the [List
            models](/docs/api-reference/models/list) API to see all of your
            available models, or see our [Model overview](/docs/models) for
            descriptions of them.
          example: gpt-4o
          anyOf:
            - type: string
            - $ref: "#/components/schemas/AssistantSupportedModels"
          x-oaiTypeLabel: string
        name:
          description: |
            The name of the assistant. The maximum length is 256 characters.
          type: string
          nullable: true
          maxLength: 256
        description:
          description: >
            The description of the assistant. The maximum length is 512
            characters.
          type: string
          nullable: true
          maxLength: 512
        instructions:
          description: >
            The system instructions that the assistant uses. The maximum length
            is 256,000 characters.
          type: string
          nullable: true
          maxLength: 256000
        reasoning_effort:
          $ref: "#/components/schemas/ReasoningEffort"
        tools:
          description: >
            A list of tool enabled on the assistant. There can be a maximum of
            128 tools per assistant. Tools can be of types `code_interpreter`,
            `file_search`, or `function`.
          default: []
          type: array
          maxItems: 128
          items:
            oneOf:
              - $ref: "#/components/schemas/AssistantToolsCode"
              - $ref: "#/components/schemas/AssistantToolsFileSearch"
              - $ref: "#/components/schemas/AssistantToolsFunction"
            x-oaiExpandable: true
        tool_resources:
          type: object
          description: >
            A set of resources that are used by the assistant's tools. The
            resources are specific to the type of tool. For example, the
            `code_interpreter` tool requires a list of file IDs, while the
            `file_search` tool requires a list of vector store IDs.
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: >
                    A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter` tool. There can be a
                    maximum of 20 files associated with the tool.
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: >
                    The [vector store](/docs/api-reference/vector-stores/object)
                    attached to this assistant. There can be a maximum of 1
                    vector store attached to the assistant.
                  maxItems: 1
                  items:
                    type: string
                vector_stores:
                  type: array
                  description: >
                    A helper to create a [vector
                    store](/docs/api-reference/vector-stores/object) with
                    file_ids and attach it to this assistant. There can be a
                    maximum of 1 vector store attached to the assistant.
                  maxItems: 1
                  items:
                    type: object
                    properties:
                      file_ids:
                        type: array
                        description: >
                          A list of [file](/docs/api-reference/files) IDs to add
                          to the vector store. There can be a maximum of 10000
                          files in a vector store.
                        maxItems: 10000
                        items:
                          type: string
                      chunking_strategy:
                        type: object
                        description: >-
                          The chunking strategy used to chunk the file(s). If
                          not set, will use the `auto` strategy.
                        oneOf:
                          - type: object
                            title: Auto Chunking Strategy
                            description: >-
                              The default strategy. This strategy currently uses
                              a `max_chunk_size_tokens` of `800` and
                              `chunk_overlap_tokens` of `400`.
                            additionalProperties: false
                            properties:
                              type:
                                type: string
                                description: Always `auto`.
                                enum:
                                  - auto
                                x-stainless-const: true
                            required:
                              - type
                          - type: object
                            title: Static Chunking Strategy
                            additionalProperties: false
                            properties:
                              type:
                                type: string
                                description: Always `static`.
                                enum:
                                  - static
                                x-stainless-const: true
                              static:
                                type: object
                                additionalProperties: false
                                properties:
                                  max_chunk_size_tokens:
                                    type: integer
                                    minimum: 100
                                    maximum: 4096
                                    description: >-
                                      The maximum number of tokens in each
                                      chunk. The default value is `800`. The
                                      minimum value is `100` and the maximum
                                      value is `4096`.
                                  chunk_overlap_tokens:
                                    type: integer
                                    description: >
                                      The number of tokens that overlap between
                                      chunks. The default value is `400`.


                                      Note that the overlap must not exceed half
                                      of `max_chunk_size_tokens`.
                                required:
                                  - max_chunk_size_tokens
                                  - chunk_overlap_tokens
                            required:
                              - type
                              - static
                        x-oaiExpandable: true
                      metadata:
                        $ref: "#/components/schemas/Metadata"
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
        temperature:
          description: >
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: >
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with
            top_p probability mass. So 0.1 means only the tokens comprising the
            top 10% probability mass are considered.


            We generally recommend altering this or temperature but not both.
        response_format:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiResponseFormatOption"
            - nullable: true
      required:
        - model
    CreateChatCompletionFunctionResponse:
      type: object
      description: >-
        Represents a chat completion response returned by model, based on the
        provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        choices:
          type: array
          description: >-
            A list of chat completion choices. Can be more than one if `n` is
            greater than 1.
          items:
            type: object
            required:
              - finish_reason
              - index
              - message
            properties:
              finish_reason:
                type: string
                description: >
                  The reason the model stopped generating tokens. This will be
                  `stop` if the model hit a natural stop point or a provided
                  stop sequence, `length` if the maximum number of tokens
                  specified in the request was reached, `content_filter` if
                  content was omitted due to a flag from our content filters, or
                  `function_call` if the model called a function.
                enum:
                  - stop
                  - length
                  - function_call
                  - content_filter
              index:
                type: integer
                description: The index of the choice in the list of choices.
              message:
                $ref: "#/components/schemas/ChatCompletionResponseMessage"
        created:
          type: integer
          description: >-
            The Unix timestamp (in seconds) of when the chat completion was
            created.
        model:
          type: string
          description: The model used for the chat completion.
        system_fingerprint:
          type: string
          description: >
            This fingerprint represents the backend configuration that the model
            runs with.


            Can be used in conjunction with the `seed` request parameter to
            understand when backend changes have been made that might impact
            determinism.
        object:
          type: string
          description: The object type, which is always `chat.completion`.
          enum:
            - chat.completion
          x-stainless-const: true
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
        - choices
        - created
        - id
        - model
        - object
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: |
          {
            "id": "chatcmpl-abc123",
            "object": "chat.completion",
            "created": 1699896916,
            "model": "gpt-4o-mini",
            "choices": [
              {
                "index": 0,
                "message": {
                  "role": "assistant",
                  "content": null,
                  "tool_calls": [
                    {
                      "id": "call_abc123",
                      "type": "function",
                      "function": {
                        "name": "get_current_weather",
                        "arguments": "{\n\"location\": \"Boston, MA\"\n}"
                      }
                    }
                  ]
                },
                "logprobs": null,
                "finish_reason": "tool_calls"
              }
            ],
            "usage": {
              "prompt_tokens": 82,
              "completion_tokens": 17,
              "total_tokens": 99,
              "completion_tokens_details": {
                "reasoning_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
              }
            }
          }
    CreateChatCompletionImageResponse:
      type: object
      description: >-
        Represents a streamed chunk of a chat completion response returned by
        model, based on the provided input.
      x-oaiMeta:
        name: The chat completion chunk object
        group: chat
        example: |
          {
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-4o-mini",
            "system_fingerprint": "fp_44709d6fcb",
            "choices": [{
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "\n\nThis image shows a wooden boardwalk extending through a lush green marshland.",
              },
              "logprobs": null,
              "finish_reason": "stop"
            }],
            "usage": {
              "prompt_tokens": 9,
              "completion_tokens": 12,
              "total_tokens": 21,
              "completion_tokens_details": {
                "reasoning_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
              }
            }
          }
    CreateChatCompletionRequest:
      type: object
      properties:
        messages:
          description: >
            A list of messages comprising the conversation so far. Depending on
            the

            [model](/docs/models) you use, different message types (modalities)
            are

            supported, like [text](/docs/guides/text-generation),

            [images](/docs/guides/vision), and [audio](/docs/guides/audio).
          type: array
          minItems: 1
          items:
            $ref: "#/components/schemas/ChatCompletionRequestMessage"
        model:
          description: >-
            ID of the model to use. See the [model endpoint
            compatibility](/docs/models#model-endpoint-compatibility) table for
            details on which models work with the Chat API.
          example: gpt-4o
          anyOf:
            - type: string
            - type: string
              enum:
                - o3-mini
                - o3-mini-2025-01-31
                - o1
                - o1-2024-12-17
                - o1-preview
                - o1-preview-2024-09-12
                - o1-mini
                - o1-mini-2024-09-12
                - gpt-4o
                - gpt-4o-2024-11-20
                - gpt-4o-2024-08-06
                - gpt-4o-2024-05-13
                - gpt-4o-audio-preview
                - gpt-4o-audio-preview-2024-10-01
                - gpt-4o-audio-preview-2024-12-17
                - gpt-4o-mini-audio-preview
                - gpt-4o-mini-audio-preview-2024-12-17
                - chatgpt-4o-latest
                - gpt-4o-mini
                - gpt-4o-mini-2024-07-18
                - gpt-4-turbo
                - gpt-4-turbo-2024-04-09
                - gpt-4-0125-preview
                - gpt-4-turbo-preview
                - gpt-4-1106-preview
                - gpt-4-vision-preview
                - gpt-4
                - gpt-4-0314
                - gpt-4-0613
                - gpt-4-32k
                - gpt-4-32k-0314
                - gpt-4-32k-0613
                - gpt-3.5-turbo
                - gpt-3.5-turbo-16k
                - gpt-3.5-turbo-0301
                - gpt-3.5-turbo-0613
                - gpt-3.5-turbo-1106
                - gpt-3.5-turbo-0125
                - gpt-3.5-turbo-16k-0613
          x-oaiTypeLabel: string
        store:
          type: boolean
          default: false
          nullable: true
          description: >
            Whether or not to store the output of this chat completion request
            for 

            use in our [model distillation](/docs/guides/distillation) or

            [evals](/docs/guides/evals) products.
        reasoning_effort:
          $ref: "#/components/schemas/ReasoningEffort"
        metadata:
          $ref: "#/components/schemas/Metadata"
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: >
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on

            their existing frequency in the text so far, decreasing the model's

            likelihood to repeat the same line verbatim.
        logit_bias:
          type: object
          x-oaiTypeLabel: map
          default: null
          nullable: true
          additionalProperties:
            type: integer
          description: >
            Modify the likelihood of specified tokens appearing in the
            completion.


            Accepts a JSON object that maps tokens (specified by their token ID
            in the

            tokenizer) to an associated bias value from -100 to 100.
            Mathematically,

            the bias is added to the logits generated by the model prior to
            sampling.

            The exact effect will vary per model, but values between -1 and 1
            should

            decrease or increase likelihood of selection; values like -100 or
            100

            should result in a ban or exclusive selection of the relevant token.
        logprobs:
          description: >
            Whether to return log probabilities of the output tokens or not. If
            true,

            returns the log probabilities of each output token returned in the

            `content` of `message`.
          type: boolean
          default: false
          nullable: true
        top_logprobs:
          description: >
            An integer between 0 and 20 specifying the number of most likely
            tokens to

            return at each token position, each with an associated log
            probability.

            `logprobs` must be set to `true` if this parameter is used.
          type: integer
          minimum: 0
          maximum: 20
          nullable: true
        max_completion_tokens:
          description: >
            An upper bound for the number of tokens that can be generated for a
            completion, including visible output tokens and [reasoning
            tokens](/docs/guides/reasoning).
          type: integer
          nullable: true
        n:
          type: integer
          minimum: 1
          maximum: 128
          default: 1
          example: 1
          nullable: true
          description: >-
            How many chat completion choices to generate for each input message.
            Note that you will be charged based on the number of generated
            tokens across all of the choices. Keep `n` as `1` to minimize costs.
        modalities:
          $ref: "#/components/schemas/ChatCompletionModalities"
        prediction:
          nullable: true
          x-oaiExpandable: true
          description: >
            Configuration for a [Predicted
            Output](/docs/guides/predicted-outputs),

            which can greatly improve response times when large parts of the
            model

            response are known ahead of time. This is most common when you are

            regenerating a file with only minor changes to most of the content.
          oneOf:
            - $ref: "#/components/schemas/PredictionContent"
        audio:
          type: object
          nullable: true
          description: >
            Parameters for audio output. Required when audio output is requested
            with

            `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
          required:
            - voice
            - format
          x-oaiExpandable: true
          properties:
            voice:
              type: string
              enum:
                - alloy
                - ash
                - ballad
                - coral
                - echo
                - sage
                - shimmer
                - verse
              description: >
                The voice the model uses to respond. Supported voices are `ash`,
                `ballad`, `coral`, `sage`, and `verse` (also supported but not
                recommended are `alloy`, `echo`, and `shimmer`; these voices are
                less expressive).
            format:
              type: string
              enum:
                - wav
                - mp3
                - flac
                - opus
                - pcm16
              description: >
                Specifies the output audio format. Must be one of `wav`, `mp3`,
                `flac`,

                `opus`, or `pcm16`.
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: >
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on

            whether they appear in the text so far, increasing the model's
            likelihood

            to talk about new topics.
        response_format:
          description: >
            An object specifying the format that the model must output.


            Setting to `{ "type": "json_schema", "json_schema": {...} }` enables

            Structured Outputs which ensures the model will match your supplied
            JSON

            schema. Learn more in the [Structured Outputs

            guide](/docs/guides/structured-outputs).


            Setting to `{ "type": "json_object" }` enables JSON mode, which
            ensures

            the message the model generates is valid JSON.


            **Important:** when using JSON mode, you **must** also instruct the
            model

            to produce JSON yourself via a system or user message. Without this,
            the

            model may generate an unending stream of whitespace until the
            generation

            reaches the token limit, resulting in a long-running and seemingly
            "stuck"

            request. Also note that the message content may be partially cut off
            if

            `finish_reason="length"`, which indicates the generation exceeded

            `max_tokens` or the conversation exceeded the max context length.
          oneOf:
            - $ref: "#/components/schemas/ResponseFormatText"
            - $ref: "#/components/schemas/ResponseFormatJsonObject"
            - $ref: "#/components/schemas/ResponseFormatJsonSchema"
          x-oaiExpandable: true
        seed:
          type: integer
          format: int64
          nullable: true
          description: >
            This feature is in Beta.

            If specified, our system will make a best effort to sample
            deterministically, such that repeated requests with the same `seed`
            and parameters should return the same result.

            Determinism is not guaranteed, and you should refer to the
            `system_fingerprint` response parameter to monitor changes in the
            backend.
          x-oaiMeta:
            beta: true
        service_tier:
          description: >
            Specifies the latency tier to use for processing the request. This
            parameter is relevant for customers subscribed to the scale tier
            service:
              - If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
              - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
              - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarantee.
              - When not set, the default behavior is 'auto'.
          type: string
          enum:
            - auto
            - default
          nullable: true
          default: auto
        stop:
          description: |
            Up to 4 sequences where the API will stop generating further tokens.
          default: null
          oneOf:
            - type: string
              nullable: true
            - type: array
              minItems: 1
              maxItems: 4
              items:
                type: string
        stream:
          description: >
            If set, partial message deltas will be sent, like in ChatGPT. Tokens
            will be sent as data-only [server-sent
            events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data:
            [DONE]` message. [Example Python
            code](https://cookbook.openai.com/examples/how_to_stream_completions).
          type: boolean
          nullable: true
          default: false
        stream_options:
          $ref: "#/components/schemas/ChatCompletionStreamOptions"
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: >
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: >
            An alternative to sampling with temperature, called nucleus
            sampling,

            where the model considers the results of the tokens with top_p
            probability

            mass. So 0.1 means only the tokens comprising the top 10%
            probability mass

            are considered.


            We generally recommend altering this or `temperature` but not both.
        tools:
          type: array
          description: >
            A list of tools the model may call. Currently, only functions are
            supported as a tool. Use this to provide a list of functions the
            model may generate JSON inputs for. A max of 128 functions are
            supported.
          items:
            $ref: "#/components/schemas/ChatCompletionTool"
        tool_choice:
          $ref: "#/components/schemas/ChatCompletionToolChoiceOption"
        parallel_tool_calls:
          $ref: "#/components/schemas/ParallelToolCalls"
        user:
          type: string
          example: user-1234
          description: >
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect abuse. [Learn
            more](/docs/guides/safety-best-practices#end-user-ids).
      required:
        - model
        - messages
    CreateChatCompletionResponse:
      type: object
      description: >-
        Represents a chat completion response returned by model, based on the
        provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        choices:
          type: array
          description: >-
            A list of chat completion choices. Can be more than one if `n` is
            greater than 1.
          items:
            type: object
            required:
              - finish_reason
              - index
              - message
              - logprobs
            properties:
              finish_reason:
                type: string
                description: >
                  The reason the model stopped generating tokens. This will be
                  `stop` if the model hit a natural stop point or a provided
                  stop sequence,

                  `length` if the maximum number of tokens specified in the
                  request was reached,

                  `content_filter` if content was omitted due to a flag from our
                  content filters,

                  `tool_calls` if the model called a tool, or `function_call`
                  (deprecated) if the model called a function.
                enum:
                  - stop
                  - length
                  - tool_calls
                  - content_filter
                  - function_call
              index:
                type: integer
                description: The index of the choice in the list of choices.
              message:
                $ref: "#/components/schemas/ChatCompletionResponseMessage"
              logprobs:
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: >-
                      A list of message content tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: "#/components/schemas/ChatCompletionTokenLogprob"
                    nullable: true
                  refusal:
                    description: >-
                      A list of message refusal tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: "#/components/schemas/ChatCompletionTokenLogprob"
                    nullable: true
                required:
                  - content
                  - refusal
        created:
          type: integer
          description: >-
            The Unix timestamp (in seconds) of when the chat completion was
            created.
        model:
          type: string
          description: The model used for the chat completion.
        service_tier:
          description: The service tier used for processing the request.
          type: string
          enum:
            - scale
            - default
          example: scale
          nullable: true
        system_fingerprint:
          type: string
          description: >
            This fingerprint represents the backend configuration that the model
            runs with.


            Can be used in conjunction with the `seed` request parameter to
            understand when backend changes have been made that might impact
            determinism.
        object:
          type: string
          description: The object type, which is always `chat.completion`.
          enum:
            - chat.completion
          x-stainless-const: true
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
        - choices
        - created
        - id
        - model
        - object
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: |
          {
            "id": "chatcmpl-123456",
            "object": "chat.completion",
            "created": 1728933352,
            "model": "gpt-4o-2024-08-06",
            "choices": [
              {
                "index": 0,
                "message": {
                  "role": "assistant",
                  "content": "Hi there! How can I assist you today?",
                  "refusal": null
                },
                "logprobs": null,
                "finish_reason": "stop"
              }
            ],
            "usage": {
              "prompt_tokens": 19,
              "completion_tokens": 10,
              "total_tokens": 29,
              "prompt_tokens_details": {
                "cached_tokens": 0
              },
              "completion_tokens_details": {
                "reasoning_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
              }
            },
            "system_fingerprint": "fp_6b68a8204b"
          }
    CreateChatCompletionStreamResponse:
      type: object
      description: >-
        Represents a streamed chunk of a chat completion response returned by
        model, based on the provided input.
      properties:
        id:
          type: string
          description: >-
            A unique identifier for the chat completion. Each chunk has the same
            ID.
        choices:
          type: array
          description: >
            A list of chat completion choices. Can contain more than one
            elements if `n` is greater than 1. Can also be empty for the

            last chunk if you set `stream_options: {"include_usage": true}`.
          items:
            type: object
            required:
              - delta
              - finish_reason
              - index
            properties:
              delta:
                $ref: "#/components/schemas/ChatCompletionStreamResponseDelta"
              logprobs:
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: >-
                      A list of message content tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: "#/components/schemas/ChatCompletionTokenLogprob"
                    nullable: true
                  refusal:
                    description: >-
                      A list of message refusal tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: "#/components/schemas/ChatCompletionTokenLogprob"
                    nullable: true
                required:
                  - content
                  - refusal
              finish_reason:
                type: string
                description: >
                  The reason the model stopped generating tokens. This will be
                  `stop` if the model hit a natural stop point or a provided
                  stop sequence,

                  `length` if the maximum number of tokens specified in the
                  request was reached,

                  `content_filter` if content was omitted due to a flag from our
                  content filters,

                  `tool_calls` if the model called a tool, or `function_call`
                  (deprecated) if the model called a function.
                enum:
                  - stop
                  - length
                  - tool_calls
                  - content_filter
                  - function_call
                nullable: true
              index:
                type: integer
                description: The index of the choice in the list of choices.
        created:
          type: integer
          description: >-
            The Unix timestamp (in seconds) of when the chat completion was
            created. Each chunk has the same timestamp.
        model:
          type: string
          description: The model to generate the completion.
        service_tier:
          description: The service tier used for processing the request.
          type: string
          enum:
            - scale
            - default
          example: scale
          nullable: true
        system_fingerprint:
          type: string
          description: >
            This fingerprint represents the backend configuration that the model
            runs with.

            Can be used in conjunction with the `seed` request parameter to
            understand when backend changes have been made that might impact
            determinism.
        object:
          type: string
          description: The object type, which is always `chat.completion.chunk`.
          enum:
            - chat.completion.chunk
          x-stainless-const: true
        usage:
          type: object
          nullable: true
          description: >
            An optional field that will only be present when you set
            `stream_options: {"include_usage": true}` in your request.

            When present, it contains a null value except for the last chunk
            which contains the token usage statistics for the entire request.
          properties:
            completion_tokens:
              type: integer
              description: Number of tokens in the generated completion.
            prompt_tokens:
              type: integer
              description: Number of tokens in the prompt.
            total_tokens:
              type: integer
              description: >-
                Total number of tokens used in the request (prompt +
                completion).
          required:
            - prompt_tokens
            - completion_tokens
            - total_tokens
      required:
        - choices
        - created
        - id
        - model
        - object
      x-oaiMeta:
        name: The chat completion chunk object
        group: chat
        example: >
          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb",
          "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}


          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb",
          "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}


          ....


          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb",
          "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}
    CreateCompletionRequest:
      type: object
      properties:
        model:
          description: >
            ID of the model to use. You can use the [List
            models](/docs/api-reference/models/list) API to see all of your
            available models, or see our [Model overview](/docs/models) for
            descriptions of them.
          anyOf:
            - type: string
            - type: string
              enum:
                - gpt-3.5-turbo-instruct
                - davinci-002
                - babbage-002
          x-oaiTypeLabel: string
        prompt:
          description: >
            The prompt(s) to generate completions for, encoded as a string,
            array of strings, array of tokens, or array of token arrays.


            Note that <|endoftext|> is the document separator that the model
            sees during training, so if a prompt is not specified the model will
            generate as if from the beginning of a new document.
          default: <|endoftext|>
          nullable: true
          oneOf:
            - type: string
              default: ""
              example: This is a test.
            - type: array
              items:
                type: string
                default: ""
                example: This is a test.
            - type: array
              minItems: 1
              items:
                type: integer
              example: "[1212, 318, 257, 1332, 13]"
            - type: array
              minItems: 1
              items:
                type: array
                minItems: 1
                items:
                  type: integer
              example: "[[1212, 318, 257, 1332, 13]]"
        best_of:
          type: integer
          default: 1
          minimum: 0
          maximum: 20
          nullable: true
          description: >
            Generates `best_of` completions server-side and returns the "best"
            (the one with the highest log probability per token). Results cannot
            be streamed.


            When used with `n`, `best_of` controls the number of candidate
            completions and `n` specifies how many to return – `best_of` must be
            greater than `n`.


            **Note:** Because this parameter generates many completions, it can
            quickly consume your token quota. Use carefully and ensure that you
            have reasonable settings for `max_tokens` and `stop`.
        echo:
          type: boolean
          default: false
          nullable: true
          description: |
            Echo back the prompt in addition to the completion
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: >
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on their existing frequency in the text so far, decreasing the
            model's likelihood to repeat the same line verbatim.


            [See more information about frequency and presence
            penalties.](/docs/guides/text-generation)
        logit_bias:
          type: object
          x-oaiTypeLabel: map
          default: null
          nullable: true
          additionalProperties:
            type: integer
          description: >
            Modify the likelihood of specified tokens appearing in the
            completion.


            Accepts a JSON object that maps tokens (specified by their token ID
            in the GPT tokenizer) to an associated bias value from -100 to 100.
            You can use this [tokenizer tool](/tokenizer?view=bpe) to convert
            text to token IDs. Mathematically, the bias is added to the logits
            generated by the model prior to sampling. The exact effect will vary
            per model, but values between -1 and 1 should decrease or increase
            likelihood of selection; values like -100 or 100 should result in a
            ban or exclusive selection of the relevant token.


            As an example, you can pass `{"50256": -100}` to prevent the
            <|endoftext|> token from being generated.
        logprobs:
          type: integer
          minimum: 0
          maximum: 5
          default: null
          nullable: true
          description: >
            Include the log probabilities on the `logprobs` most likely output
            tokens, as well the chosen tokens. For example, if `logprobs` is 5,
            the API will return a list of the 5 most likely tokens. The API will
            always return the `logprob` of the sampled token, so there may be up
            to `logprobs+1` elements in the response.


            The maximum value for `logprobs` is 5.
        max_tokens:
          type: integer
          minimum: 0
          default: 16
          example: 16
          nullable: true
          description: >
            The maximum number of [tokens](/tokenizer) that can be generated in
            the completion.


            The token count of your prompt plus `max_tokens` cannot exceed the
            model's context length. [Example Python
            code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
            for counting tokens.
        n:
          type: integer
          minimum: 1
          maximum: 128
          default: 1
          example: 1
          nullable: true
          description: >
            How many completions to generate for each prompt.


            **Note:** Because this parameter generates many completions, it can
            quickly consume your token quota. Use carefully and ensure that you
            have reasonable settings for `max_tokens` and `stop`.
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: >
            Number between -2.0 and 2.0. Positive values penalize new tokens
            based on whether they appear in the text so far, increasing the
            model's likelihood to talk about new topics.


            [See more information about frequency and presence
            penalties.](/docs/guides/text-generation)
        seed:
          type: integer
          format: int64
          nullable: true
          description: >
            If specified, our system will make a best effort to sample
            deterministically, such that repeated requests with the same `seed`
            and parameters should return the same result.


            Determinism is not guaranteed, and you should refer to the
            `system_fingerprint` response parameter to monitor changes in the
            backend.
        stop:
          description: >
            Up to 4 sequences where the API will stop generating further tokens.
            The returned text will not contain the stop sequence.
          default: null
          nullable: true
          oneOf:
            - type: string
              default: <|endoftext|>
              example: |+

              nullable: true
            - type: array
              minItems: 1
              maxItems: 4
              items:
                type: string
                example: "[\"\\n\"]"
        stream:
          description: >
            Whether to stream back partial progress. If set, tokens will be sent
            as data-only [server-sent
            events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data:
            [DONE]` message. [Example Python
            code](https://cookbook.openai.com/examples/how_to_stream_completions).
          type: boolean
          nullable: true
          default: false
        stream_options:
          $ref: "#/components/schemas/ChatCompletionStreamOptions"
        suffix:
          description: |
            The suffix that comes after a completion of inserted text.

            This parameter is only supported for `gpt-3.5-turbo-instruct`.
          default: null
          nullable: true
          type: string
          example: test.
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: >
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.


            We generally recommend altering this or `top_p` but not both.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: >
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with
            top_p probability mass. So 0.1 means only the tokens comprising the
            top 10% probability mass are considered.


            We generally recommend altering this or `temperature` but not both.
        user:
          type: string
          example: user-1234
          description: >
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect abuse. [Learn
            more](/docs/guides/safety-best-practices#end-user-ids).
      required:
        - model
        - prompt
    CreateCompletionResponse:
      type: object
      description: >
        Represents a completion response from the API. Note: both the streamed
        and non-streamed response objects share the same shape (unlike the chat
        endpoint).
      properties:
        id:
          type: string
          description: A unique identifier for the completion.
        choices:
          type: array
          description: >-
            The list of completion choices the model generated for the input
            prompt.
          items:
            type: object
            required:
              - finish_reason
              - index
              - logprobs
              - text
            properties:
              finish_reason:
                type: string
                description: >
                  The reason the model stopped generating tokens. This will be
                  `stop` if the model hit a natural stop point or a provided
                  stop sequence,

                  `length` if the maximum number of tokens specified in the
                  request was reached,

                  or `content_filter` if content was omitted due to a flag from
                  our content filters.
                enum:
                  - stop
                  - length
                  - content_filter
              index:
                type: integer
              logprobs:
                type: object
                nullable: true
                properties:
                  text_offset:
                    type: array
                    items:
                      type: integer
                  token_logprobs:
                    type: array
                    items:
                      type: number
                  tokens:
                    type: array
                    items:
                      type: string
                  top_logprobs:
                    type: array
                    items:
                      type: object
                      additionalProperties:
                        type: number
              text:
                type: string
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the completion was created.
        model:
          type: string
          description: The model used for completion.
        system_fingerprint:
          type: string
          description: >
            This fingerprint represents the backend configuration that the model
            runs with.


            Can be used in conjunction with the `seed` request parameter to
            understand when backend changes have been made that might impact
            determinism.
        object:
          type: string
          description: The object type, which is always "text_completion"
          enum:
            - text_completion
          x-stainless-const: true
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
        - id
        - object
        - created
        - model
        - choices
      x-oaiMeta:
        name: The completion object
        legacy: true
        example: |
          {
            "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
            "object": "text_completion",
            "created": 1589478378,
            "model": "gpt-4-turbo",
            "choices": [
              {
                "text": "\n\nThis is indeed a test",
                "index": 0,
                "logprobs": null,
                "finish_reason": "length"
              }
            ],
            "usage": {
              "prompt_tokens": 5,
              "completion_tokens": 7,
              "total_tokens": 12
            }
          }
    CreateEmbeddingRequest:
      type: object
      additionalProperties: false
      properties:
        input:
          description: >
            Input text to embed, encoded as a string or array of tokens. To
            embed multiple inputs in a single request, pass an array of strings
            or array of token arrays. The input must not exceed the max input
            tokens for the model (8192 tokens for `text-embedding-ada-002`),
            cannot be an empty string, and any array must be 2048 dimensions or
            less. [Example Python
            code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
            for counting tokens. Some models may also impose a limit on total
            number of tokens summed across inputs.
          example: The quick brown fox jumped over the lazy dog
          oneOf:
            - type: string
              title: string
              description: The string that will be turned into an embedding.
              default: ""
              example: This is a test.
            - type: array
              title: array
              description: The array of strings that will be turned into an embedding.
              minItems: 1
              maxItems: 2048
              items:
                type: string
                default: ""
                example: "['This is a test.']"
            - type: array
              title: array
              description: The array of integers that will be turned into an embedding.
              minItems: 1
              maxItems: 2048
              items:
                type: integer
              example: "[1212, 318, 257, 1332, 13]"
            - type: array
              title: array
              description: >-
                The array of arrays containing integers that will be turned into
                an embedding.
              minItems: 1
              maxItems: 2048
              items:
                type: array
                minItems: 1
                items:
                  type: integer
              example: "[[1212, 318, 257, 1332, 13]]"
          x-oaiExpandable: true
        model:
          description: >
            ID of the model to use. You can use the [List
            models](/docs/api-reference/models/list) API to see all of your
            available models, or see our [Model overview](/docs/models) for
            descriptions of them.
          example: text-embedding-3-small
          anyOf:
            - type: string
            - type: string
              enum:
                - text-embedding-ada-002
                - text-embedding-3-small
                - text-embedding-3-large
          x-oaiTypeLabel: string
        encoding_format:
          description: >-
            The format to return the embeddings in. Can be either `float` or
            [`base64`](https://pypi.org/project/pybase64/).
          example: float
          default: float
          type: string
          enum:
            - float
            - base64
        dimensions:
          description: >
            The number of dimensions the resulting output embeddings should
            have. Only supported in `text-embedding-3` and later models.
          type: integer
          minimum: 1
        user:
          type: string
          example: user-1234
          description: >
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect abuse. [Learn
            more](/docs/guides/safety-best-practices#end-user-ids).
      required:
        - model
        - input
    CreateEmbeddingResponse:
      type: object
      properties:
        data:
          type: array
          description: The list of embeddings generated by the model.
          items:
            $ref: "#/components/schemas/Embedding"
        model:
          type: string
          description: The name of the model used to generate the embedding.
        object:
          type: string
          description: The object type, which is always "list".
          enum:
            - list
          x-stainless-const: true
        usage:
          type: object
          description: The usage information for the request.
          properties:
            prompt_tokens:
              type: integer
              description: The number of tokens used by the prompt.
            total_tokens:
              type: integer
              description: The total number of tokens used by the request.
          required:
            - prompt_tokens
            - total_tokens
      required:
        - object
        - model
        - data
        - usage
    CreateFileRequest:
      type: object
      additionalProperties: false
      properties:
        file:
          description: |
            The File object (not file name) to be uploaded.
          type: string
          format: binary
        purpose:
          description: >
            The intended purpose of the uploaded file.


            Use "assistants" for [Assistants](/docs/api-reference/assistants)
            and [Message](/docs/api-reference/messages) files, "vision" for
            Assistants image file inputs, "batch" for [Batch
            API](/docs/guides/batch), and "fine-tune" for
            [Fine-tuning](/docs/api-reference/fine-tuning).
          type: string
          enum:
            - assistants
            - batch
            - fine-tune
            - vision
      required:
        - file
        - purpose
    CreateFineTuningJobRequest:
      type: object
      properties:
        model:
          description: >
            The name of the model to fine-tune. You can select one of the

            [supported
            models](/docs/guides/fine-tuning#which-models-can-be-fine-tuned).
          example: gpt-4o-mini
          anyOf:
            - type: string
            - type: string
              enum:
                - babbage-002
                - davinci-002
                - gpt-3.5-turbo
                - gpt-4o-mini
          x-oaiTypeLabel: string
        training_file:
          description: >
            The ID of an uploaded file that contains training data.


            See [upload file](/docs/api-reference/files/create) for how to
            upload a file.


            Your dataset must be formatted as a JSONL file. Additionally, you
            must upload your file with the purpose `fine-tune`.


            The contents of the file should differ depending on if the model
            uses the [chat](/docs/api-reference/fine-tuning/chat-input),
            [completions](/docs/api-reference/fine-tuning/completions-input)
            format, or if the fine-tuning method uses the
            [preference](/docs/api-reference/fine-tuning/preference-input)
            format.


            See the [fine-tuning guide](/docs/guides/fine-tuning) for more
            details.
          type: string
          example: file-abc123
        suffix:
          description: >
            A string of up to 64 characters that will be added to your
            fine-tuned model name.


            For example, a `suffix` of "custom-model-name" would produce a model
            name like `ft:gpt-4o-mini:openai:custom-model-name:7p4lURel`.
          type: string
          minLength: 1
          maxLength: 64
          default: null
          nullable: true
        validation_file:
          description: >
            The ID of an uploaded file that contains validation data.


            If you provide this file, the data is used to generate validation

            metrics periodically during fine-tuning. These metrics can be viewed
            in

            the fine-tuning results file.

            The same data should not be present in both train and validation
            files.


            Your dataset must be formatted as a JSONL file. You must upload your
            file with the purpose `fine-tune`.


            See the [fine-tuning guide](/docs/guides/fine-tuning) for more
            details.
          type: string
          nullable: true
          example: file-abc123
        integrations:
          type: array
          description: A list of integrations to enable for your fine-tuning job.
          nullable: true
          items:
            type: object
            required:
              - type
              - wandb
            properties:
              type:
                description: >
                  The type of integration to enable. Currently, only "wandb"
                  (Weights and Biases) is supported.
                oneOf:
                  - type: string
                    enum:
                      - wandb
                    x-stainless-const: true
              wandb:
                type: object
                description: >
                  The settings for your integration with Weights and Biases.
                  This payload specifies the project that

                  metrics will be sent to. Optionally, you can set an explicit
                  display name for your run, add tags

                  to your run, and set a default entity (team, username, etc) to
                  be associated with your run.
                required:
                  - project
                properties:
                  project:
                    description: >
                      The name of the project that the new run will be created
                      under.
                    type: string
                    example: my-wandb-project
                  name:
                    description: >
                      A display name to set for the run. If not set, we will use
                      the Job ID as the name.
                    nullable: true
                    type: string
                  entity:
                    description: >
                      The entity to use for the run. This allows you to set the
                      team or username of the WandB user that you would

                      like associated with the run. If not set, the default
                      entity for the registered WandB API key is used.
                    nullable: true
                    type: string
                  tags:
                    description: >
                      A list of tags to be attached to the newly created run.
                      These tags are passed through directly to WandB. Some

                      default tags are generated by OpenAI: "openai/finetune",
                      "openai/{base-model}", "openai/{ftjob-abcdef}".
                    type: array
                    items:
                      type: string
                      example: custom-tag
        seed:
          description: >
            The seed controls the reproducibility of the job. Passing in the
            same seed and job parameters should produce the same results, but
            may differ in rare cases.

            If a seed is not specified, one will be generated for you.
          type: integer
          nullable: true
          minimum: 0
          maximum: 2147483647
          example: 42
        method:
          $ref: "#/components/schemas/FineTuneMethod"
      required:
        - model
        - training_file
    CreateImageEditRequest:
      type: object
      properties:
        image:
          description: >-
            The image to edit. Must be a valid PNG file, less than 4MB, and
            square. If mask is not provided, image must have transparency, which
            will be used as the mask.
          type: string
          format: binary
        prompt:
          description: >-
            A text description of the desired image(s). The maximum length is
            1000 characters.
          type: string
          example: A cute baby sea otter wearing a beret
        mask:
          description: >-
            An additional image whose fully transparent areas (e.g. where alpha
            is zero) indicate where `image` should be edited. Must be a valid
            PNG file, less than 4MB, and have the same dimensions as `image`.
          type: string
          format: binary
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - dall-e-2
              x-stainless-const: true
          x-oaiTypeLabel: string
          default: dall-e-2
          example: dall-e-2
          nullable: true
          description: >-
            The model to use for image generation. Only `dall-e-2` is supported
            at this time.
        n:
          type: integer
          minimum: 1
          maximum: 10
          default: 1
          example: 1
          nullable: true
          description: The number of images to generate. Must be between 1 and 10.
        size:
          type: string
          enum:
            - 256x256
            - 512x512
            - 1024x1024
          default: 1024x1024
          example: 1024x1024
          nullable: true
          description: >-
            The size of the generated images. Must be one of `256x256`,
            `512x512`, or `1024x1024`.
        response_format:
          type: string
          enum:
            - url
            - b64_json
          default: url
          example: url
          nullable: true
          description: >-
            The format in which the generated images are returned. Must be one
            of `url` or `b64_json`. URLs are only valid for 60 minutes after the
            image has been generated.
        user:
          type: string
          example: user-1234
          description: >
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect abuse. [Learn
            more](/docs/guides/safety-best-practices#end-user-ids).
      required:
        - prompt
        - image
    CreateImageRequest:
      type: object
      properties:
        prompt:
          description: >-
            A text description of the desired image(s). The maximum length is
            1000 characters for `dall-e-2` and 4000 characters for `dall-e-3`.
          type: string
          example: A cute baby sea otter
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - dall-e-2
                - dall-e-3
          x-oaiTypeLabel: string
          default: dall-e-2
          example: dall-e-3
          nullable: true
          description: The model to use for image generation.
        n:
          type: integer
          minimum: 1
          maximum: 10
          default: 1
          example: 1
          nullable: true
          description: >-
            The number of images to generate. Must be between 1 and 10. For
            `dall-e-3`, only `n=1` is supported.
        quality:
          type: string
          enum:
            - standard
            - hd
          default: standard
          example: standard
          description: >-
            The quality of the image that will be generated. `hd` creates images
            with finer details and greater consistency across the image. This
            param is only supported for `dall-e-3`.
        response_format:
          type: string
          enum:
            - url
            - b64_json
          default: url
          example: url
          nullable: true
          description: >-
            The format in which the generated images are returned. Must be one
            of `url` or `b64_json`. URLs are only valid for 60 minutes after the
            image has been generated.
        size:
          type: string
          enum:
            - 256x256
            - 512x512
            - 1024x1024
            - 1792x1024
            - 1024x1792
          default: 1024x1024
          example: 1024x1024
          nullable: true
          description: >-
            The size of the generated images. Must be one of `256x256`,
            `512x512`, or `1024x1024` for `dall-e-2`. Must be one of
            `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models.
        style:
          type: string
          enum:
            - vivid
            - natural
          default: vivid
          example: vivid
          nullable: true
          description: >-
            The style of the generated images. Must be one of `vivid` or
            `natural`. Vivid causes the model to lean towards generating
            hyper-real and dramatic images. Natural causes the model to produce
            more natural, less hyper-real looking images. This param is only
            supported for `dall-e-3`.
        user:
          type: string
          example: user-1234
          description: >
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect abuse. [Learn
            more](/docs/guides/safety-best-practices#end-user-ids).
      required:
        - prompt
    CreateImageVariationRequest:
      type: object
      properties:
        image:
          description: >-
            The image to use as the basis for the variation(s). Must be a valid
            PNG file, less than 4MB, and square.
          type: string
          format: binary
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - dall-e-2
              x-stainless-const: true
          x-oaiTypeLabel: string
          default: dall-e-2
          example: dall-e-2
          nullable: true
          description: >-
            The model to use for image generation. Only `dall-e-2` is supported
            at this time.
        n:
          type: integer
          minimum: 1
          maximum: 10
          default: 1
          example: 1
          nullable: true
          description: >-
            The number of images to generate. Must be between 1 and 10. For
            `dall-e-3`, only `n=1` is supported.
        response_format:
          type: string
          enum:
            - url
            - b64_json
          default: url
          example: url
          nullable: true
          description: >-
            The format in which the generated images are returned. Must be one
            of `url` or `b64_json`. URLs are only valid for 60 minutes after the
            image has been generated.
        size:
          type: string
          enum:
            - 256x256
            - 512x512
            - 1024x1024
          default: 1024x1024
          example: 1024x1024
          nullable: true
          description: >-
            The size of the generated images. Must be one of `256x256`,
            `512x512`, or `1024x1024`.
        user:
          type: string
          example: user-1234
          description: >
            A unique identifier representing your end-user, which can help
            OpenAI to monitor and detect abuse. [Learn
            more](/docs/guides/safety-best-practices#end-user-ids).
      required:
        - image
    CreateMessageRequest:
      type: object
      additionalProperties: false
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum:
            - user
            - assistant
          description: >
            The role of the entity that is creating the message. Allowed values
            include:

            - `user`: Indicates the message is sent by an actual user and should
            be used in most cases to represent user-generated messages.

            - `assistant`: Indicates the message is generated by the assistant.
            Use this value to insert messages from the assistant into the
            conversation.
        content:
          oneOf:
            - type: string
              description: The text contents of the message.
              title: Text content
            - type: array
              description: >-
                An array of content parts with a defined type, each can be of
                type `text` or images can be passed with `image_url` or
                `image_file`. Image types are only supported on
                [Vision-compatible models](/docs/models).
              title: Array of content parts
              items:
                oneOf:
                  - $ref: "#/components/schemas/MessageContentImageFileObject"
                  - $ref: "#/components/schemas/MessageContentImageUrlObject"
                  - $ref: "#/components/schemas/MessageRequestContentTextObject"
                x-oaiExpandable: true
              minItems: 1
          x-oaiExpandable: true
        attachments:
          type: array
          items:
            type: object
            properties:
              file_id:
                type: string
                description: The ID of the file to attach to the message.
              tools:
                description: The tools to add this file to.
                type: array
                items:
                  oneOf:
                    - $ref: "#/components/schemas/AssistantToolsCode"
                    - $ref: "#/components/schemas/AssistantToolsFileSearchTypeOnly"
                  x-oaiExpandable: true
          description: >-
            A list of files attached to the message, and the tools they should
            be added to.
          required:
            - file_id
            - tools
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
    CreateModerationRequest:
      type: object
      properties:
        input:
          description: >
            Input (or inputs) to classify. Can be a single string, an array of
            strings, or

            an array of multi-modal input objects similar to other models.
          oneOf:
            - type: string
              description: A string of text to classify for moderation.
              default: ""
              example: I want to kill them.
            - type: array
              description: An array of strings to classify for moderation.
              items:
                type: string
                default: ""
                example: I want to kill them.
            - type: array
              description: An array of multi-modal inputs to the moderation model.
              items:
                x-oaiExpandable: true
                oneOf:
                  - type: object
                    description: An object describing an image to classify.
                    properties:
                      type:
                        description: Always `image_url`.
                        type: string
                        enum:
                          - image_url
                        x-stainless-const: true
                      image_url:
                        type: object
                        description: >-
                          Contains either an image URL or a data URL for a
                          base64 encoded image.
                        properties:
                          url:
                            type: string
                            description: >-
                              Either a URL of the image or the base64 encoded
                              image data.
                            format: uri
                            example: https://example.com/image.jpg
                        required:
                          - url
                    required:
                      - type
                      - image_url
                  - type: object
                    description: An object describing text to classify.
                    properties:
                      type:
                        description: Always `text`.
                        type: string
                        enum:
                          - text
                        x-stainless-const: true
                      text:
                        description: A string of text to classify.
                        type: string
                        example: I want to kill them
                    required:
                      - type
                      - text
          x-oaiExpandable: true
        model:
          description: |
            The content moderation model you would like to use. Learn more in
            [the moderation guide](/docs/guides/moderation), and learn about
            available models [here](/docs/models#moderation).
          nullable: false
          default: omni-moderation-latest
          example: omni-moderation-2024-09-26
          anyOf:
            - type: string
            - type: string
              enum:
                - omni-moderation-latest
                - omni-moderation-2024-09-26
                - text-moderation-latest
                - text-moderation-stable
          x-oaiTypeLabel: string
      required:
        - input
    CreateModerationResponse:
      type: object
      description: Represents if a given text input is potentially harmful.
      properties:
        id:
          type: string
          description: The unique identifier for the moderation request.
        model:
          type: string
          description: The model used to generate the moderation results.
        results:
          type: array
          description: A list of moderation objects.
          items:
            type: object
            properties:
              flagged:
                type: boolean
                description: Whether any of the below categories are flagged.
              categories:
                type: object
                description: A list of the categories, and whether they are flagged or not.
                properties:
                  hate:
                    type: boolean
                    description: >-
                      Content that expresses, incites, or promotes hate based on
                      race, gender, ethnicity, religion, nationality, sexual
                      orientation, disability status, or caste. Hateful content
                      aimed at non-protected groups (e.g., chess players) is
                      harassment.
                  hate/threatening:
                    type: boolean
                    description: >-
                      Hateful content that also includes violence or serious
                      harm towards the targeted group based on race, gender,
                      ethnicity, religion, nationality, sexual orientation,
                      disability status, or caste.
                  harassment:
                    type: boolean
                    description: >-
                      Content that expresses, incites, or promotes harassing
                      language towards any target.
                  harassment/threatening:
                    type: boolean
                    description: >-
                      Harassment content that also includes violence or serious
                      harm towards any target.
                  illicit:
                    type: boolean
                    nullable: true
                    description: >-
                      Content that includes instructions or advice that
                      facilitate the planning or execution of wrongdoing, or
                      that gives advice or instruction on how to commit illicit
                      acts. For example, "how to shoplift" would fit this
                      category.
                  illicit/violent:
                    type: boolean
                    nullable: true
                    description: >-
                      Content that includes instructions or advice that
                      facilitate the planning or execution of wrongdoing that
                      also includes violence, or that gives advice or
                      instruction on the procurement of any weapon.
                  self-harm:
                    type: boolean
                    description: >-
                      Content that promotes, encourages, or depicts acts of
                      self-harm, such as suicide, cutting, and eating disorders.
                  self-harm/intent:
                    type: boolean
                    description: >-
                      Content where the speaker expresses that they are engaging
                      or intend to engage in acts of self-harm, such as suicide,
                      cutting, and eating disorders.
                  self-harm/instructions:
                    type: boolean
                    description: >-
                      Content that encourages performing acts of self-harm, such
                      as suicide, cutting, and eating disorders, or that gives
                      instructions or advice on how to commit such acts.
                  sexual:
                    type: boolean
                    description: >-
                      Content meant to arouse sexual excitement, such as the
                      description of sexual activity, or that promotes sexual
                      services (excluding sex education and wellness).
                  sexual/minors:
                    type: boolean
                    description: >-
                      Sexual content that includes an individual who is under 18
                      years old.
                  violence:
                    type: boolean
                    description: Content that depicts death, violence, or physical injury.
                  violence/graphic:
                    type: boolean
                    description: >-
                      Content that depicts death, violence, or physical injury
                      in graphic detail.
                required:
                  - hate
                  - hate/threatening
                  - harassment
                  - harassment/threatening
                  - illicit
                  - illicit/violent
                  - self-harm
                  - self-harm/intent
                  - self-harm/instructions
                  - sexual
                  - sexual/minors
                  - violence
                  - violence/graphic
              category_scores:
                type: object
                description: >-
                  A list of the categories along with their scores as predicted
                  by model.
                properties:
                  hate:
                    type: number
                    description: The score for the category 'hate'.
                  hate/threatening:
                    type: number
                    description: The score for the category 'hate/threatening'.
                  harassment:
                    type: number
                    description: The score for the category 'harassment'.
                  harassment/threatening:
                    type: number
                    description: The score for the category 'harassment/threatening'.
                  illicit:
                    type: number
                    description: The score for the category 'illicit'.
                  illicit/violent:
                    type: number
                    description: The score for the category 'illicit/violent'.
                  self-harm:
                    type: number
                    description: The score for the category 'self-harm'.
                  self-harm/intent:
                    type: number
                    description: The score for the category 'self-harm/intent'.
                  self-harm/instructions:
                    type: number
                    description: The score for the category 'self-harm/instructions'.
                  sexual:
                    type: number
                    description: The score for the category 'sexual'.
                  sexual/minors:
                    type: number
                    description: The score for the category 'sexual/minors'.
                  violence:
                    type: number
                    description: The score for the category 'violence'.
                  violence/graphic:
                    type: number
                    description: The score for the category 'violence/graphic'.
                required:
                  - hate
                  - hate/threatening
                  - harassment
                  - harassment/threatening
                  - illicit
                  - illicit/violent
                  - self-harm
                  - self-harm/intent
                  - self-harm/instructions
                  - sexual
                  - sexual/minors
                  - violence
                  - violence/graphic
              category_applied_input_types:
                type: object
                description: >-
                  A list of the categories along with the input type(s) that the
                  score applies to.
                properties:
                  hate:
                    type: array
                    description: The applied input type(s) for the category 'hate'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  hate/threatening:
                    type: array
                    description: >-
                      The applied input type(s) for the category
                      'hate/threatening'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  harassment:
                    type: array
                    description: The applied input type(s) for the category 'harassment'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  harassment/threatening:
                    type: array
                    description: >-
                      The applied input type(s) for the category
                      'harassment/threatening'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  illicit:
                    type: array
                    description: The applied input type(s) for the category 'illicit'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  illicit/violent:
                    type: array
                    description: >-
                      The applied input type(s) for the category
                      'illicit/violent'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  self-harm:
                    type: array
                    description: The applied input type(s) for the category 'self-harm'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  self-harm/intent:
                    type: array
                    description: >-
                      The applied input type(s) for the category
                      'self-harm/intent'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  self-harm/instructions:
                    type: array
                    description: >-
                      The applied input type(s) for the category
                      'self-harm/instructions'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  sexual:
                    type: array
                    description: The applied input type(s) for the category 'sexual'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  sexual/minors:
                    type: array
                    description: >-
                      The applied input type(s) for the category
                      'sexual/minors'.
                    items:
                      type: string
                      enum:
                        - text
                      x-stainless-const: true
                  violence:
                    type: array
                    description: The applied input type(s) for the category 'violence'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                  violence/graphic:
                    type: array
                    description: >-
                      The applied input type(s) for the category
                      'violence/graphic'.
                    items:
                      type: string
                      enum:
                        - text
                        - image
                required:
                  - hate
                  - hate/threatening
                  - harassment
                  - harassment/threatening
                  - illicit
                  - illicit/violent
                  - self-harm
                  - self-harm/intent
                  - self-harm/instructions
                  - sexual
                  - sexual/minors
                  - violence
                  - violence/graphic
            required:
              - flagged
              - categories
              - category_scores
              - category_applied_input_types
      required:
        - id
        - model
        - results
      x-oaiMeta:
        name: The moderation object
        example: |
          {
            "id": "modr-0d9740456c391e43c445bf0f010940c7",
            "model": "omni-moderation-latest",
            "results": [
              {
                "flagged": true,
                "categories": {
                  "harassment": true,
                  "harassment/threatening": true,
                  "sexual": false,
                  "hate": false,
                  "hate/threatening": false,
                  "illicit": false,
                  "illicit/violent": false,
                  "self-harm/intent": false,
                  "self-harm/instructions": false,
                  "self-harm": false,
                  "sexual/minors": false,
                  "violence": true,
                  "violence/graphic": true
                },
                "category_scores": {
                  "harassment": 0.8189693396524255,
                  "harassment/threatening": 0.804985420696006,
                  "sexual": 1.573112165348997e-6,
                  "hate": 0.007562942636942845,
                  "hate/threatening": 0.004208854591835476,
                  "illicit": 0.030535955153511665,
                  "illicit/violent": 0.008925306722380033,
                  "self-harm/intent": 0.00023023930975076432,
                  "self-harm/instructions": 0.0002293869201073356,
                  "self-harm": 0.012598046106750154,
                  "sexual/minors": 2.212566909570261e-8,
                  "violence": 0.9999992735124786,
                  "violence/graphic": 0.843064871157054
                },
                "category_applied_input_types": {
                  "harassment": [
                    "text"
                  ],
                  "harassment/threatening": [
                    "text"
                  ],
                  "sexual": [
                    "text",
                    "image"
                  ],
                  "hate": [
                    "text"
                  ],
                  "hate/threatening": [
                    "text"
                  ],
                  "illicit": [
                    "text"
                  ],
                  "illicit/violent": [
                    "text"
                  ],
                  "self-harm/intent": [
                    "text",
                    "image"
                  ],
                  "self-harm/instructions": [
                    "text",
                    "image"
                  ],
                  "self-harm": [
                    "text",
                    "image"
                  ],
                  "sexual/minors": [
                    "text"
                  ],
                  "violence": [
                    "text",
                    "image"
                  ],
                  "violence/graphic": [
                    "text",
                    "image"
                  ]
                }
              }
            ]
          }
    CreateRunRequest:
      type: object
      additionalProperties: false
      properties:
        assistant_id:
          description: >-
            The ID of the [assistant](/docs/api-reference/assistants) to use to
            execute this run.
          type: string
        model:
          description: >-
            The ID of the [Model](/docs/api-reference/models) to be used to
            execute this run. If a value is provided here, it will override the
            model associated with the assistant. If not, the model associated
            with the assistant will be used.
          example: gpt-4o
          anyOf:
            - type: string
            - $ref: "#/components/schemas/AssistantSupportedModels"
          x-oaiTypeLabel: string
          nullable: true
        reasoning_effort:
          $ref: "#/components/schemas/ReasoningEffort"
        instructions:
          description: >-
            Overrides the
            [instructions](/docs/api-reference/assistants/createAssistant) of
            the assistant. This is useful for modifying the behavior on a
            per-run basis.
          type: string
          nullable: true
        additional_instructions:
          description: >-
            Appends additional instructions at the end of the instructions for
            the run. This is useful for modifying the behavior on a per-run
            basis without overriding other instructions.
          type: string
          nullable: true
        additional_messages:
          description: Adds additional messages to the thread before creating the run.
          type: array
          items:
            $ref: "#/components/schemas/CreateMessageRequest"
          nullable: true
        tools:
          description: >-
            Override the tools the assistant can use for this run. This is
            useful for modifying the behavior on a per-run basis.
          nullable: true
          type: array
          maxItems: 20
          items:
            oneOf:
              - $ref: "#/components/schemas/AssistantToolsCode"
              - $ref: "#/components/schemas/AssistantToolsFileSearch"
              - $ref: "#/components/schemas/AssistantToolsFunction"
            x-oaiExpandable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: >
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: >
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with
            top_p probability mass. So 0.1 means only the tokens comprising the
            top 10% probability mass are considered.


            We generally recommend altering this or temperature but not both.
        stream:
          type: boolean
          nullable: true
          description: >
            If `true`, returns a stream of events that happen during the Run as
            server-sent events, terminating when the Run enters a terminal state
            with a `data: [DONE]` message.
        max_prompt_tokens:
          type: integer
          nullable: true
          description: >
            The maximum number of prompt tokens that may be used over the course
            of the run. The run will make a best effort to use only the number
            of prompt tokens specified, across multiple turns of the run. If the
            run exceeds the number of prompt tokens specified, the run will end
            with status `incomplete`. See `incomplete_details` for more info.
          minimum: 256
        max_completion_tokens:
          type: integer
          nullable: true
          description: >
            The maximum number of completion tokens that may be used over the
            course of the run. The run will make a best effort to use only the
            number of completion tokens specified, across multiple turns of the
            run. If the run exceeds the number of completion tokens specified,
            the run will end with status `incomplete`. See `incomplete_details`
            for more info.
          minimum: 256
        truncation_strategy:
          allOf:
            - $ref: "#/components/schemas/TruncationObject"
            - nullable: true
        tool_choice:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiToolChoiceOption"
            - nullable: true
        parallel_tool_calls:
          $ref: "#/components/schemas/ParallelToolCalls"
        response_format:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiResponseFormatOption"
            - nullable: true
      required:
        - assistant_id
    CreateSpeechRequest:
      type: object
      additionalProperties: false
      properties:
        model:
          description: >
            One of the available [TTS models](/docs/models#tts): `tts-1` or
            `tts-1-hd`
          anyOf:
            - type: string
            - type: string
              enum:
                - tts-1
                - tts-1-hd
          x-oaiTypeLabel: string
        input:
          type: string
          description: >-
            The text to generate audio for. The maximum length is 4096
            characters.
          maxLength: 4096
        voice:
          description: >-
            The voice to use when generating the audio. Supported voices are
            `alloy`, `ash`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage` and
            `shimmer`. Previews of the voices are available in the [Text to
            speech guide](/docs/guides/text-to-speech#voice-options).
          type: string
          enum:
            - alloy
            - ash
            - coral
            - echo
            - fable
            - onyx
            - nova
            - sage
            - shimmer
        response_format:
          description: >-
            The format to audio in. Supported formats are `mp3`, `opus`, `aac`,
            `flac`, `wav`, and `pcm`.
          default: mp3
          type: string
          enum:
            - mp3
            - opus
            - aac
            - flac
            - wav
            - pcm
        speed:
          description: >-
            The speed of the generated audio. Select a value from `0.25` to
            `4.0`. `1.0` is the default.
          type: number
          default: 1
          minimum: 0.25
          maximum: 4
      required:
        - model
        - input
        - voice
    CreateThreadAndRunRequest:
      type: object
      additionalProperties: false
      properties:
        assistant_id:
          description: >-
            The ID of the [assistant](/docs/api-reference/assistants) to use to
            execute this run.
          type: string
        thread:
          $ref: "#/components/schemas/CreateThreadRequest"
        model:
          description: >-
            The ID of the [Model](/docs/api-reference/models) to be used to
            execute this run. If a value is provided here, it will override the
            model associated with the assistant. If not, the model associated
            with the assistant will be used.
          example: gpt-4o
          anyOf:
            - type: string
            - type: string
              enum:
                - gpt-4o
                - gpt-4o-2024-11-20
                - gpt-4o-2024-08-06
                - gpt-4o-2024-05-13
                - gpt-4o-mini
                - gpt-4o-mini-2024-07-18
                - gpt-4-turbo
                - gpt-4-turbo-2024-04-09
                - gpt-4-0125-preview
                - gpt-4-turbo-preview
                - gpt-4-1106-preview
                - gpt-4-vision-preview
                - gpt-4
                - gpt-4-0314
                - gpt-4-0613
                - gpt-4-32k
                - gpt-4-32k-0314
                - gpt-4-32k-0613
                - gpt-3.5-turbo
                - gpt-3.5-turbo-16k
                - gpt-3.5-turbo-0613
                - gpt-3.5-turbo-1106
                - gpt-3.5-turbo-0125
                - gpt-3.5-turbo-16k-0613
          x-oaiTypeLabel: string
          nullable: true
        instructions:
          description: >-
            Override the default system message of the assistant. This is useful
            for modifying the behavior on a per-run basis.
          type: string
          nullable: true
        tools:
          description: >-
            Override the tools the assistant can use for this run. This is
            useful for modifying the behavior on a per-run basis.
          nullable: true
          type: array
          maxItems: 20
          items:
            oneOf:
              - $ref: "#/components/schemas/AssistantToolsCode"
              - $ref: "#/components/schemas/AssistantToolsFileSearch"
              - $ref: "#/components/schemas/AssistantToolsFunction"
        tool_resources:
          type: object
          description: >
            A set of resources that are used by the assistant's tools. The
            resources are specific to the type of tool. For example, the
            `code_interpreter` tool requires a list of file IDs, while the
            `file_search` tool requires a list of vector store IDs.
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: >
                    A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter` tool. There can be a
                    maximum of 20 files associated with the tool.
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: >
                    The ID of the [vector
                    store](/docs/api-reference/vector-stores/object) attached to
                    this assistant. There can be a maximum of 1 vector store
                    attached to the assistant.
                  maxItems: 1
                  items:
                    type: string
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: >
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: >
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with
            top_p probability mass. So 0.1 means only the tokens comprising the
            top 10% probability mass are considered.


            We generally recommend altering this or temperature but not both.
        stream:
          type: boolean
          nullable: true
          description: >
            If `true`, returns a stream of events that happen during the Run as
            server-sent events, terminating when the Run enters a terminal state
            with a `data: [DONE]` message.
        max_prompt_tokens:
          type: integer
          nullable: true
          description: >
            The maximum number of prompt tokens that may be used over the course
            of the run. The run will make a best effort to use only the number
            of prompt tokens specified, across multiple turns of the run. If the
            run exceeds the number of prompt tokens specified, the run will end
            with status `incomplete`. See `incomplete_details` for more info.
          minimum: 256
        max_completion_tokens:
          type: integer
          nullable: true
          description: >
            The maximum number of completion tokens that may be used over the
            course of the run. The run will make a best effort to use only the
            number of completion tokens specified, across multiple turns of the
            run. If the run exceeds the number of completion tokens specified,
            the run will end with status `incomplete`. See `incomplete_details`
            for more info.
          minimum: 256
        truncation_strategy:
          allOf:
            - $ref: "#/components/schemas/TruncationObject"
            - nullable: true
        tool_choice:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiToolChoiceOption"
            - nullable: true
        parallel_tool_calls:
          $ref: "#/components/schemas/ParallelToolCalls"
        response_format:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiResponseFormatOption"
            - nullable: true
      required:
        - assistant_id
    CreateThreadRequest:
      type: object
      description: |
        Options to create a new thread. If no thread is provided when running a 
        request, an empty thread will be created.
      additionalProperties: false
      properties:
        messages:
          description: >-
            A list of [messages](/docs/api-reference/messages) to start the
            thread with.
          type: array
          items:
            $ref: "#/components/schemas/CreateMessageRequest"
        tool_resources:
          type: object
          description: >
            A set of resources that are made available to the assistant's tools
            in this thread. The resources are specific to the type of tool. For
            example, the `code_interpreter` tool requires a list of file IDs,
            while the `file_search` tool requires a list of vector store IDs.
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: >
                    A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter` tool. There can be a
                    maximum of 20 files associated with the tool.
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: >
                    The [vector store](/docs/api-reference/vector-stores/object)
                    attached to this thread. There can be a maximum of 1 vector
                    store attached to the thread.
                  maxItems: 1
                  items:
                    type: string
                vector_stores:
                  type: array
                  description: >
                    A helper to create a [vector
                    store](/docs/api-reference/vector-stores/object) with
                    file_ids and attach it to this thread. There can be a
                    maximum of 1 vector store attached to the thread.
                  maxItems: 1
                  items:
                    type: object
                    properties:
                      file_ids:
                        type: array
                        description: >
                          A list of [file](/docs/api-reference/files) IDs to add
                          to the vector store. There can be a maximum of 10000
                          files in a vector store.
                        maxItems: 10000
                        items:
                          type: string
                      chunking_strategy:
                        type: object
                        description: >-
                          The chunking strategy used to chunk the file(s). If
                          not set, will use the `auto` strategy.
                        oneOf:
                          - type: object
                            title: Auto Chunking Strategy
                            description: >-
                              The default strategy. This strategy currently uses
                              a `max_chunk_size_tokens` of `800` and
                              `chunk_overlap_tokens` of `400`.
                            additionalProperties: false
                            properties:
                              type:
                                type: string
                                description: Always `auto`.
                                enum:
                                  - auto
                                x-stainless-const: true
                            required:
                              - type
                          - type: object
                            title: Static Chunking Strategy
                            additionalProperties: false
                            properties:
                              type:
                                type: string
                                description: Always `static`.
                                enum:
                                  - static
                                x-stainless-const: true
                              static:
                                type: object
                                additionalProperties: false
                                properties:
                                  max_chunk_size_tokens:
                                    type: integer
                                    minimum: 100
                                    maximum: 4096
                                    description: >-
                                      The maximum number of tokens in each
                                      chunk. The default value is `800`. The
                                      minimum value is `100` and the maximum
                                      value is `4096`.
                                  chunk_overlap_tokens:
                                    type: integer
                                    description: >
                                      The number of tokens that overlap between
                                      chunks. The default value is `400`.


                                      Note that the overlap must not exceed half
                                      of `max_chunk_size_tokens`.
                                required:
                                  - max_chunk_size_tokens
                                  - chunk_overlap_tokens
                            required:
                              - type
                              - static
                        x-oaiExpandable: true
                      metadata:
                        $ref: "#/components/schemas/Metadata"
                    x-oaiExpandable: true
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
    CreateTranscriptionRequest:
      type: object
      additionalProperties: false
      properties:
        file:
          description: >
            The audio file object (not file name) to transcribe, in one of these
            formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
          type: string
          x-oaiTypeLabel: file
          format: binary
        model:
          description: >
            ID of the model to use. Only `whisper-1` (which is powered by our
            open source Whisper V2 model) is currently available.
          example: whisper-1
          anyOf:
            - type: string
            - type: string
              enum:
                - whisper-1
              x-stainless-const: true
          x-oaiTypeLabel: string
        language:
          description: >
            The language of the input audio. Supplying the input language in
            [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)
            (e.g. `en`) format will improve accuracy and latency.
          type: string
        prompt:
          description: >
            An optional text to guide the model's style or continue a previous
            audio segment. The [prompt](/docs/guides/speech-to-text#prompting)
            should match the audio language.
          type: string
        response_format:
          $ref: "#/components/schemas/AudioResponseFormat"
        temperature:
          description: >
            The sampling temperature, between 0 and 1. Higher values like 0.8
            will make the output more random, while lower values like 0.2 will
            make it more focused and deterministic. If set to 0, the model will
            use [log probability](https://en.wikipedia.org/wiki/Log_probability)
            to automatically increase the temperature until certain thresholds
            are hit.
          type: number
          default: 0
        timestamp_granularities[]:
          description: >
            The timestamp granularities to populate for this transcription.
            `response_format` must be set `verbose_json` to use timestamp
            granularities. Either or both of these options are supported:
            `word`, or `segment`. Note: There is no additional latency for
            segment timestamps, but generating word timestamps incurs additional
            latency.
          type: array
          items:
            type: string
            enum:
              - word
              - segment
          default:
            - segment
      required:
        - file
        - model
    CreateTranscriptionResponseJson:
      type: object
      description: >-
        Represents a transcription response returned by model, based on the
        provided input.
      properties:
        text:
          type: string
          description: The transcribed text.
      required:
        - text
      x-oaiMeta:
        name: The transcription object (JSON)
        group: audio
        example: |
          {
            "text": "Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that."
          }
    CreateTranscriptionResponseVerboseJson:
      type: object
      description: >-
        Represents a verbose json transcription response returned by model,
        based on the provided input.
      properties:
        language:
          type: string
          description: The language of the input audio.
        duration:
          type: number
          description: The duration of the input audio.
        text:
          type: string
          description: The transcribed text.
        words:
          type: array
          description: Extracted words and their corresponding timestamps.
          items:
            $ref: "#/components/schemas/TranscriptionWord"
        segments:
          type: array
          description: Segments of the transcribed text and their corresponding details.
          items:
            $ref: "#/components/schemas/TranscriptionSegment"
      required:
        - language
        - duration
        - text
      x-oaiMeta:
        name: The transcription object (Verbose JSON)
        group: audio
        example: |
          {
            "task": "transcribe",
            "language": "english",
            "duration": 8.470000267028809,
            "text": "The beach was a popular spot on a hot summer day. People were swimming in the ocean, building sandcastles, and playing beach volleyball.",
            "segments": [
              {
                "id": 0,
                "seek": 0,
                "start": 0.0,
                "end": 3.319999933242798,
                "text": " The beach was a popular spot on a hot summer day.",
                "tokens": [
                  50364, 440, 7534, 390, 257, 3743, 4008, 322, 257, 2368, 4266, 786, 13, 50530
                ],
                "temperature": 0.0,
                "avg_logprob": -0.2860786020755768,
                "compression_ratio": 1.2363636493682861,
                "no_speech_prob": 0.00985979475080967
              },
              ...
            ]
          }
    CreateTranslationRequest:
      type: object
      additionalProperties: false
      properties:
        file:
          description: >
            The audio file object (not file name) translate, in one of these
            formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
          type: string
          x-oaiTypeLabel: file
          format: binary
        model:
          description: >
            ID of the model to use. Only `whisper-1` (which is powered by our
            open source Whisper V2 model) is currently available.
          example: whisper-1
          anyOf:
            - type: string
            - type: string
              enum:
                - whisper-1
              x-stainless-const: true
          x-oaiTypeLabel: string
        prompt:
          description: >
            An optional text to guide the model's style or continue a previous
            audio segment. The [prompt](/docs/guides/speech-to-text#prompting)
            should be in English.
          type: string
        response_format:
          $ref: "#/components/schemas/AudioResponseFormat"
        temperature:
          description: >
            The sampling temperature, between 0 and 1. Higher values like 0.8
            will make the output more random, while lower values like 0.2 will
            make it more focused and deterministic. If set to 0, the model will
            use [log probability](https://en.wikipedia.org/wiki/Log_probability)
            to automatically increase the temperature until certain thresholds
            are hit.
          type: number
          default: 0
      required:
        - file
        - model
    CreateTranslationResponseJson:
      type: object
      properties:
        text:
          type: string
      required:
        - text
    CreateTranslationResponseVerboseJson:
      type: object
      properties:
        language:
          type: string
          description: The language of the output translation (always `english`).
        duration:
          type: number
          description: The duration of the input audio.
        text:
          type: string
          description: The translated text.
        segments:
          type: array
          description: Segments of the translated text and their corresponding details.
          items:
            $ref: "#/components/schemas/TranscriptionSegment"
      required:
        - language
        - duration
        - text
    CreateUploadRequest:
      type: object
      additionalProperties: false
      properties:
        filename:
          description: |
            The name of the file to upload.
          type: string
        purpose:
          description: >
            The intended purpose of the uploaded file.


            See the [documentation on File
            purposes](/docs/api-reference/files/create#files-create-purpose).
          type: string
          enum:
            - assistants
            - batch
            - fine-tune
            - vision
        bytes:
          description: |
            The number of bytes in the file you are uploading.
          type: integer
        mime_type:
          description: >
            The MIME type of the file.


            This must fall within the supported MIME types for your file
            purpose. See the supported MIME types for assistants and vision.
          type: string
      required:
        - filename
        - purpose
        - bytes
        - mime_type
    CreateVectorStoreFileBatchRequest:
      type: object
      additionalProperties: false
      properties:
        file_ids:
          description: >-
            A list of [File](/docs/api-reference/files) IDs that the vector
            store should use. Useful for tools like `file_search` that can
            access files.
          type: array
          minItems: 1
          maxItems: 500
          items:
            type: string
        chunking_strategy:
          $ref: "#/components/schemas/ChunkingStrategyRequestParam"
      required:
        - file_ids
    CreateVectorStoreFileRequest:
      type: object
      additionalProperties: false
      properties:
        file_id:
          description: >-
            A [File](/docs/api-reference/files) ID that the vector store should
            use. Useful for tools like `file_search` that can access files.
          type: string
        chunking_strategy:
          $ref: "#/components/schemas/ChunkingStrategyRequestParam"
      required:
        - file_id
    CreateVectorStoreRequest:
      type: object
      additionalProperties: false
      properties:
        file_ids:
          description: >-
            A list of [File](/docs/api-reference/files) IDs that the vector
            store should use. Useful for tools like `file_search` that can
            access files.
          type: array
          maxItems: 500
          items:
            type: string
        name:
          description: The name of the vector store.
          type: string
        expires_after:
          $ref: "#/components/schemas/VectorStoreExpirationAfter"
        chunking_strategy:
          type: object
          description: >-
            The chunking strategy used to chunk the file(s). If not set, will
            use the `auto` strategy. Only applicable if `file_ids` is non-empty.
          oneOf:
            - $ref: "#/components/schemas/AutoChunkingStrategyRequestParam"
            - $ref: "#/components/schemas/StaticChunkingStrategyRequestParam"
          x-oaiExpandable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
    DefaultProjectErrorResponse:
      type: object
      properties:
        code:
          type: integer
        message:
          type: string
      required:
        - code
        - message
    DeleteAssistantResponse:
      type: object
      properties:
        id:
          type: string
        deleted:
          type: boolean
        object:
          type: string
          enum:
            - assistant.deleted
          x-stainless-const: true
      required:
        - id
        - object
        - deleted
    DeleteFileResponse:
      type: object
      properties:
        id:
          type: string
        object:
          type: string
          enum:
            - file
          x-stainless-const: true
        deleted:
          type: boolean
      required:
        - id
        - object
        - deleted
    DeleteMessageResponse:
      type: object
      properties:
        id:
          type: string
        deleted:
          type: boolean
        object:
          type: string
          enum:
            - thread.message.deleted
          x-stainless-const: true
      required:
        - id
        - object
        - deleted
    DeleteModelResponse:
      type: object
      properties:
        id:
          type: string
        deleted:
          type: boolean
        object:
          type: string
      required:
        - id
        - object
        - deleted
    DeleteThreadResponse:
      type: object
      properties:
        id:
          type: string
        deleted:
          type: boolean
        object:
          type: string
          enum:
            - thread.deleted
          x-stainless-const: true
      required:
        - id
        - object
        - deleted
    DeleteVectorStoreFileResponse:
      type: object
      properties:
        id:
          type: string
        deleted:
          type: boolean
        object:
          type: string
          enum:
            - vector_store.file.deleted
          x-stainless-const: true
      required:
        - id
        - object
        - deleted
    DeleteVectorStoreResponse:
      type: object
      properties:
        id:
          type: string
        deleted:
          type: boolean
        object:
          type: string
          enum:
            - vector_store.deleted
          x-stainless-const: true
      required:
        - id
        - object
        - deleted
    DoneEvent:
      type: object
      properties:
        event:
          type: string
          enum:
            - done
          x-stainless-const: true
        data:
          type: string
          enum:
            - "[DONE]"
          x-stainless-const: true
      required:
        - event
        - data
      description: Occurs when a stream ends.
      x-oaiMeta:
        dataDescription: "`data` is `[DONE]`"
    Embedding:
      type: object
      description: |
        Represents an embedding vector returned by embedding endpoint.
      properties:
        index:
          type: integer
          description: The index of the embedding in the list of embeddings.
        embedding:
          type: array
          description: >
            The embedding vector, which is a list of floats. The length of
            vector depends on the model as listed in the [embedding
            guide](/docs/guides/embeddings).
          items:
            type: number
        object:
          type: string
          description: The object type, which is always "embedding".
          enum:
            - embedding
          x-stainless-const: true
      required:
        - index
        - object
        - embedding
      x-oaiMeta:
        name: The embedding object
        example: |
          {
            "object": "embedding",
            "embedding": [
              0.0023064255,
              -0.009327292,
              .... (1536 floats total for ada-002)
              -0.0028842222,
            ],
            "index": 0
          }
    Error:
      type: object
      properties:
        code:
          type: string
          nullable: true
        message:
          type: string
          nullable: false
        param:
          type: string
          nullable: true
        type:
          type: string
          nullable: false
      required:
        - type
        - message
        - param
        - code
    ErrorEvent:
      type: object
      properties:
        event:
          type: string
          enum:
            - error
          x-stainless-const: true
        data:
          $ref: "#/components/schemas/Error"
      required:
        - event
        - data
      description: >-
        Occurs when an [error](/docs/guides/error-codes#api-errors) occurs. This
        can happen due to an internal server error or a timeout.
      x-oaiMeta:
        dataDescription: "`data` is an [error](/docs/guides/error-codes#api-errors)"
    ErrorResponse:
      type: object
      properties:
        error:
          $ref: "#/components/schemas/Error"
      required:
        - error
    FileSearchRankingOptions:
      title: File search tool call ranking options
      type: object
      description: >
        The ranking options for the file search. If not specified, the file
        search tool will use the `auto` ranker and a score_threshold of 0.


        See the [file search tool
        documentation](/docs/assistants/tools/file-search#customizing-file-search-settings)
        for more information.
      properties:
        ranker:
          type: string
          description: >-
            The ranker to use for the file search. If not specified will use the
            `auto` ranker.
          enum:
            - auto
            - default_2024_08_21
        score_threshold:
          type: number
          description: >-
            The score threshold for the file search. All values must be a
            floating point number between 0 and 1.
          minimum: 0
          maximum: 1
      required:
        - score_threshold
    FineTuneChatCompletionRequestAssistantMessage:
      allOf:
        - type: object
          title: Assistant message
          deprecated: false
          properties:
            weight:
              type: integer
              enum:
                - 1
              description: >-
                Controls whether the assistant message is trained against (0 or
                1)
        - $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage"
      required:
        - role
    FineTuneChatRequestInput:
      type: object
      description: >-
        The per-line training example of a fine-tuning input file for chat
        models using the supervised method.
      properties:
        messages:
          type: array
          minItems: 1
          items:
            oneOf:
              - $ref: "#/components/schemas/ChatCompletionRequestSystemMessage"
              - $ref: "#/components/schemas/ChatCompletionRequestUserMessage"
              - $ref: >-
                  #/components/schemas/FineTuneChatCompletionRequestAssistantMessage
              - $ref: "#/components/schemas/ChatCompletionRequestToolMessage"
            x-oaiExpandable: true
        tools:
          type: array
          description: A list of tools the model may generate JSON inputs for.
          items:
            $ref: "#/components/schemas/ChatCompletionTool"
        parallel_tool_calls:
          $ref: "#/components/schemas/ParallelToolCalls"
      x-oaiMeta:
        name: Training format for chat models using the supervised method
        example: |
          {
            "messages": [
              { "role": "user", "content": "What is the weather in San Francisco?" },
              {
                "role": "assistant",
                "tool_calls": [
                  {
                    "id": "call_id",
                    "type": "function",
                    "function": {
                      "name": "get_current_weather",
                      "arguments": "{\"location\": \"San Francisco, USA\", \"format\": \"celsius\"}"
                    }
                  }
                ]
              }
            ],
            "parallel_tool_calls": false,
            "tools": [
              {
                "type": "function",
                "function": {
                  "name": "get_current_weather",
                  "description": "Get the current weather",
                  "parameters": {
                    "type": "object",
                    "properties": {
                      "location": {
                          "type": "string",
                          "description": "The city and country, eg. San Francisco, USA"
                      },
                      "format": { "type": "string", "enum": ["celsius", "fahrenheit"] }
                    },
                    "required": ["location", "format"]
                  }
                }
              }
            ]
          }
    FineTuneCompletionRequestInput:
      type: object
      description: >-
        The per-line training example of a fine-tuning input file for
        completions models
      properties:
        prompt:
          type: string
          description: The input prompt for this training example.
        completion:
          type: string
          description: The desired completion for this training example.
      x-oaiMeta:
        name: Training format for completions models
        example: |
          {
            "prompt": "What is the answer to 2+2",
            "completion": "4"
          }
    FineTuneDPOMethod:
      type: object
      description: Configuration for the DPO fine-tuning method.
      properties:
        hyperparameters:
          type: object
          description: The hyperparameters used for the fine-tuning job.
          properties:
            beta:
              description: >
                The beta value for the DPO method. A higher beta value will
                increase the weight of the penalty between the policy and
                reference model.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: number
                  minimum: 0
                  maximum: 2
                  exclusiveMinimum: true
              default: auto
            batch_size:
              description: >
                Number of examples in each batch. A larger batch size means that
                model parameters are updated less frequently, but with lower
                variance.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: integer
                  minimum: 1
                  maximum: 256
              default: auto
            learning_rate_multiplier:
              description: >
                Scaling factor for the learning rate. A smaller learning rate
                may be useful to avoid overfitting.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: number
                  minimum: 0
                  exclusiveMinimum: true
              default: auto
            n_epochs:
              description: >
                The number of epochs to train the model for. An epoch refers to
                one full cycle through the training dataset.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: integer
                  minimum: 1
                  maximum: 50
              default: auto
    FineTuneMethod:
      type: object
      description: The method used for fine-tuning.
      properties:
        type:
          type: string
          description: The type of method. Is either `supervised` or `dpo`.
          enum:
            - supervised
            - dpo
        supervised:
          $ref: "#/components/schemas/FineTuneSupervisedMethod"
        dpo:
          $ref: "#/components/schemas/FineTuneDPOMethod"
    FineTunePreferenceRequestInput:
      type: object
      description: >-
        The per-line training example of a fine-tuning input file for chat
        models using the dpo method.
      properties:
        input:
          type: object
          properties:
            messages:
              type: array
              minItems: 1
              items:
                oneOf:
                  - $ref: "#/components/schemas/ChatCompletionRequestSystemMessage"
                  - $ref: "#/components/schemas/ChatCompletionRequestUserMessage"
                  - $ref: >-
                      #/components/schemas/FineTuneChatCompletionRequestAssistantMessage
                  - $ref: "#/components/schemas/ChatCompletionRequestToolMessage"
                x-oaiExpandable: true
            tools:
              type: array
              description: A list of tools the model may generate JSON inputs for.
              items:
                $ref: "#/components/schemas/ChatCompletionTool"
            parallel_tool_calls:
              $ref: "#/components/schemas/ParallelToolCalls"
        preferred_completion:
          type: array
          description: The preferred completion message for the output.
          maxItems: 1
          items:
            oneOf:
              - $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage"
            x-oaiExpandable: true
        non_preferred_completion:
          type: array
          description: The non-preferred completion message for the output.
          maxItems: 1
          items:
            oneOf:
              - $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage"
            x-oaiExpandable: true
      x-oaiMeta:
        name: Training format for chat models using the preference method
        example: |
          {
            "input": {
              "messages": [
                { "role": "user", "content": "What is the weather in San Francisco?" }
              ]
            },
            "preferred_completion": [
              {
                "role": "assistant",
                "content": "The weather in San Francisco is 70 degrees Fahrenheit."
              }
            ],
            "non_preferred_completion": [
              {
                "role": "assistant",
                "content": "The weather in San Francisco is 21 degrees Celsius."
              }
            ]
          }
    FineTuneSupervisedMethod:
      type: object
      description: Configuration for the supervised fine-tuning method.
      properties:
        hyperparameters:
          type: object
          description: The hyperparameters used for the fine-tuning job.
          properties:
            batch_size:
              description: >
                Number of examples in each batch. A larger batch size means that
                model parameters are updated less frequently, but with lower
                variance.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: integer
                  minimum: 1
                  maximum: 256
              default: auto
            learning_rate_multiplier:
              description: >
                Scaling factor for the learning rate. A smaller learning rate
                may be useful to avoid overfitting.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: number
                  minimum: 0
                  exclusiveMinimum: true
              default: auto
            n_epochs:
              description: >
                The number of epochs to train the model for. An epoch refers to
                one full cycle through the training dataset.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: integer
                  minimum: 1
                  maximum: 50
              default: auto
    FineTuningIntegration:
      type: object
      title: Fine-Tuning Job Integration
      required:
        - type
        - wandb
      properties:
        type:
          type: string
          description: The type of the integration being enabled for the fine-tuning job
          enum:
            - wandb
          x-stainless-const: true
        wandb:
          type: object
          description: >
            The settings for your integration with Weights and Biases. This
            payload specifies the project that

            metrics will be sent to. Optionally, you can set an explicit display
            name for your run, add tags

            to your run, and set a default entity (team, username, etc) to be
            associated with your run.
          required:
            - project
          properties:
            project:
              description: |
                The name of the project that the new run will be created under.
              type: string
              example: my-wandb-project
            name:
              description: >
                A display name to set for the run. If not set, we will use the
                Job ID as the name.
              nullable: true
              type: string
            entity:
              description: >
                The entity to use for the run. This allows you to set the team
                or username of the WandB user that you would

                like associated with the run. If not set, the default entity for
                the registered WandB API key is used.
              nullable: true
              type: string
            tags:
              description: >
                A list of tags to be attached to the newly created run. These
                tags are passed through directly to WandB. Some

                default tags are generated by OpenAI: "openai/finetune",
                "openai/{base-model}", "openai/{ftjob-abcdef}".
              type: array
              items:
                type: string
                example: custom-tag
    FineTuningJob:
      type: object
      title: FineTuningJob
      description: >
        The `fine_tuning.job` object represents a fine-tuning job that has been
        created through the API.
      properties:
        id:
          type: string
          description: The object identifier, which can be referenced in the API endpoints.
        created_at:
          type: integer
          description: >-
            The Unix timestamp (in seconds) for when the fine-tuning job was
            created.
        error:
          type: object
          nullable: true
          description: >-
            For fine-tuning jobs that have `failed`, this will contain more
            information on the cause of the failure.
          properties:
            code:
              type: string
              description: A machine-readable error code.
            message:
              type: string
              description: A human-readable error message.
            param:
              type: string
              description: >-
                The parameter that was invalid, usually `training_file` or
                `validation_file`. This field will be null if the failure was
                not parameter-specific.
              nullable: true
          required:
            - code
            - message
            - param
        fine_tuned_model:
          type: string
          nullable: true
          description: >-
            The name of the fine-tuned model that is being created. The value
            will be null if the fine-tuning job is still running.
        finished_at:
          type: integer
          nullable: true
          description: >-
            The Unix timestamp (in seconds) for when the fine-tuning job was
            finished. The value will be null if the fine-tuning job is still
            running.
        hyperparameters:
          type: object
          description: >-
            The hyperparameters used for the fine-tuning job. This value will
            only be returned when running `supervised` jobs.
          properties:
            batch_size:
              description: >
                Number of examples in each batch. A larger batch size means that
                model parameters

                are updated less frequently, but with lower variance.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: integer
                  minimum: 1
                  maximum: 256
              default: auto
            learning_rate_multiplier:
              description: >
                Scaling factor for the learning rate. A smaller learning rate
                may be useful to avoid

                overfitting.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: number
                  minimum: 0
                  exclusiveMinimum: true
              default: auto
            n_epochs:
              description: >
                The number of epochs to train the model for. An epoch refers to
                one full cycle

                through the training dataset.
              oneOf:
                - type: string
                  enum:
                    - auto
                  x-stainless-const: true
                - type: integer
                  minimum: 1
                  maximum: 50
              default: auto
        model:
          type: string
          description: The base model that is being fine-tuned.
        object:
          type: string
          description: The object type, which is always "fine_tuning.job".
          enum:
            - fine_tuning.job
          x-stainless-const: true
        organization_id:
          type: string
          description: The organization that owns the fine-tuning job.
        result_files:
          type: array
          description: >-
            The compiled results file ID(s) for the fine-tuning job. You can
            retrieve the results with the [Files
            API](/docs/api-reference/files/retrieve-contents).
          items:
            type: string
            example: file-abc123
        status:
          type: string
          description: >-
            The current status of the fine-tuning job, which can be either
            `validating_files`, `queued`, `running`, `succeeded`, `failed`, or
            `cancelled`.
          enum:
            - validating_files
            - queued
            - running
            - succeeded
            - failed
            - cancelled
        trained_tokens:
          type: integer
          nullable: true
          description: >-
            The total number of billable tokens processed by this fine-tuning
            job. The value will be null if the fine-tuning job is still running.
        training_file:
          type: string
          description: >-
            The file ID used for training. You can retrieve the training data
            with the [Files API](/docs/api-reference/files/retrieve-contents).
        validation_file:
          type: string
          nullable: true
          description: >-
            The file ID used for validation. You can retrieve the validation
            results with the [Files
            API](/docs/api-reference/files/retrieve-contents).
        integrations:
          type: array
          nullable: true
          description: A list of integrations to enable for this fine-tuning job.
          maxItems: 5
          items:
            oneOf:
              - $ref: "#/components/schemas/FineTuningIntegration"
            x-oaiExpandable: true
        seed:
          type: integer
          description: The seed used for the fine-tuning job.
        estimated_finish:
          type: integer
          nullable: true
          description: >-
            The Unix timestamp (in seconds) for when the fine-tuning job is
            estimated to finish. The value will be null if the fine-tuning job
            is not running.
        method:
          $ref: "#/components/schemas/FineTuneMethod"
      required:
        - created_at
        - error
        - finished_at
        - fine_tuned_model
        - hyperparameters
        - id
        - model
        - object
        - organization_id
        - result_files
        - status
        - trained_tokens
        - training_file
        - validation_file
        - seed
      x-oaiMeta:
        name: The fine-tuning job object
        example: |
          {
            "object": "fine_tuning.job",
            "id": "ftjob-abc123",
            "model": "davinci-002",
            "created_at": 1692661014,
            "finished_at": 1692661190,
            "fine_tuned_model": "ft:davinci-002:my-org:custom_suffix:7q8mpxmy",
            "organization_id": "org-123",
            "result_files": [
                "file-abc123"
            ],
            "status": "succeeded",
            "validation_file": null,
            "training_file": "file-abc123",
            "hyperparameters": {
                "n_epochs": 4,
                "batch_size": 1,
                "learning_rate_multiplier": 1.0
            },
            "trained_tokens": 5768,
            "integrations": [],
            "seed": 0,
            "estimated_finish": 0,
            "method": {
              "type": "supervised",
              "supervised": {
                "hyperparameters": {
                  "n_epochs": 4,
                  "batch_size": 1,
                  "learning_rate_multiplier": 1.0
                }
              }
            }
          }
    FineTuningJobCheckpoint:
      type: object
      title: FineTuningJobCheckpoint
      description: >
        The `fine_tuning.job.checkpoint` object represents a model checkpoint
        for a fine-tuning job that is ready to use.
      properties:
        id:
          type: string
          description: >-
            The checkpoint identifier, which can be referenced in the API
            endpoints.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the checkpoint was created.
        fine_tuned_model_checkpoint:
          type: string
          description: The name of the fine-tuned checkpoint model that is created.
        step_number:
          type: integer
          description: The step number that the checkpoint was created at.
        metrics:
          type: object
          description: Metrics at the step number during the fine-tuning job.
          properties:
            step:
              type: number
            train_loss:
              type: number
            train_mean_token_accuracy:
              type: number
            valid_loss:
              type: number
            valid_mean_token_accuracy:
              type: number
            full_valid_loss:
              type: number
            full_valid_mean_token_accuracy:
              type: number
        fine_tuning_job_id:
          type: string
          description: >-
            The name of the fine-tuning job that this checkpoint was created
            from.
        object:
          type: string
          description: The object type, which is always "fine_tuning.job.checkpoint".
          enum:
            - fine_tuning.job.checkpoint
          x-stainless-const: true
      required:
        - created_at
        - fine_tuning_job_id
        - fine_tuned_model_checkpoint
        - id
        - metrics
        - object
        - step_number
      x-oaiMeta:
        name: The fine-tuning job checkpoint object
        example: |
          {
            "object": "fine_tuning.job.checkpoint",
            "id": "ftckpt_qtZ5Gyk4BLq1SfLFWp3RtO3P",
            "created_at": 1712211699,
            "fine_tuned_model_checkpoint": "ft:gpt-4o-mini-2024-07-18:my-org:custom_suffix:9ABel2dg:ckpt-step-88",
            "fine_tuning_job_id": "ftjob-fpbNQ3H1GrMehXRf8cO97xTN",
            "metrics": {
              "step": 88,
              "train_loss": 0.478,
              "train_mean_token_accuracy": 0.924,
              "valid_loss": 10.112,
              "valid_mean_token_accuracy": 0.145,
              "full_valid_loss": 0.567,
              "full_valid_mean_token_accuracy": 0.944
            },
            "step_number": 88
          }
    FineTuningJobEvent:
      type: object
      description: Fine-tuning job event object
      properties:
        object:
          type: string
          description: The object type, which is always "fine_tuning.job.event".
          enum:
            - fine_tuning.job.event
          x-stainless-const: true
        id:
          type: string
          description: The object identifier.
        created_at:
          type: integer
          description: >-
            The Unix timestamp (in seconds) for when the fine-tuning job was
            created.
        level:
          type: string
          description: The log level of the event.
          enum:
            - info
            - warn
            - error
        message:
          type: string
          description: The message of the event.
        type:
          type: string
          description: The type of event.
          enum:
            - message
            - metrics
        data:
          type: object
          description: The data associated with the event.
      required:
        - id
        - object
        - created_at
        - level
        - message
      x-oaiMeta:
        name: The fine-tuning job event object
        example: |
          {
            "object": "fine_tuning.job.event",
            "id": "ftevent-abc123"
            "created_at": 1677610602,
            "level": "info",
            "message": "Created fine-tuning job",
            "data": {},
            "type": "message"
          }
    FunctionObject:
      type: object
      properties:
        description:
          type: string
          description: >-
            A description of what the function does, used by the model to choose
            when and how to call the function.
        name:
          type: string
          description: >-
            The name of the function to be called. Must be a-z, A-Z, 0-9, or
            contain underscores and dashes, with a maximum length of 64.
        parameters:
          $ref: "#/components/schemas/FunctionParameters"
        strict:
          type: boolean
          nullable: true
          default: false
          description: >-
            Whether to enable strict schema adherence when generating the
            function call. If set to true, the model will follow the exact
            schema defined in the `parameters` field. Only a subset of JSON
            Schema is supported when `strict` is `true`. Learn more about
            Structured Outputs in the [function calling
            guide](docs/guides/function-calling).
      required:
        - name
    FunctionParameters:
      type: object
      description: >-
        The parameters the functions accepts, described as a JSON Schema object.
        See the [guide](/docs/guides/function-calling) for examples, and the
        [JSON Schema
        reference](https://json-schema.org/understanding-json-schema/) for
        documentation about the format. 


        Omitting `parameters` defines a function with an empty parameter list.
      additionalProperties: true
    Image:
      type: object
      description: >-
        Represents the url or the content of an image generated by the OpenAI
        API.
      properties:
        b64_json:
          type: string
          description: >-
            The base64-encoded JSON of the generated image, if `response_format`
            is `b64_json`.
        url:
          type: string
          description: >-
            The URL of the generated image, if `response_format` is `url`
            (default).
        revised_prompt:
          type: string
          description: >-
            The prompt that was used to generate the image, if there was any
            revision to the prompt.
      x-oaiMeta:
        name: The image object
        example: |
          {
            "url": "...",
            "revised_prompt": "..."
          }
    ImagesResponse:
      properties:
        created:
          type: integer
        data:
          type: array
          items:
            $ref: "#/components/schemas/Image"
      required:
        - created
        - data
    Invite:
      type: object
      description: Represents an individual `invite` to the organization.
      properties:
        object:
          type: string
          enum:
            - organization.invite
          description: The object type, which is always `organization.invite`
          x-stainless-const: true
        id:
          type: string
          description: The identifier, which can be referenced in API endpoints
        email:
          type: string
          description: The email address of the individual to whom the invite was sent
        role:
          type: string
          enum:
            - owner
            - reader
          description: "`owner` or `reader`"
        status:
          type: string
          enum:
            - accepted
            - expired
            - pending
          description: "`accepted`,`expired`, or `pending`"
        invited_at:
          type: integer
          description: The Unix timestamp (in seconds) of when the invite was sent.
        expires_at:
          type: integer
          description: The Unix timestamp (in seconds) of when the invite expires.
        accepted_at:
          type: integer
          description: The Unix timestamp (in seconds) of when the invite was accepted.
        projects:
          type: array
          description: >-
            The projects that were granted membership upon acceptance of the
            invite.
          items:
            type: object
            properties:
              id:
                type: string
                description: Project's public ID
              role:
                type: string
                enum:
                  - member
                  - owner
                description: Project membership role
      required:
        - object
        - id
        - email
        - role
        - status
        - invited_at
        - expires_at
      x-oaiMeta:
        name: The invite object
        example: |
          {
            "object": "organization.invite",
            "id": "invite-abc",
            "email": "user@example.com",
            "role": "owner",
            "status": "accepted",
            "invited_at": 1711471533,
            "expires_at": 1711471533,
            "accepted_at": 1711471533,
            "projects": [
              {
                "id": "project-xyz",
                "role": "member"
              }
            ]
          }
    InviteDeleteResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - organization.invite.deleted
          description: The object type, which is always `organization.invite.deleted`
          x-stainless-const: true
        id:
          type: string
        deleted:
          type: boolean
      required:
        - object
        - id
        - deleted
    InviteListResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          description: The object type, which is always `list`
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/Invite"
        first_id:
          type: string
          description: The first `invite_id` in the retrieved `list`
        last_id:
          type: string
          description: The last `invite_id` in the retrieved `list`
        has_more:
          type: boolean
          description: >-
            The `has_more` property is used for pagination to indicate there are
            additional results.
      required:
        - object
        - data
    InviteRequest:
      type: object
      properties:
        email:
          type: string
          description: Send an email to this address
        role:
          type: string
          enum:
            - reader
            - owner
          description: "`owner` or `reader`"
        projects:
          type: array
          description: >-
            An array of projects to which membership is granted at the same time
            the org invite is accepted. If omitted, the user will be invited to
            the default project for compatibility with legacy behavior.
          items:
            type: object
            properties:
              id:
                type: string
                description: Project's public ID
              role:
                type: string
                enum:
                  - member
                  - owner
                description: Project membership role
            required:
              - id
              - role
      required:
        - email
        - role
    ListAssistantsResponse:
      type: object
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/AssistantObject"
        first_id:
          type: string
          example: asst_abc123
        last_id:
          type: string
          example: asst_abc456
        has_more:
          type: boolean
          example: false
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
      x-oaiMeta:
        name: List assistants response object
        group: chat
        example: |
          {
            "object": "list",
            "data": [
              {
                "id": "asst_abc123",
                "object": "assistant",
                "created_at": 1698982736,
                "name": "Coding Tutor",
                "description": null,
                "model": "gpt-4o",
                "instructions": "You are a helpful assistant designed to make me better at coding!",
                "tools": [],
                "tool_resources": {},
                "metadata": {},
                "top_p": 1.0,
                "temperature": 1.0,
                "response_format": "auto"
              },
              {
                "id": "asst_abc456",
                "object": "assistant",
                "created_at": 1698982718,
                "name": "My Assistant",
                "description": null,
                "model": "gpt-4o",
                "instructions": "You are a helpful assistant designed to make me better at coding!",
                "tools": [],
                "tool_resources": {},
                "metadata": {},
                "top_p": 1.0,
                "temperature": 1.0,
                "response_format": "auto"
              },
              {
                "id": "asst_abc789",
                "object": "assistant",
                "created_at": 1698982643,
                "name": null,
                "description": null,
                "model": "gpt-4o",
                "instructions": null,
                "tools": [],
                "tool_resources": {},
                "metadata": {},
                "top_p": 1.0,
                "temperature": 1.0,
                "response_format": "auto"
              }
            ],
            "first_id": "asst_abc123",
            "last_id": "asst_abc789",
            "has_more": false
          }
    ListAuditLogsResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/AuditLog"
        first_id:
          type: string
          example: audit_log-defb456h8dks
        last_id:
          type: string
          example: audit_log-hnbkd8s93s
        has_more:
          type: boolean
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ListBatchesResponse:
      type: object
      properties:
        data:
          type: array
          items:
            $ref: "#/components/schemas/Batch"
        first_id:
          type: string
          example: batch_abc123
        last_id:
          type: string
          example: batch_abc456
        has_more:
          type: boolean
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
      required:
        - object
        - data
        - has_more
    ListFilesResponse:
      type: object
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/OpenAIFile"
        first_id:
          type: string
          example: file-abc123
        last_id:
          type: string
          example: file-abc456
        has_more:
          type: boolean
          example: false
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ListFineTuningJobCheckpointsResponse:
      type: object
      properties:
        data:
          type: array
          items:
            $ref: "#/components/schemas/FineTuningJobCheckpoint"
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        first_id:
          type: string
          nullable: true
        last_id:
          type: string
          nullable: true
        has_more:
          type: boolean
      required:
        - object
        - data
        - has_more
    ListFineTuningJobEventsResponse:
      type: object
      properties:
        data:
          type: array
          items:
            $ref: "#/components/schemas/FineTuningJobEvent"
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        has_more:
          type: boolean
      required:
        - object
        - data
        - has_more
    ListMessagesResponse:
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/MessageObject"
        first_id:
          type: string
          example: msg_abc123
        last_id:
          type: string
          example: msg_abc123
        has_more:
          type: boolean
          example: false
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ListModelsResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/Model"
      required:
        - object
        - data
    ListPaginatedFineTuningJobsResponse:
      type: object
      properties:
        data:
          type: array
          items:
            $ref: "#/components/schemas/FineTuningJob"
        has_more:
          type: boolean
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
      required:
        - object
        - data
        - has_more
    ListRunStepsResponse:
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/RunStepObject"
        first_id:
          type: string
          example: step_abc123
        last_id:
          type: string
          example: step_abc456
        has_more:
          type: boolean
          example: false
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ListRunsResponse:
      type: object
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/RunObject"
        first_id:
          type: string
          example: run_abc123
        last_id:
          type: string
          example: run_abc456
        has_more:
          type: boolean
          example: false
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ListThreadsResponse:
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/ThreadObject"
        first_id:
          type: string
          example: asst_abc123
        last_id:
          type: string
          example: asst_abc456
        has_more:
          type: boolean
          example: false
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ListVectorStoreFilesResponse:
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/VectorStoreFileObject"
        first_id:
          type: string
          example: file-abc123
        last_id:
          type: string
          example: file-abc456
        has_more:
          type: boolean
          example: false
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ListVectorStoresResponse:
      properties:
        object:
          type: string
          example: list
        data:
          type: array
          items:
            $ref: "#/components/schemas/VectorStoreObject"
        first_id:
          type: string
          example: vs_abc123
        last_id:
          type: string
          example: vs_abc456
        has_more:
          type: boolean
          example: false
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    MessageContentImageFileObject:
      title: Image file
      type: object
      description: >-
        References an image [File](/docs/api-reference/files) in the content of
        a message.
      properties:
        type:
          description: Always `image_file`.
          type: string
          enum:
            - image_file
          x-stainless-const: true
        image_file:
          type: object
          properties:
            file_id:
              description: >-
                The [File](/docs/api-reference/files) ID of the image in the
                message content. Set `purpose="vision"` when uploading the File
                if you need to later display the file content.
              type: string
            detail:
              type: string
              description: >-
                Specifies the detail level of the image if specified by the
                user. `low` uses fewer tokens, you can opt in to high resolution
                using `high`.
              enum:
                - auto
                - low
                - high
              default: auto
          required:
            - file_id
      required:
        - type
        - image_file
    MessageContentImageUrlObject:
      title: Image URL
      type: object
      description: References an image URL in the content of a message.
      properties:
        type:
          type: string
          enum:
            - image_url
          description: The type of the content part.
          x-stainless-const: true
        image_url:
          type: object
          properties:
            url:
              type: string
              description: >-
                The external URL of the image, must be a supported image types:
                jpeg, jpg, png, gif, webp.
              format: uri
            detail:
              type: string
              description: >-
                Specifies the detail level of the image. `low` uses fewer
                tokens, you can opt in to high resolution using `high`. Default
                value is `auto`
              enum:
                - auto
                - low
                - high
              default: auto
          required:
            - url
      required:
        - type
        - image_url
    MessageContentRefusalObject:
      title: Refusal
      type: object
      description: The refusal content generated by the assistant.
      properties:
        type:
          description: Always `refusal`.
          type: string
          enum:
            - refusal
          x-stainless-const: true
        refusal:
          type: string
          nullable: false
      required:
        - type
        - refusal
    MessageContentTextAnnotationsFileCitationObject:
      title: File citation
      type: object
      description: >-
        A citation within the message that points to a specific quote from a
        specific File associated with the assistant or the message. Generated
        when the assistant uses the "file_search" tool to search files.
      properties:
        type:
          description: Always `file_citation`.
          type: string
          enum:
            - file_citation
          x-stainless-const: true
        text:
          description: The text in the message content that needs to be replaced.
          type: string
        file_citation:
          type: object
          properties:
            file_id:
              description: The ID of the specific File the citation is from.
              type: string
          required:
            - file_id
        start_index:
          type: integer
          minimum: 0
        end_index:
          type: integer
          minimum: 0
      required:
        - type
        - text
        - file_citation
        - start_index
        - end_index
    MessageContentTextAnnotationsFilePathObject:
      title: File path
      type: object
      description: >-
        A URL for the file that's generated when the assistant used the
        `code_interpreter` tool to generate a file.
      properties:
        type:
          description: Always `file_path`.
          type: string
          enum:
            - file_path
          x-stainless-const: true
        text:
          description: The text in the message content that needs to be replaced.
          type: string
        file_path:
          type: object
          properties:
            file_id:
              description: The ID of the file that was generated.
              type: string
          required:
            - file_id
        start_index:
          type: integer
          minimum: 0
        end_index:
          type: integer
          minimum: 0
      required:
        - type
        - text
        - file_path
        - start_index
        - end_index
    MessageContentTextObject:
      title: Text
      type: object
      description: The text content that is part of a message.
      properties:
        type:
          description: Always `text`.
          type: string
          enum:
            - text
          x-stainless-const: true
        text:
          type: object
          properties:
            value:
              description: The data that makes up the text.
              type: string
            annotations:
              type: array
              items:
                oneOf:
                  - $ref: >-
                      #/components/schemas/MessageContentTextAnnotationsFileCitationObject
                  - $ref: >-
                      #/components/schemas/MessageContentTextAnnotationsFilePathObject
                x-oaiExpandable: true
          required:
            - value
            - annotations
      required:
        - type
        - text
    MessageDeltaContentImageFileObject:
      title: Image file
      type: object
      description: >-
        References an image [File](/docs/api-reference/files) in the content of
        a message.
      properties:
        index:
          type: integer
          description: The index of the content part in the message.
        type:
          description: Always `image_file`.
          type: string
          enum:
            - image_file
          x-stainless-const: true
        image_file:
          type: object
          properties:
            file_id:
              description: >-
                The [File](/docs/api-reference/files) ID of the image in the
                message content. Set `purpose="vision"` when uploading the File
                if you need to later display the file content.
              type: string
            detail:
              type: string
              description: >-
                Specifies the detail level of the image if specified by the
                user. `low` uses fewer tokens, you can opt in to high resolution
                using `high`.
              enum:
                - auto
                - low
                - high
              default: auto
      required:
        - index
        - type
    MessageDeltaContentImageUrlObject:
      title: Image URL
      type: object
      description: References an image URL in the content of a message.
      properties:
        index:
          type: integer
          description: The index of the content part in the message.
        type:
          description: Always `image_url`.
          type: string
          enum:
            - image_url
          x-stainless-const: true
        image_url:
          type: object
          properties:
            url:
              description: >-
                The URL of the image, must be a supported image types: jpeg,
                jpg, png, gif, webp.
              type: string
            detail:
              type: string
              description: >-
                Specifies the detail level of the image. `low` uses fewer
                tokens, you can opt in to high resolution using `high`.
              enum:
                - auto
                - low
                - high
              default: auto
      required:
        - index
        - type
    MessageDeltaContentRefusalObject:
      title: Refusal
      type: object
      description: The refusal content that is part of a message.
      properties:
        index:
          type: integer
          description: The index of the refusal part in the message.
        type:
          description: Always `refusal`.
          type: string
          enum:
            - refusal
          x-stainless-const: true
        refusal:
          type: string
      required:
        - index
        - type
    MessageDeltaContentTextAnnotationsFileCitationObject:
      title: File citation
      type: object
      description: >-
        A citation within the message that points to a specific quote from a
        specific File associated with the assistant or the message. Generated
        when the assistant uses the "file_search" tool to search files.
      properties:
        index:
          type: integer
          description: The index of the annotation in the text content part.
        type:
          description: Always `file_citation`.
          type: string
          enum:
            - file_citation
          x-stainless-const: true
        text:
          description: The text in the message content that needs to be replaced.
          type: string
        file_citation:
          type: object
          properties:
            file_id:
              description: The ID of the specific File the citation is from.
              type: string
            quote:
              description: The specific quote in the file.
              type: string
        start_index:
          type: integer
          minimum: 0
        end_index:
          type: integer
          minimum: 0
      required:
        - index
        - type
    MessageDeltaContentTextAnnotationsFilePathObject:
      title: File path
      type: object
      description: >-
        A URL for the file that's generated when the assistant used the
        `code_interpreter` tool to generate a file.
      properties:
        index:
          type: integer
          description: The index of the annotation in the text content part.
        type:
          description: Always `file_path`.
          type: string
          enum:
            - file_path
          x-stainless-const: true
        text:
          description: The text in the message content that needs to be replaced.
          type: string
        file_path:
          type: object
          properties:
            file_id:
              description: The ID of the file that was generated.
              type: string
        start_index:
          type: integer
          minimum: 0
        end_index:
          type: integer
          minimum: 0
      required:
        - index
        - type
    MessageDeltaContentTextObject:
      title: Text
      type: object
      description: The text content that is part of a message.
      properties:
        index:
          type: integer
          description: The index of the content part in the message.
        type:
          description: Always `text`.
          type: string
          enum:
            - text
          x-stainless-const: true
        text:
          type: object
          properties:
            value:
              description: The data that makes up the text.
              type: string
            annotations:
              type: array
              items:
                oneOf:
                  - $ref: >-
                      #/components/schemas/MessageDeltaContentTextAnnotationsFileCitationObject
                  - $ref: >-
                      #/components/schemas/MessageDeltaContentTextAnnotationsFilePathObject
                x-oaiExpandable: true
      required:
        - index
        - type
    MessageDeltaObject:
      type: object
      title: Message delta object
      description: >
        Represents a message delta i.e. any changed fields on a message during
        streaming.
      properties:
        id:
          description: >-
            The identifier of the message, which can be referenced in API
            endpoints.
          type: string
        object:
          description: The object type, which is always `thread.message.delta`.
          type: string
          enum:
            - thread.message.delta
          x-stainless-const: true
        delta:
          description: The delta containing the fields that have changed on the Message.
          type: object
          properties:
            role:
              description: >-
                The entity that produced the message. One of `user` or
                `assistant`.
              type: string
              enum:
                - user
                - assistant
            content:
              description: The content of the message in array of text and/or images.
              type: array
              items:
                oneOf:
                  - $ref: "#/components/schemas/MessageDeltaContentImageFileObject"
                  - $ref: "#/components/schemas/MessageDeltaContentTextObject"
                  - $ref: "#/components/schemas/MessageDeltaContentRefusalObject"
                  - $ref: "#/components/schemas/MessageDeltaContentImageUrlObject"
                x-oaiExpandable: true
      required:
        - id
        - object
        - delta
      x-oaiMeta:
        name: The message delta object
        beta: true
        example: |
          {
            "id": "msg_123",
            "object": "thread.message.delta",
            "delta": {
              "content": [
                {
                  "index": 0,
                  "type": "text",
                  "text": { "value": "Hello", "annotations": [] }
                }
              ]
            }
          }
    MessageObject:
      type: object
      title: The message object
      description: Represents a message within a [thread](/docs/api-reference/threads).
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `thread.message`.
          type: string
          enum:
            - thread.message
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the message was created.
          type: integer
        thread_id:
          description: >-
            The [thread](/docs/api-reference/threads) ID that this message
            belongs to.
          type: string
        status:
          description: >-
            The status of the message, which can be either `in_progress`,
            `incomplete`, or `completed`.
          type: string
          enum:
            - in_progress
            - incomplete
            - completed
        incomplete_details:
          description: >-
            On an incomplete message, details about why the message is
            incomplete.
          type: object
          properties:
            reason:
              type: string
              description: The reason the message is incomplete.
              enum:
                - content_filter
                - max_tokens
                - run_cancelled
                - run_expired
                - run_failed
          nullable: true
          required:
            - reason
        completed_at:
          description: The Unix timestamp (in seconds) for when the message was completed.
          type: integer
          nullable: true
        incomplete_at:
          description: >-
            The Unix timestamp (in seconds) for when the message was marked as
            incomplete.
          type: integer
          nullable: true
        role:
          description: The entity that produced the message. One of `user` or `assistant`.
          type: string
          enum:
            - user
            - assistant
        content:
          description: The content of the message in array of text and/or images.
          type: array
          items:
            oneOf:
              - $ref: "#/components/schemas/MessageContentImageFileObject"
              - $ref: "#/components/schemas/MessageContentImageUrlObject"
              - $ref: "#/components/schemas/MessageContentTextObject"
              - $ref: "#/components/schemas/MessageContentRefusalObject"
            x-oaiExpandable: true
        assistant_id:
          description: >-
            If applicable, the ID of the
            [assistant](/docs/api-reference/assistants) that authored this
            message.
          type: string
          nullable: true
        run_id:
          description: >-
            The ID of the [run](/docs/api-reference/runs) associated with the
            creation of this message. Value is `null` when messages are created
            manually using the create message or create thread endpoints.
          type: string
          nullable: true
        attachments:
          type: array
          items:
            type: object
            properties:
              file_id:
                type: string
                description: The ID of the file to attach to the message.
              tools:
                description: The tools to add this file to.
                type: array
                items:
                  oneOf:
                    - $ref: "#/components/schemas/AssistantToolsCode"
                    - $ref: "#/components/schemas/AssistantToolsFileSearchTypeOnly"
                  x-oaiExpandable: true
          description: >-
            A list of files attached to the message, and the tools they were
            added to.
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
      required:
        - id
        - object
        - created_at
        - thread_id
        - status
        - incomplete_details
        - completed_at
        - incomplete_at
        - role
        - content
        - assistant_id
        - run_id
        - attachments
        - metadata
      x-oaiMeta:
        name: The message object
        beta: true
        example: |
          {
            "id": "msg_abc123",
            "object": "thread.message",
            "created_at": 1698983503,
            "thread_id": "thread_abc123",
            "role": "assistant",
            "content": [
              {
                "type": "text",
                "text": {
                  "value": "Hi! How can I help you today?",
                  "annotations": []
                }
              }
            ],
            "assistant_id": "asst_abc123",
            "run_id": "run_abc123",
            "attachments": [],
            "metadata": {}
          }
    MessageRequestContentTextObject:
      title: Text
      type: object
      description: The text content that is part of a message.
      properties:
        type:
          description: Always `text`.
          type: string
          enum:
            - text
          x-stainless-const: true
        text:
          type: string
          description: Text content to be sent to the model
      required:
        - type
        - text
    MessageStreamEvent:
      oneOf:
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.message.created
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/MessageObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [message](/docs/api-reference/messages/object) is
            created.
          x-oaiMeta:
            dataDescription: "`data` is a [message](/docs/api-reference/messages/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.message.in_progress
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/MessageObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [message](/docs/api-reference/messages/object) moves
            to an `in_progress` state.
          x-oaiMeta:
            dataDescription: "`data` is a [message](/docs/api-reference/messages/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.message.delta
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/MessageDeltaObject"
          required:
            - event
            - data
          description: >-
            Occurs when parts of a
            [Message](/docs/api-reference/messages/object) are being streamed.
          x-oaiMeta:
            dataDescription: >-
              `data` is a [message
              delta](/docs/api-reference/assistants-streaming/message-delta-object)
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.message.completed
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/MessageObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [message](/docs/api-reference/messages/object) is
            completed.
          x-oaiMeta:
            dataDescription: "`data` is a [message](/docs/api-reference/messages/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.message.incomplete
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/MessageObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [message](/docs/api-reference/messages/object) ends
            before it is completed.
          x-oaiMeta:
            dataDescription: "`data` is a [message](/docs/api-reference/messages/object)"
    Metadata:
      type: object
      description: >
        Set of 16 key-value pairs that can be attached to an object. This can be

        useful for storing additional information about the object in a
        structured

        format, and querying for objects via API or the dashboard. 


        Keys are strings with a maximum length of 64 characters. Values are
        strings

        with a maximum length of 512 characters.
      additionalProperties:
        type: string
      x-oaiTypeLabel: map
      nullable: true
    Model:
      title: Model
      description: Describes an OpenAI model offering that can be used with the API.
      properties:
        id:
          type: string
          description: The model identifier, which can be referenced in the API endpoints.
        created:
          type: integer
          description: The Unix timestamp (in seconds) when the model was created.
        object:
          type: string
          description: The object type, which is always "model".
          enum:
            - model
          x-stainless-const: true
        owned_by:
          type: string
          description: The organization that owns the model.
      required:
        - id
        - object
        - created
        - owned_by
      x-oaiMeta:
        name: The model object
        example: |
          {
            "id": "VAR_chat_model_id",
            "object": "model",
            "created": 1686935002,
            "owned_by": "openai"
          }
    ModifyAssistantRequest:
      type: object
      additionalProperties: false
      properties:
        model:
          description: >
            ID of the model to use. You can use the [List
            models](/docs/api-reference/models/list) API to see all of your
            available models, or see our [Model overview](/docs/models) for
            descriptions of them.
          anyOf:
            - type: string
            - $ref: "#/components/schemas/AssistantSupportedModels"
        reasoning_effort:
          $ref: "#/components/schemas/ReasoningEffort"
        name:
          description: |
            The name of the assistant. The maximum length is 256 characters.
          type: string
          nullable: true
          maxLength: 256
        description:
          description: >
            The description of the assistant. The maximum length is 512
            characters.
          type: string
          nullable: true
          maxLength: 512
        instructions:
          description: >
            The system instructions that the assistant uses. The maximum length
            is 256,000 characters.
          type: string
          nullable: true
          maxLength: 256000
        tools:
          description: >
            A list of tool enabled on the assistant. There can be a maximum of
            128 tools per assistant. Tools can be of types `code_interpreter`,
            `file_search`, or `function`.
          default: []
          type: array
          maxItems: 128
          items:
            oneOf:
              - $ref: "#/components/schemas/AssistantToolsCode"
              - $ref: "#/components/schemas/AssistantToolsFileSearch"
              - $ref: "#/components/schemas/AssistantToolsFunction"
            x-oaiExpandable: true
        tool_resources:
          type: object
          description: >
            A set of resources that are used by the assistant's tools. The
            resources are specific to the type of tool. For example, the
            `code_interpreter` tool requires a list of file IDs, while the
            `file_search` tool requires a list of vector store IDs.
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: >
                    Overrides the list of [file](/docs/api-reference/files) IDs
                    made available to the `code_interpreter` tool. There can be
                    a maximum of 20 files associated with the tool.
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: >
                    Overrides the [vector
                    store](/docs/api-reference/vector-stores/object) attached to
                    this assistant. There can be a maximum of 1 vector store
                    attached to the assistant.
                  maxItems: 1
                  items:
                    type: string
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
        temperature:
          description: >
            What sampling temperature to use, between 0 and 2. Higher values
            like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: >
            An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with
            top_p probability mass. So 0.1 means only the tokens comprising the
            top 10% probability mass are considered.


            We generally recommend altering this or temperature but not both.
        response_format:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiResponseFormatOption"
            - nullable: true
    ModifyMessageRequest:
      type: object
      additionalProperties: false
      properties:
        metadata:
          $ref: "#/components/schemas/Metadata"
    ModifyRunRequest:
      type: object
      additionalProperties: false
      properties:
        metadata:
          $ref: "#/components/schemas/Metadata"
    ModifyThreadRequest:
      type: object
      additionalProperties: false
      properties:
        tool_resources:
          type: object
          description: >
            A set of resources that are made available to the assistant's tools
            in this thread. The resources are specific to the type of tool. For
            example, the `code_interpreter` tool requires a list of file IDs,
            while the `file_search` tool requires a list of vector store IDs.
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: >
                    A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter` tool. There can be a
                    maximum of 20 files associated with the tool.
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: >
                    The [vector store](/docs/api-reference/vector-stores/object)
                    attached to this thread. There can be a maximum of 1 vector
                    store attached to the thread.
                  maxItems: 1
                  items:
                    type: string
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
    OpenAIFile:
      title: OpenAIFile
      description: >-
        The `File` object represents a document that has been uploaded to
        OpenAI.
      properties:
        id:
          type: string
          description: The file identifier, which can be referenced in the API endpoints.
        bytes:
          type: integer
          description: The size of the file, in bytes.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the file was created.
        filename:
          type: string
          description: The name of the file.
        object:
          type: string
          description: The object type, which is always `file`.
          enum:
            - file
          x-stainless-const: true
        purpose:
          type: string
          description: >-
            The intended purpose of the file. Supported values are `assistants`,
            `assistants_output`, `batch`, `batch_output`, `fine-tune`,
            `fine-tune-results` and `vision`.
          enum:
            - assistants
            - assistants_output
            - batch
            - batch_output
            - fine-tune
            - fine-tune-results
            - vision
      required:
        - id
        - object
        - bytes
        - created_at
        - filename
        - purpose
      x-oaiMeta:
        name: The file object
        example: |
          {
            "id": "file-abc123",
            "object": "file",
            "bytes": 120000,
            "created_at": 1677610602,
            "filename": "salesOverview.pdf",
            "purpose": "assistants",
          }
    OtherChunkingStrategyResponseParam:
      type: object
      title: Other Chunking Strategy
      description: >-
        This is returned when the chunking strategy is unknown. Typically, this
        is because the file was indexed before the `chunking_strategy` concept
        was introduced in the API.
      additionalProperties: false
      properties:
        type:
          type: string
          description: Always `other`.
          enum:
            - other
          x-stainless-const: true
      required:
        - type
    ParallelToolCalls:
      description: >-
        Whether to enable [parallel function
        calling](/docs/guides/function-calling#configuring-parallel-function-calling)
        during tool use.
      type: boolean
      default: true
    PredictionContent:
      type: object
      title: Static Content
      description: >
        Static predicted output content, such as the content of a text file that
        is

        being regenerated.
      required:
        - type
        - content
      properties:
        type:
          type: string
          enum:
            - content
          description: |
            The type of the predicted content you want to provide. This type is
            currently always `content`.
          x-stainless-const: true
        content:
          x-oaiExpandable: true
          description: >
            The content that should be matched when generating a model response.

            If generated tokens would match this content, the entire model
            response

            can be returned much more quickly.
          oneOf:
            - type: string
              title: Text content
              description: |
                The content used for a Predicted Output. This is often the
                text of a file you are regenerating with minor changes.
            - type: array
              description: >-
                An array of content parts with a defined type. Supported options
                differ based on the [model](/docs/models) being used to generate
                the response. Can contain text inputs.
              title: Array of content parts
              items:
                $ref: >-
                  #/components/schemas/ChatCompletionRequestMessageContentPartText
              minItems: 1
    Project:
      type: object
      description: Represents an individual project.
      properties:
        id:
          type: string
          description: The identifier, which can be referenced in API endpoints
        object:
          type: string
          enum:
            - organization.project
          description: The object type, which is always `organization.project`
          x-stainless-const: true
        name:
          type: string
          description: The name of the project. This appears in reporting.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) of when the project was created.
        archived_at:
          type: integer
          nullable: true
          description: >-
            The Unix timestamp (in seconds) of when the project was archived or
            `null`.
        status:
          type: string
          enum:
            - active
            - archived
          description: "`active` or `archived`"
      required:
        - id
        - object
        - name
        - created_at
        - status
      x-oaiMeta:
        name: The project object
        example: |
          {
              "id": "proj_abc",
              "object": "organization.project",
              "name": "Project example",
              "created_at": 1711471533,
              "archived_at": null,
              "status": "active"
          }
    ProjectApiKey:
      type: object
      description: Represents an individual API key in a project.
      properties:
        object:
          type: string
          enum:
            - organization.project.api_key
          description: The object type, which is always `organization.project.api_key`
          x-stainless-const: true
        redacted_value:
          type: string
          description: The redacted value of the API key
        name:
          type: string
          description: The name of the API key
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) of when the API key was created
        id:
          type: string
          description: The identifier, which can be referenced in API endpoints
        owner:
          type: object
          properties:
            type:
              type: string
              enum:
                - user
                - service_account
              description: "`user` or `service_account`"
            user:
              $ref: "#/components/schemas/ProjectUser"
            service_account:
              $ref: "#/components/schemas/ProjectServiceAccount"
      required:
        - object
        - redacted_value
        - name
        - created_at
        - id
        - owner
      x-oaiMeta:
        name: The project API key object
        example: |
          {
              "object": "organization.project.api_key",
              "redacted_value": "sk-abc...def",
              "name": "My API Key",
              "created_at": 1711471533,
              "id": "key_abc",
              "owner": {
                  "type": "user",
                  "user": {
                      "object": "organization.project.user",
                      "id": "user_abc",
                      "name": "First Last",
                      "email": "user@example.com",
                      "role": "owner",
                      "created_at": 1711471533
                  }
              }
          }
    ProjectApiKeyDeleteResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - organization.project.api_key.deleted
          x-stainless-const: true
        id:
          type: string
        deleted:
          type: boolean
      required:
        - object
        - id
        - deleted
    ProjectApiKeyListResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/ProjectApiKey"
        first_id:
          type: string
        last_id:
          type: string
        has_more:
          type: boolean
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ProjectCreateRequest:
      type: object
      properties:
        name:
          type: string
          description: The friendly name of the project, this name appears in reports.
      required:
        - name
    ProjectListResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/Project"
        first_id:
          type: string
        last_id:
          type: string
        has_more:
          type: boolean
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ProjectRateLimit:
      type: object
      description: Represents a project rate limit config.
      properties:
        object:
          type: string
          enum:
            - project.rate_limit
          description: The object type, which is always `project.rate_limit`
          x-stainless-const: true
        id:
          type: string
          description: The identifier, which can be referenced in API endpoints.
        model:
          type: string
          description: The model this rate limit applies to.
        max_requests_per_1_minute:
          type: integer
          description: The maximum requests per minute.
        max_tokens_per_1_minute:
          type: integer
          description: The maximum tokens per minute.
        max_images_per_1_minute:
          type: integer
          description: The maximum images per minute. Only present for relevant models.
        max_audio_megabytes_per_1_minute:
          type: integer
          description: >-
            The maximum audio megabytes per minute. Only present for relevant
            models.
        max_requests_per_1_day:
          type: integer
          description: The maximum requests per day. Only present for relevant models.
        batch_1_day_max_input_tokens:
          type: integer
          description: >-
            The maximum batch input tokens per day. Only present for relevant
            models.
      required:
        - object
        - id
        - model
        - max_requests_per_1_minute
        - max_tokens_per_1_minute
      x-oaiMeta:
        name: The project rate limit object
        example: |
          {
              "object": "project.rate_limit",
              "id": "rl_ada",
              "model": "ada",
              "max_requests_per_1_minute": 600,
              "max_tokens_per_1_minute": 150000,
              "max_images_per_1_minute": 10
          }
    ProjectRateLimitListResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/ProjectRateLimit"
        first_id:
          type: string
        last_id:
          type: string
        has_more:
          type: boolean
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ProjectRateLimitUpdateRequest:
      type: object
      properties:
        max_requests_per_1_minute:
          type: integer
          description: The maximum requests per minute.
        max_tokens_per_1_minute:
          type: integer
          description: The maximum tokens per minute.
        max_images_per_1_minute:
          type: integer
          description: The maximum images per minute. Only relevant for certain models.
        max_audio_megabytes_per_1_minute:
          type: integer
          description: >-
            The maximum audio megabytes per minute. Only relevant for certain
            models.
        max_requests_per_1_day:
          type: integer
          description: The maximum requests per day. Only relevant for certain models.
        batch_1_day_max_input_tokens:
          type: integer
          description: >-
            The maximum batch input tokens per day. Only relevant for certain
            models.
    ProjectServiceAccount:
      type: object
      description: Represents an individual service account in a project.
      properties:
        object:
          type: string
          enum:
            - organization.project.service_account
          description: >-
            The object type, which is always
            `organization.project.service_account`
          x-stainless-const: true
        id:
          type: string
          description: The identifier, which can be referenced in API endpoints
        name:
          type: string
          description: The name of the service account
        role:
          type: string
          enum:
            - owner
            - member
          description: "`owner` or `member`"
        created_at:
          type: integer
          description: >-
            The Unix timestamp (in seconds) of when the service account was
            created
      required:
        - object
        - id
        - name
        - role
        - created_at
      x-oaiMeta:
        name: The project service account object
        example: |
          {
              "object": "organization.project.service_account",
              "id": "svc_acct_abc",
              "name": "Service Account",
              "role": "owner",
              "created_at": 1711471533
          }
    ProjectServiceAccountApiKey:
      type: object
      properties:
        object:
          type: string
          enum:
            - organization.project.service_account.api_key
          description: >-
            The object type, which is always
            `organization.project.service_account.api_key`
          x-stainless-const: true
        value:
          type: string
        name:
          type: string
        created_at:
          type: integer
        id:
          type: string
      required:
        - object
        - value
        - name
        - created_at
        - id
    ProjectServiceAccountCreateRequest:
      type: object
      properties:
        name:
          type: string
          description: The name of the service account being created.
      required:
        - name
    ProjectServiceAccountCreateResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - organization.project.service_account
          x-stainless-const: true
        id:
          type: string
        name:
          type: string
        role:
          type: string
          enum:
            - member
          description: Service accounts can only have one role of type `member`
          x-stainless-const: true
        created_at:
          type: integer
        api_key:
          $ref: "#/components/schemas/ProjectServiceAccountApiKey"
      required:
        - object
        - id
        - name
        - role
        - created_at
        - api_key
    ProjectServiceAccountDeleteResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - organization.project.service_account.deleted
          x-stainless-const: true
        id:
          type: string
        deleted:
          type: boolean
      required:
        - object
        - id
        - deleted
    ProjectServiceAccountListResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/ProjectServiceAccount"
        first_id:
          type: string
        last_id:
          type: string
        has_more:
          type: boolean
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ProjectUpdateRequest:
      type: object
      properties:
        name:
          type: string
          description: The updated name of the project, this name appears in reports.
      required:
        - name
    ProjectUser:
      type: object
      description: Represents an individual user in a project.
      properties:
        object:
          type: string
          enum:
            - organization.project.user
          description: The object type, which is always `organization.project.user`
          x-stainless-const: true
        id:
          type: string
          description: The identifier, which can be referenced in API endpoints
        name:
          type: string
          description: The name of the user
        email:
          type: string
          description: The email address of the user
        role:
          type: string
          enum:
            - owner
            - member
          description: "`owner` or `member`"
        added_at:
          type: integer
          description: The Unix timestamp (in seconds) of when the project was added.
      required:
        - object
        - id
        - name
        - email
        - role
        - added_at
      x-oaiMeta:
        name: The project user object
        example: |
          {
              "object": "organization.project.user",
              "id": "user_abc",
              "name": "First Last",
              "email": "user@example.com",
              "role": "owner",
              "added_at": 1711471533
          }
    ProjectUserCreateRequest:
      type: object
      properties:
        user_id:
          type: string
          description: The ID of the user.
        role:
          type: string
          enum:
            - owner
            - member
          description: "`owner` or `member`"
      required:
        - user_id
        - role
    ProjectUserDeleteResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - organization.project.user.deleted
          x-stainless-const: true
        id:
          type: string
        deleted:
          type: boolean
      required:
        - object
        - id
        - deleted
    ProjectUserListResponse:
      type: object
      properties:
        object:
          type: string
        data:
          type: array
          items:
            $ref: "#/components/schemas/ProjectUser"
        first_id:
          type: string
        last_id:
          type: string
        has_more:
          type: boolean
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    ProjectUserUpdateRequest:
      type: object
      properties:
        role:
          type: string
          enum:
            - owner
            - member
          description: "`owner` or `member`"
      required:
        - role
    RealtimeClientEventConversationItemCreate:
      type: object
      description: >
        Add a new Item to the Conversation's context, including messages,
        function 

        calls, and function call responses. This event can be used both to
        populate a 

        "history" of the conversation and to add new items mid-stream, but has
        the 

        current limitation that it cannot populate assistant audio messages.


        If successful, the server will respond with a
        `conversation.item.created` 

        event, otherwise an `error` event will be sent.
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - conversation.item.create
          description: The event type, must be `conversation.item.create`.
          x-stainless-const: true
        previous_item_id:
          type: string
          description: >
            The ID of the preceding item after which the new item will be
            inserted. 

            If not set, the new item will be appended to the end of the
            conversation.

            If set to `root`, the new item will be added to the beginning of the
            conversation.

            If set to an existing ID, it allows an item to be inserted
            mid-conversation. If the

            ID cannot be found, an error will be returned and the item will not
            be added.
        item:
          $ref: "#/components/schemas/RealtimeConversationItem"
      required:
        - type
        - item
      x-oaiMeta:
        name: conversation.item.create
        group: realtime
        example: |
          {
              "event_id": "event_345",
              "type": "conversation.item.create",
              "previous_item_id": null,
              "item": {
                  "id": "msg_001",
                  "type": "message",
                  "role": "user",
                  "content": [
                      {
                          "type": "input_text",
                          "text": "Hello, how are you?"
                      }
                  ]
              }
          }
    RealtimeClientEventConversationItemDelete:
      type: object
      description: >
        Send this event when you want to remove any item from the conversation 

        history. The server will respond with a `conversation.item.deleted`
        event, 

        unless the item does not exist in the conversation history, in which
        case the 

        server will respond with an error.
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - conversation.item.delete
          description: The event type, must be `conversation.item.delete`.
          x-stainless-const: true
        item_id:
          type: string
          description: The ID of the item to delete.
      required:
        - type
        - item_id
      x-oaiMeta:
        name: conversation.item.delete
        group: realtime
        example: |
          {
              "event_id": "event_901",
              "type": "conversation.item.delete",
              "item_id": "msg_003"
          }
    RealtimeClientEventConversationItemTruncate:
      type: object
      description: >
        Send this event to truncate a previous assistant message’s audio. The
        server 

        will produce audio faster than realtime, so this event is useful when
        the user 

        interrupts to truncate audio that has already been sent to the client
        but not 

        yet played. This will synchronize the server's understanding of the
        audio with 

        the client's playback.


        Truncating audio will delete the server-side text transcript to ensure
        there 

        is not text in the context that hasn't been heard by the user.


        If successful, the server will respond with a
        `conversation.item.truncated` 

        event. 
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - conversation.item.truncate
          description: The event type, must be `conversation.item.truncate`.
          x-stainless-const: true
        item_id:
          type: string
          description: >
            The ID of the assistant message item to truncate. Only assistant
            message 

            items can be truncated.
        content_index:
          type: integer
          description: The index of the content part to truncate. Set this to 0.
        audio_end_ms:
          type: integer
          description: >
            Inclusive duration up to which audio is truncated, in milliseconds.
            If 

            the audio_end_ms is greater than the actual audio duration, the
            server 

            will respond with an error.
      required:
        - type
        - item_id
        - content_index
        - audio_end_ms
      x-oaiMeta:
        name: conversation.item.truncate
        group: realtime
        example: |
          {
              "event_id": "event_678",
              "type": "conversation.item.truncate",
              "item_id": "msg_002",
              "content_index": 0,
              "audio_end_ms": 1500
          }
    RealtimeClientEventInputAudioBufferAppend:
      type: object
      description: >
        Send this event to append audio bytes to the input audio buffer. The
        audio 

        buffer is temporary storage you can write to and later commit. In Server
        VAD 

        mode, the audio buffer is used to detect speech and the server will
        decide 

        when to commit. When Server VAD is disabled, you must commit the audio
        buffer

        manually.


        The client may choose how much audio to place in each event up to a
        maximum 

        of 15 MiB, for example streaming smaller chunks from the client may
        allow the 

        VAD to be more responsive. Unlike made other client events, the server
        will 

        not send a confirmation response to this event.
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - input_audio_buffer.append
          description: The event type, must be `input_audio_buffer.append`.
          x-stainless-const: true
        audio:
          type: string
          description: >
            Base64-encoded audio bytes. This must be in the format specified by
            the 

            `input_audio_format` field in the session configuration.
      required:
        - type
        - audio
      x-oaiMeta:
        name: input_audio_buffer.append
        group: realtime
        example: |
          {
              "event_id": "event_456",
              "type": "input_audio_buffer.append",
              "audio": "Base64EncodedAudioData"
          }
    RealtimeClientEventInputAudioBufferClear:
      type: object
      description: |
        Send this event to clear the audio bytes in the buffer. The server will 
        respond with an `input_audio_buffer.cleared` event.
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - input_audio_buffer.clear
          description: The event type, must be `input_audio_buffer.clear`.
          x-stainless-const: true
      required:
        - type
      x-oaiMeta:
        name: input_audio_buffer.clear
        group: realtime
        example: |
          {
              "event_id": "event_012",
              "type": "input_audio_buffer.clear"
          }
    RealtimeClientEventInputAudioBufferCommit:
      type: object
      description: >
        Send this event to commit the user input audio buffer, which will create
        a 

        new user message item in the conversation. This event will produce an
        error 

        if the input audio buffer is empty. When in Server VAD mode, the client
        does 

        not need to send this event, the server will commit the audio buffer 

        automatically.


        Committing the input audio buffer will trigger input audio
        transcription 

        (if enabled in session configuration), but it will not create a
        response 

        from the model. The server will respond with an
        `input_audio_buffer.committed` 

        event.
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - input_audio_buffer.commit
          description: The event type, must be `input_audio_buffer.commit`.
          x-stainless-const: true
      required:
        - type
      x-oaiMeta:
        name: input_audio_buffer.commit
        group: realtime
        example: |
          {
              "event_id": "event_789",
              "type": "input_audio_buffer.commit"
          }
    RealtimeClientEventResponseCancel:
      type: object
      description: >
        Send this event to cancel an in-progress response. The server will
        respond 

        with a `response.cancelled` event or an error if there is no response
        to 

        cancel.
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - response.cancel
          description: The event type, must be `response.cancel`.
          x-stainless-const: true
        response_id:
          type: string
          description: |
            A specific response ID to cancel - if not provided, will cancel an 
            in-progress response in the default conversation.
      required:
        - type
      x-oaiMeta:
        name: response.cancel
        group: realtime
        example: |
          {
              "event_id": "event_567",
              "type": "response.cancel"
          }
    RealtimeClientEventResponseCreate:
      type: object
      description: >
        This event instructs the server to create a Response, which means
        triggering 

        model inference. When in Server VAD mode, the server will create
        Responses 

        automatically.


        A Response will include at least one Item, and may have two, in which
        case 

        the second will be a function call. These Items will be appended to the 

        conversation history.


        The server will respond with a `response.created` event, events for
        Items 

        and content created, and finally a `response.done` event to indicate
        the 

        Response is complete.


        The `response.create` event includes inference configuration like 

        `instructions`, and `temperature`. These fields will override the
        Session's 

        configuration for this Response only.
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - response.create
          description: The event type, must be `response.create`.
          x-stainless-const: true
        response:
          $ref: "#/components/schemas/RealtimeResponseCreateParams"
      required:
        - type
      x-oaiMeta:
        name: response.create
        group: realtime
        example: |
          {
              "event_id": "event_234",
              "type": "response.create",
              "response": {
                  "modalities": ["text", "audio"],
                  "instructions": "Please assist the user.",
                  "voice": "sage",
                  "output_audio_format": "pcm16",
                  "tools": [
                      {
                          "type": "function",
                          "name": "calculate_sum",
                          "description": "Calculates the sum of two numbers.",
                          "parameters": {
                              "type": "object",
                              "properties": {
                                  "a": { "type": "number" },
                                  "b": { "type": "number" }
                              },
                              "required": ["a", "b"]
                          }
                      }
                  ],
                  "tool_choice": "auto",
                  "temperature": 0.8,
                  "max_output_tokens": 1024
              }
          }
    RealtimeClientEventSessionUpdate:
      type: object
      description: >
        Send this event to update the session’s default configuration. The
        client may 

        send this event at any time to update the session configuration, and
        any 

        field may be updated at any time, except for "voice". The server will
        respond 

        with a `session.updated` event that shows the full effective
        configuration. 

        Only fields that are present are updated, thus the correct way to clear
        a 

        field like "instructions" is to pass an empty string.
      properties:
        event_id:
          type: string
          description: Optional client-generated ID used to identify this event.
        type:
          type: string
          enum:
            - session.update
          description: The event type, must be `session.update`.
          x-stainless-const: true
        session:
          $ref: "#/components/schemas/RealtimeSessionCreateRequest"
      required:
        - type
        - session
      x-oaiMeta:
        name: session.update
        group: realtime
        example: |
          {
              "event_id": "event_123",
              "type": "session.update",
              "session": {
                  "modalities": ["text", "audio"],
                  "instructions": "You are a helpful assistant.",
                  "voice": "sage",
                  "input_audio_format": "pcm16",
                  "output_audio_format": "pcm16",
                  "input_audio_transcription": {
                      "model": "whisper-1"
                  },
                  "turn_detection": {
                      "type": "server_vad",
                      "threshold": 0.5,
                      "prefix_padding_ms": 300,
                      "silence_duration_ms": 500,
                      "create_response": true
                  },
                  "tools": [
                      {
                          "type": "function",
                          "name": "get_weather",
                          "description": "Get the current weather...",
                          "parameters": {
                              "type": "object",
                              "properties": {
                                  "location": { "type": "string" }
                              },
                              "required": ["location"]
                          }
                      }
                  ],
                  "tool_choice": "auto",
                  "temperature": 0.8,
                  "max_response_output_tokens": "inf"
              }
          }
    RealtimeConversationItem:
      type: object
      x-oaiExpandable: true
      description: The item to add to the conversation.
      properties:
        id:
          type: string
          description: >
            The unique ID of the item, this can be generated by the client to
            help 

            manage server-side context, but is not required because the server
            will 

            generate one if not provided.
        type:
          type: string
          enum:
            - message
            - function_call
            - function_call_output
          description: >
            The type of the item (`message`, `function_call`,
            `function_call_output`).
        object:
          type: string
          enum:
            - realtime.item
          description: >
            Identifier for the API object being returned - always
            `realtime.item`.
          x-stainless-const: true
        status:
          type: string
          enum:
            - completed
            - incomplete
          description: >
            The status of the item (`completed`, `incomplete`). These have no
            effect 

            on the conversation, but are accepted for consistency with the 

            `conversation.item.created` event.
        role:
          type: string
          enum:
            - user
            - assistant
            - system
          description: >
            The role of the message sender (`user`, `assistant`, `system`),
            only 

            applicable for `message` items.
        content:
          type: array
          x-oaiExpandable: true
          description: >
            The content of the message, applicable for `message` items. 

            - Message items of role `system` support only `input_text` content

            - Message items of role `user` support `input_text` and
            `input_audio` 
              content
            - Message items of role `assistant` support `text` content.
          items:
            type: object
            x-oaiExpandable: true
            properties:
              type:
                type: string
                enum:
                  - input_audio
                  - input_text
                  - item_reference
                  - text
                description: >
                  The content type (`input_text`, `input_audio`,
                  `item_reference`, `text`).
              text:
                type: string
                description: >
                  The text content, used for `input_text` and `text` content
                  types.
              id:
                type: string
                description: >
                  ID of a previous conversation item to reference (for
                  `item_reference`

                  content types in `response.create` events). These can
                  reference both

                  client and server created items.
              audio:
                type: string
                description: >
                  Base64-encoded audio bytes, used for `input_audio` content
                  type.
              transcript:
                type: string
                description: >
                  The transcript of the audio, used for `input_audio` content
                  type.
        call_id:
          type: string
          description: >
            The ID of the function call (for `function_call` and 

            `function_call_output` items). If passed on a
            `function_call_output` 

            item, the server will check that a `function_call` item with the
            same 

            ID exists in the conversation history.
        name:
          type: string
          description: |
            The name of the function being called (for `function_call` items).
        arguments:
          type: string
          description: |
            The arguments of the function call (for `function_call` items).
        output:
          type: string
          description: |
            The output of the function call (for `function_call_output` items).
    RealtimeConversationItemWithReference:
      type: object
      x-oaiExpandable: true
      description: The item to add to the conversation.
      properties:
        id:
          type: string
          description: >
            For an item of type (`message` | `function_call` |
            `function_call_output`)

            this field allows the client to assign the unique ID of the item. It
            is

            not required because the server will generate one if not provided.


            For an item of type `item_reference`, this field is required and is
            a

            reference to any item that has previously existed in the
            conversation.
        type:
          type: string
          enum:
            - message
            - function_call
            - function_call_output
          description: >
            The type of the item (`message`, `function_call`,
            `function_call_output`, `item_reference`).
        object:
          type: string
          enum:
            - realtime.item
          description: >
            Identifier for the API object being returned - always
            `realtime.item`.
          x-stainless-const: true
        status:
          type: string
          enum:
            - completed
            - incomplete
          description: >
            The status of the item (`completed`, `incomplete`). These have no
            effect 

            on the conversation, but are accepted for consistency with the 

            `conversation.item.created` event.
        role:
          type: string
          enum:
            - user
            - assistant
            - system
          description: >
            The role of the message sender (`user`, `assistant`, `system`),
            only 

            applicable for `message` items.
        content:
          type: array
          x-oaiExpandable: true
          description: >
            The content of the message, applicable for `message` items. 

            - Message items of role `system` support only `input_text` content

            - Message items of role `user` support `input_text` and
            `input_audio` 
              content
            - Message items of role `assistant` support `text` content.
          items:
            type: object
            x-oaiExpandable: true
            properties:
              type:
                type: string
                enum:
                  - input_audio
                  - input_text
                  - item_reference
                  - text
                description: >
                  The content type (`input_text`, `input_audio`,
                  `item_reference`, `text`).
              text:
                type: string
                description: >
                  The text content, used for `input_text` and `text` content
                  types.
              id:
                type: string
                description: >
                  ID of a previous conversation item to reference (for
                  `item_reference`

                  content types in `response.create` events). These can
                  reference both

                  client and server created items.
              audio:
                type: string
                description: >
                  Base64-encoded audio bytes, used for `input_audio` content
                  type.
              transcript:
                type: string
                description: >
                  The transcript of the audio, used for `input_audio` content
                  type.
        call_id:
          type: string
          description: >
            The ID of the function call (for `function_call` and 

            `function_call_output` items). If passed on a
            `function_call_output` 

            item, the server will check that a `function_call` item with the
            same 

            ID exists in the conversation history.
        name:
          type: string
          description: |
            The name of the function being called (for `function_call` items).
        arguments:
          type: string
          description: |
            The arguments of the function call (for `function_call` items).
        output:
          type: string
          description: |
            The output of the function call (for `function_call_output` items).
    RealtimeResponse:
      type: object
      description: The response resource.
      properties:
        id:
          type: string
          description: The unique ID of the response.
        object:
          type: string
          enum:
            - realtime.response
          description: The object type, must be `realtime.response`.
          x-stainless-const: true
        status:
          type: string
          enum:
            - completed
            - cancelled
            - failed
            - incomplete
          description: >
            The final status of the response (`completed`, `cancelled`,
            `failed`, or 

            `incomplete`).
        status_details:
          type: object
          description: Additional details about the status.
          properties:
            type:
              type: string
              enum:
                - completed
                - cancelled
                - failed
                - incomplete
              description: >
                The type of error that caused the response to fail,
                corresponding 

                with the `status` field (`completed`, `cancelled`,
                `incomplete`, 

                `failed`).
            reason:
              type: string
              enum:
                - turn_detected
                - client_cancelled
                - max_output_tokens
                - content_filter
              description: >
                The reason the Response did not complete. For a `cancelled`
                Response, 

                one of `turn_detected` (the server VAD detected a new start of
                speech) 

                or `client_cancelled` (the client sent a cancel event). For an 

                `incomplete` Response, one of `max_output_tokens` or
                `content_filter` 

                (the server-side safety filter activated and cut off the
                response).
            error:
              type: object
              description: |
                A description of the error that caused the response to fail, 
                populated when the `status` is `failed`.
              properties:
                type:
                  type: string
                  description: The type of error.
                code:
                  type: string
                  description: Error code, if any.
        output:
          type: array
          description: The list of output items generated by the response.
          items:
            $ref: "#/components/schemas/RealtimeConversationItem"
        metadata:
          $ref: "#/components/schemas/Metadata"
        usage:
          type: object
          description: >
            Usage statistics for the Response, this will correspond to billing.
            A 

            Realtime API session will maintain a conversation context and append
            new 

            Items to the Conversation, thus output from previous turns (text
            and 

            audio tokens) will become the input for later turns.
          properties:
            total_tokens:
              type: integer
              description: >
                The total number of tokens in the Response including input and
                output 

                text and audio tokens.
            input_tokens:
              type: integer
              description: >
                The number of input tokens used in the Response, including text
                and 

                audio tokens.
            output_tokens:
              type: integer
              description: >
                The number of output tokens sent in the Response, including text
                and 

                audio tokens.
            input_token_details:
              type: object
              description: Details about the input tokens used in the Response.
              properties:
                cached_tokens:
                  type: integer
                  description: The number of cached tokens used in the Response.
                text_tokens:
                  type: integer
                  description: The number of text tokens used in the Response.
                audio_tokens:
                  type: integer
                  description: The number of audio tokens used in the Response.
            output_token_details:
              type: object
              description: Details about the output tokens used in the Response.
              properties:
                text_tokens:
                  type: integer
                  description: The number of text tokens used in the Response.
                audio_tokens:
                  type: integer
                  description: The number of audio tokens used in the Response.
        conversation_id:
          description: >
            Which conversation the response is added to, determined by the
            `conversation`

            field in the `response.create` event. If `auto`, the response will
            be added to

            the default conversation and the value of `conversation_id` will be
            an id like

            `conv_1234`. If `none`, the response will not be added to any
            conversation and

            the value of `conversation_id` will be `null`. If responses are
            being triggered

            by server VAD, the response will be added to the default
            conversation, thus

            the `conversation_id` will be an id like `conv_1234`.
          type: string
        voice:
          type: string
          enum:
            - alloy
            - ash
            - ballad
            - coral
            - echo
            - sage
            - shimmer
            - verse
          description: >
            The voice the model used to respond.

            Current voice options are `alloy`, `ash`, `ballad`, `coral`, `echo`
            `sage`, 

            `shimmer` and `verse`.
        modalities:
          type: array
          description: >
            The set of modalities the model used to respond. If there are
            multiple modalities,

            the model will pick one, for example if `modalities` is `["text",
            "audio"]`, the model

            could be responding in either text or audio.
          items:
            type: string
            enum:
              - text
              - audio
        output_audio_format:
          type: string
          enum:
            - pcm16
            - g711_ulaw
            - g711_alaw
          description: >
            The format of output audio. Options are `pcm16`, `g711_ulaw`, or
            `g711_alaw`.
        temperature:
          type: number
          description: >
            Sampling temperature for the model, limited to [0.6, 1.2]. Defaults
            to 0.8.
        max_output_tokens:
          oneOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls, that was used in this response.
    RealtimeResponseCreateParams:
      type: object
      description: Create a new Realtime response with these parameters
      properties:
        modalities:
          type: array
          description: |
            The set of modalities the model can respond with. To disable audio,
            set this to ["text"].
          items:
            type: string
            enum:
              - text
              - audio
        instructions:
          type: string
          description: >
            The default system instructions (i.e. system message) prepended to
            model 

            calls. This field allows the client to guide the model on desired 

            responses. The model can be instructed on response content and
            format, 

            (e.g. "be extremely succinct", "act friendly", "here are examples of
            good 

            responses") and on audio behavior (e.g. "talk quickly", "inject
            emotion 

            into your voice", "laugh frequently"). The instructions are not
            guaranteed 

            to be followed by the model, but they provide guidance to the model
            on the 

            desired behavior.


            Note that the server sets default instructions which will be used if
            this 

            field is not set and are visible in the `session.created` event at
            the 

            start of the session.
        voice:
          type: string
          enum:
            - alloy
            - ash
            - ballad
            - coral
            - echo
            - sage
            - shimmer
            - verse
          description: >
            The voice the model uses to respond. Voice cannot be changed during
            the 

            session once the model has responded with audio at least once.
            Current 

            voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`, 

            `shimmer` and `verse`.
        output_audio_format:
          type: string
          enum:
            - pcm16
            - g711_ulaw
            - g711_alaw
          description: >
            The format of output audio. Options are `pcm16`, `g711_ulaw`, or
            `g711_alaw`.
        tools:
          type: array
          description: Tools (functions) available to the model.
          items:
            type: object
            properties:
              type:
                type: string
                enum:
                  - function
                description: The type of the tool, i.e. `function`.
                x-stainless-const: true
              name:
                type: string
                description: The name of the function.
              description:
                type: string
                description: >
                  The description of the function, including guidance on when
                  and how 

                  to call it, and guidance about what to tell the user when
                  calling 

                  (if anything).
              parameters:
                type: object
                description: Parameters of the function in JSON Schema.
        tool_choice:
          type: string
          description: >
            How the model chooses tools. Options are `auto`, `none`, `required`,
            or 

            specify a function, like `{"type": "function", "function": {"name":
            "my_function"}}`.
        temperature:
          type: number
          description: >
            Sampling temperature for the model, limited to [0.6, 1.2]. Defaults
            to 0.8.
        max_response_output_tokens:
          oneOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls. Provide an integer between 1 and 4096 to
            limit output tokens, or `inf` for the maximum available tokens for a
            given model. Defaults to `inf`.
        conversation:
          description: >
            Controls which conversation the response is added to. Currently
            supports

            `auto` and `none`, with `auto` as the default value. The `auto`
            value

            means that the contents of the response will be added to the default

            conversation. Set this to `none` to create an out-of-band response
            which 

            will not add items to default conversation.
          oneOf:
            - type: string
            - type: string
              default: auto
              enum:
                - auto
                - none
        metadata:
          $ref: "#/components/schemas/Metadata"
        input:
          type: array
          description: >
            Input items to include in the prompt for the model. Using this field

            creates a new context for this Response instead of using the default

            conversation. An empty array `[]` will clear the context for this
            Response.

            Note that this can include references to items from the default
            conversation.
          items:
            $ref: "#/components/schemas/RealtimeConversationItemWithReference"
    RealtimeServerEventConversationCreated:
      type: object
      description: >
        Returned when a conversation is created. Emitted right after session
        creation.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - conversation.created
          description: The event type, must be `conversation.created`.
          x-stainless-const: true
        conversation:
          type: object
          description: The conversation resource.
          properties:
            id:
              type: string
              description: The unique ID of the conversation.
            object:
              type: string
              description: The object type, must be `realtime.conversation`.
      required:
        - event_id
        - type
        - conversation
      x-oaiMeta:
        name: conversation.created
        group: realtime
        example: |
          {
              "event_id": "event_9101",
              "type": "conversation.created",
              "conversation": {
                  "id": "conv_001",
                  "object": "realtime.conversation"
              }
          }
    RealtimeServerEventConversationItemCreated:
      type: object
      description: >
        Returned when a conversation item is created. There are several
        scenarios that 

        produce this event:
          - The server is generating a Response, which if successful will produce 
            either one or two Items, which will be of type `message` 
            (role `assistant`) or type `function_call`.
          - The input audio buffer has been committed, either by the client or the 
            server (in `server_vad` mode). The server will take the content of the 
            input audio buffer and add it to a new user message Item.
          - The client has sent a `conversation.item.create` event to add a new Item 
            to the Conversation.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - conversation.item.created
          description: The event type, must be `conversation.item.created`.
          x-stainless-const: true
        previous_item_id:
          type: string
          description: >
            The ID of the preceding item in the Conversation context, allows
            the 

            client to understand the order of the conversation.
        item:
          $ref: "#/components/schemas/RealtimeConversationItem"
      required:
        - event_id
        - type
        - previous_item_id
        - item
      x-oaiMeta:
        name: conversation.item.created
        group: realtime
        example: |
          {
              "event_id": "event_1920",
              "type": "conversation.item.created",
              "previous_item_id": "msg_002",
              "item": {
                  "id": "msg_003",
                  "object": "realtime.item",
                  "type": "message",
                  "status": "completed",
                  "role": "user",
                  "content": [
                      {
                          "type": "input_audio",
                          "transcript": "hello how are you",
                          "audio": "base64encodedaudio=="
                      }
                  ]
              }
          }
    RealtimeServerEventConversationItemDeleted:
      type: object
      description: >
        Returned when an item in the conversation is deleted by the client with
        a 

        `conversation.item.delete` event. This event is used to synchronize the 

        server's understanding of the conversation history with the client's
        view.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - conversation.item.deleted
          description: The event type, must be `conversation.item.deleted`.
          x-stainless-const: true
        item_id:
          type: string
          description: The ID of the item that was deleted.
      required:
        - event_id
        - type
        - item_id
      x-oaiMeta:
        name: conversation.item.deleted
        group: realtime
        example: |
          {
              "event_id": "event_2728",
              "type": "conversation.item.deleted",
              "item_id": "msg_005"
          }
    RealtimeServerEventConversationItemInputAudioTranscriptionCompleted:
      type: object
      description: >
        This event is the output of audio transcription for user audio written
        to the 

        user audio buffer. Transcription begins when the input audio buffer is 

        committed by the client or server (in `server_vad` mode). Transcription
        runs 

        asynchronously with Response creation, so this event may come before or
        after 

        the Response events.


        Realtime API models accept audio natively, and thus input transcription
        is a 

        separate process run on a separate ASR (Automatic Speech Recognition)
        model, 

        currently always `whisper-1`. Thus the transcript may diverge somewhat
        from 

        the model's interpretation, and should be treated as a rough guide.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - conversation.item.input_audio_transcription.completed
          description: |
            The event type, must be
            `conversation.item.input_audio_transcription.completed`.
          x-stainless-const: true
        item_id:
          type: string
          description: The ID of the user message item containing the audio.
        content_index:
          type: integer
          description: The index of the content part containing the audio.
        transcript:
          type: string
          description: The transcribed text.
      required:
        - event_id
        - type
        - item_id
        - content_index
        - transcript
      x-oaiMeta:
        name: conversation.item.input_audio_transcription.completed
        group: realtime
        example: |
          {
              "event_id": "event_2122",
              "type": "conversation.item.input_audio_transcription.completed",
              "item_id": "msg_003",
              "content_index": 0,
              "transcript": "Hello, how are you?"
          }
    RealtimeServerEventConversationItemInputAudioTranscriptionFailed:
      type: object
      description: >
        Returned when input audio transcription is configured, and a
        transcription 

        request for a user message failed. These events are separate from other 

        `error` events so that the client can identify the related Item.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - conversation.item.input_audio_transcription.failed
          description: |
            The event type, must be
            `conversation.item.input_audio_transcription.failed`.
          x-stainless-const: true
        item_id:
          type: string
          description: The ID of the user message item.
        content_index:
          type: integer
          description: The index of the content part containing the audio.
        error:
          type: object
          description: Details of the transcription error.
          properties:
            type:
              type: string
              description: The type of error.
            code:
              type: string
              description: Error code, if any.
            message:
              type: string
              description: A human-readable error message.
            param:
              type: string
              description: Parameter related to the error, if any.
      required:
        - event_id
        - type
        - item_id
        - content_index
        - error
      x-oaiMeta:
        name: conversation.item.input_audio_transcription.failed
        group: realtime
        example: |
          {
              "event_id": "event_2324",
              "type": "conversation.item.input_audio_transcription.failed",
              "item_id": "msg_003",
              "content_index": 0,
              "error": {
                  "type": "transcription_error",
                  "code": "audio_unintelligible",
                  "message": "The audio could not be transcribed.",
                  "param": null
              }
          }
    RealtimeServerEventConversationItemTruncated:
      type: object
      description: >
        Returned when an earlier assistant audio message item is truncated by
        the 

        client with a `conversation.item.truncate` event. This event is used to 

        synchronize the server's understanding of the audio with the client's
        playback.


        This action will truncate the audio and remove the server-side text
        transcript 

        to ensure there is no text in the context that hasn't been heard by the
        user.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - conversation.item.truncated
          description: The event type, must be `conversation.item.truncated`.
          x-stainless-const: true
        item_id:
          type: string
          description: The ID of the assistant message item that was truncated.
        content_index:
          type: integer
          description: The index of the content part that was truncated.
        audio_end_ms:
          type: integer
          description: |
            The duration up to which the audio was truncated, in milliseconds.
      required:
        - event_id
        - type
        - item_id
        - content_index
        - audio_end_ms
      x-oaiMeta:
        name: conversation.item.truncated
        group: realtime
        example: |
          {
              "event_id": "event_2526",
              "type": "conversation.item.truncated",
              "item_id": "msg_004",
              "content_index": 0,
              "audio_end_ms": 1500
          }
    RealtimeServerEventError:
      type: object
      description: >
        Returned when an error occurs, which could be a client problem or a
        server 

        problem. Most errors are recoverable and the session will stay open, we 

        recommend to implementors to monitor and log error messages by default.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - error
          description: The event type, must be `error`.
          x-stainless-const: true
        error:
          type: object
          description: Details of the error.
          required:
            - type
            - message
          properties:
            type:
              type: string
              description: >
                The type of error (e.g., "invalid_request_error",
                "server_error").
            code:
              type: string
              description: Error code, if any.
              nullable: true
            message:
              type: string
              description: A human-readable error message.
            param:
              type: string
              description: Parameter related to the error, if any.
              nullable: true
            event_id:
              type: string
              description: >
                The event_id of the client event that caused the error, if
                applicable.
              nullable: true
      required:
        - event_id
        - type
        - error
      x-oaiMeta:
        name: error
        group: realtime
        example: |
          {
              "event_id": "event_890",
              "type": "error",
              "error": {
                  "type": "invalid_request_error",
                  "code": "invalid_event",
                  "message": "The 'type' field is missing.",
                  "param": null,
                  "event_id": "event_567"
              }
          }
    RealtimeServerEventInputAudioBufferCleared:
      type: object
      description: |
        Returned when the input audio buffer is cleared by the client with a 
        `input_audio_buffer.clear` event.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - input_audio_buffer.cleared
          description: The event type, must be `input_audio_buffer.cleared`.
          x-stainless-const: true
      required:
        - event_id
        - type
      x-oaiMeta:
        name: input_audio_buffer.cleared
        group: realtime
        example: |
          {
              "event_id": "event_1314",
              "type": "input_audio_buffer.cleared"
          }
    RealtimeServerEventInputAudioBufferCommitted:
      type: object
      description: >
        Returned when an input audio buffer is committed, either by the client
        or 

        automatically in server VAD mode. The `item_id` property is the ID of
        the user

        message item that will be created, thus a `conversation.item.created`
        event 

        will also be sent to the client.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - input_audio_buffer.committed
          description: The event type, must be `input_audio_buffer.committed`.
          x-stainless-const: true
        previous_item_id:
          type: string
          description: >
            The ID of the preceding item after which the new item will be
            inserted.
        item_id:
          type: string
          description: The ID of the user message item that will be created.
      required:
        - event_id
        - type
        - previous_item_id
        - item_id
      x-oaiMeta:
        name: input_audio_buffer.committed
        group: realtime
        example: |
          {
              "event_id": "event_1121",
              "type": "input_audio_buffer.committed",
              "previous_item_id": "msg_001",
              "item_id": "msg_002"
          }
    RealtimeServerEventInputAudioBufferSpeechStarted:
      type: object
      description: >
        Sent by the server when in `server_vad` mode to indicate that speech has
        been 

        detected in the audio buffer. This can happen any time audio is added to
        the 

        buffer (unless speech is already detected). The client may want to use
        this 

        event to interrupt audio playback or provide visual feedback to the
        user. 


        The client should expect to receive a
        `input_audio_buffer.speech_stopped` event 

        when speech stops. The `item_id` property is the ID of the user message
        item 

        that will be created when speech stops and will also be included in the 

        `input_audio_buffer.speech_stopped` event (unless the client manually
        commits 

        the audio buffer during VAD activation).
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - input_audio_buffer.speech_started
          description: The event type, must be `input_audio_buffer.speech_started`.
          x-stainless-const: true
        audio_start_ms:
          type: integer
          description: >
            Milliseconds from the start of all audio written to the buffer
            during the 

            session when speech was first detected. This will correspond to the 

            beginning of audio sent to the model, and thus includes the 

            `prefix_padding_ms` configured in the Session.
        item_id:
          type: string
          description: >
            The ID of the user message item that will be created when speech
            stops.
      required:
        - event_id
        - type
        - audio_start_ms
        - item_id
      x-oaiMeta:
        name: input_audio_buffer.speech_started
        group: realtime
        example: |
          {
              "event_id": "event_1516",
              "type": "input_audio_buffer.speech_started",
              "audio_start_ms": 1000,
              "item_id": "msg_003"
          }
    RealtimeServerEventInputAudioBufferSpeechStopped:
      type: object
      description: >
        Returned in `server_vad` mode when the server detects the end of speech
        in 

        the audio buffer. The server will also send an
        `conversation.item.created` 

        event with the user message item that is created from the audio buffer.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - input_audio_buffer.speech_stopped
          description: The event type, must be `input_audio_buffer.speech_stopped`.
          x-stainless-const: true
        audio_end_ms:
          type: integer
          description: >
            Milliseconds since the session started when speech stopped. This
            will 

            correspond to the end of audio sent to the model, and thus includes
            the 

            `min_silence_duration_ms` configured in the Session.
        item_id:
          type: string
          description: The ID of the user message item that will be created.
      required:
        - event_id
        - type
        - audio_end_ms
        - item_id
      x-oaiMeta:
        name: input_audio_buffer.speech_stopped
        group: realtime
        example: |
          {
              "event_id": "event_1718",
              "type": "input_audio_buffer.speech_stopped",
              "audio_end_ms": 2000,
              "item_id": "msg_003"
          }
    RealtimeServerEventRateLimitsUpdated:
      type: object
      description: >
        Emitted at the beginning of a Response to indicate the updated rate
        limits. 

        When a Response is created some tokens will be "reserved" for the
        output 

        tokens, the rate limits shown here reflect that reservation, which is
        then 

        adjusted accordingly once the Response is completed.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - rate_limits.updated
          description: The event type, must be `rate_limits.updated`.
          x-stainless-const: true
        rate_limits:
          type: array
          description: List of rate limit information.
          items:
            type: object
            properties:
              name:
                type: string
                enum:
                  - requests
                  - tokens
                description: |
                  The name of the rate limit (`requests`, `tokens`).
              limit:
                type: integer
                description: The maximum allowed value for the rate limit.
              remaining:
                type: integer
                description: The remaining value before the limit is reached.
              reset_seconds:
                type: number
                description: Seconds until the rate limit resets.
      required:
        - event_id
        - type
        - rate_limits
      x-oaiMeta:
        name: rate_limits.updated
        group: realtime
        example: |
          {
              "event_id": "event_5758",
              "type": "rate_limits.updated",
              "rate_limits": [
                  {
                      "name": "requests",
                      "limit": 1000,
                      "remaining": 999,
                      "reset_seconds": 60
                  },
                  {
                      "name": "tokens",
                      "limit": 50000,
                      "remaining": 49950,
                      "reset_seconds": 60
                  }
              ]
          }
    RealtimeServerEventResponseAudioDelta:
      type: object
      description: Returned when the model-generated audio is updated.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.audio.delta
          description: The event type, must be `response.audio.delta`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        content_index:
          type: integer
          description: The index of the content part in the item's content array.
        delta:
          type: string
          description: Base64-encoded audio data delta.
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - content_index
        - delta
      x-oaiMeta:
        name: response.audio.delta
        group: realtime
        example: |
          {
              "event_id": "event_4950",
              "type": "response.audio.delta",
              "response_id": "resp_001",
              "item_id": "msg_008",
              "output_index": 0,
              "content_index": 0,
              "delta": "Base64EncodedAudioDelta"
          }
    RealtimeServerEventResponseAudioDone:
      type: object
      description: >
        Returned when the model-generated audio is done. Also emitted when a
        Response

        is interrupted, incomplete, or cancelled.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.audio.done
          description: The event type, must be `response.audio.done`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        content_index:
          type: integer
          description: The index of the content part in the item's content array.
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - content_index
      x-oaiMeta:
        name: response.audio.done
        group: realtime
        example: |
          {
              "event_id": "event_5152",
              "type": "response.audio.done",
              "response_id": "resp_001",
              "item_id": "msg_008",
              "output_index": 0,
              "content_index": 0
          }
    RealtimeServerEventResponseAudioTranscriptDelta:
      type: object
      description: >
        Returned when the model-generated transcription of audio output is
        updated.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.audio_transcript.delta
          description: The event type, must be `response.audio_transcript.delta`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        content_index:
          type: integer
          description: The index of the content part in the item's content array.
        delta:
          type: string
          description: The transcript delta.
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - content_index
        - delta
      x-oaiMeta:
        name: response.audio_transcript.delta
        group: realtime
        example: |
          {
              "event_id": "event_4546",
              "type": "response.audio_transcript.delta",
              "response_id": "resp_001",
              "item_id": "msg_008",
              "output_index": 0,
              "content_index": 0,
              "delta": "Hello, how can I a"
          }
    RealtimeServerEventResponseAudioTranscriptDone:
      type: object
      description: |
        Returned when the model-generated transcription of audio output is done
        streaming. Also emitted when a Response is interrupted, incomplete, or
        cancelled.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.audio_transcript.done
          description: The event type, must be `response.audio_transcript.done`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        content_index:
          type: integer
          description: The index of the content part in the item's content array.
        transcript:
          type: string
          description: The final transcript of the audio.
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - content_index
        - transcript
      x-oaiMeta:
        name: response.audio_transcript.done
        group: realtime
        example: |
          {
              "event_id": "event_4748",
              "type": "response.audio_transcript.done",
              "response_id": "resp_001",
              "item_id": "msg_008",
              "output_index": 0,
              "content_index": 0,
              "transcript": "Hello, how can I assist you today?"
          }
    RealtimeServerEventResponseContentPartAdded:
      type: object
      description: >
        Returned when a new content part is added to an assistant message item
        during

        response generation.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.content_part.added
          description: The event type, must be `response.content_part.added`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the item to which the content part was added.
        output_index:
          type: integer
          description: The index of the output item in the response.
        content_index:
          type: integer
          description: The index of the content part in the item's content array.
        part:
          type: object
          description: The content part that was added.
          properties:
            type:
              type: string
              enum:
                - audio
                - text
              description: The content type ("text", "audio").
            text:
              type: string
              description: The text content (if type is "text").
            audio:
              type: string
              description: Base64-encoded audio data (if type is "audio").
            transcript:
              type: string
              description: The transcript of the audio (if type is "audio").
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - content_index
        - part
      x-oaiMeta:
        name: response.content_part.added
        group: realtime
        example: |
          {
              "event_id": "event_3738",
              "type": "response.content_part.added",
              "response_id": "resp_001",
              "item_id": "msg_007",
              "output_index": 0,
              "content_index": 0,
              "part": {
                  "type": "text",
                  "text": ""
              }
          }
    RealtimeServerEventResponseContentPartDone:
      type: object
      description: >
        Returned when a content part is done streaming in an assistant message
        item.

        Also emitted when a Response is interrupted, incomplete, or cancelled.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.content_part.done
          description: The event type, must be `response.content_part.done`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        content_index:
          type: integer
          description: The index of the content part in the item's content array.
        part:
          type: object
          description: The content part that is done.
          properties:
            type:
              type: string
              enum:
                - audio
                - text
              description: The content type ("text", "audio").
            text:
              type: string
              description: The text content (if type is "text").
            audio:
              type: string
              description: Base64-encoded audio data (if type is "audio").
            transcript:
              type: string
              description: The transcript of the audio (if type is "audio").
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - content_index
        - part
      x-oaiMeta:
        name: response.content_part.done
        group: realtime
        example: |
          {
              "event_id": "event_3940",
              "type": "response.content_part.done",
              "response_id": "resp_001",
              "item_id": "msg_007",
              "output_index": 0,
              "content_index": 0,
              "part": {
                  "type": "text",
                  "text": "Sure, I can help with that."
              }
          }
    RealtimeServerEventResponseCreated:
      type: object
      description: >
        Returned when a new Response is created. The first event of response
        creation,

        where the response is in an initial state of `in_progress`.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.created
          description: The event type, must be `response.created`.
          x-stainless-const: true
        response:
          $ref: "#/components/schemas/RealtimeResponse"
      required:
        - event_id
        - type
        - response
      x-oaiMeta:
        name: response.created
        group: realtime
        example: |
          {
              "event_id": "event_2930",
              "type": "response.created",
              "response": {
                  "id": "resp_001",
                  "object": "realtime.response",
                  "status": "in_progress",
                  "status_details": null,
                  "output": [],
                  "usage": null
              }
          }
    RealtimeServerEventResponseDone:
      type: object
      description: >
        Returned when a Response is done streaming. Always emitted, no matter
        the 

        final state. The Response object included in the `response.done` event
        will 

        include all output Items in the Response but will omit the raw audio
        data.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.done
          description: The event type, must be `response.done`.
          x-stainless-const: true
        response:
          $ref: "#/components/schemas/RealtimeResponse"
      required:
        - event_id
        - type
        - response
      x-oaiMeta:
        name: response.done
        group: realtime
        example: |
          {
              "event_id": "event_3132",
              "type": "response.done",
              "response": {
                  "id": "resp_001",
                  "object": "realtime.response",
                  "status": "completed",
                  "status_details": null,
                  "output": [
                      {
                          "id": "msg_006",
                          "object": "realtime.item",
                          "type": "message",
                          "status": "completed",
                          "role": "assistant",
                          "content": [
                              {
                                  "type": "text",
                                  "text": "Sure, how can I assist you today?"
                              }
                          ]
                      }
                  ],
                  "usage": {
                      "total_tokens":275,
                      "input_tokens":127,
                      "output_tokens":148,
                      "input_token_details": {
                          "cached_tokens":384,
                          "text_tokens":119,
                          "audio_tokens":8,
                          "cached_tokens_details": {
                              "text_tokens": 128,
                              "audio_tokens": 256
                          }
                      },
                      "output_token_details": {
                        "text_tokens":36,
                        "audio_tokens":112
                      }
                  }
              }
          }
    RealtimeServerEventResponseFunctionCallArgumentsDelta:
      type: object
      description: |
        Returned when the model-generated function call arguments are updated.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.function_call_arguments.delta
          description: |
            The event type, must be `response.function_call_arguments.delta`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the function call item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        call_id:
          type: string
          description: The ID of the function call.
        delta:
          type: string
          description: The arguments delta as a JSON string.
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - call_id
        - delta
      x-oaiMeta:
        name: response.function_call_arguments.delta
        group: realtime
        example: |
          {
              "event_id": "event_5354",
              "type": "response.function_call_arguments.delta",
              "response_id": "resp_002",
              "item_id": "fc_001",
              "output_index": 0,
              "call_id": "call_001",
              "delta": "{\"location\": \"San\""
          }
    RealtimeServerEventResponseFunctionCallArgumentsDone:
      type: object
      description: >
        Returned when the model-generated function call arguments are done
        streaming.

        Also emitted when a Response is interrupted, incomplete, or cancelled.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.function_call_arguments.done
          description: |
            The event type, must be `response.function_call_arguments.done`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the function call item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        call_id:
          type: string
          description: The ID of the function call.
        arguments:
          type: string
          description: The final arguments as a JSON string.
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - call_id
        - arguments
      x-oaiMeta:
        name: response.function_call_arguments.done
        group: realtime
        example: |
          {
              "event_id": "event_5556",
              "type": "response.function_call_arguments.done",
              "response_id": "resp_002",
              "item_id": "fc_001",
              "output_index": 0,
              "call_id": "call_001",
              "arguments": "{\"location\": \"San Francisco\"}"
          }
    RealtimeServerEventResponseOutputItemAdded:
      type: object
      description: Returned when a new Item is created during Response generation.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.output_item.added
          description: The event type, must be `response.output_item.added`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the Response to which the item belongs.
        output_index:
          type: integer
          description: The index of the output item in the Response.
        item:
          $ref: "#/components/schemas/RealtimeConversationItem"
      required:
        - event_id
        - type
        - response_id
        - output_index
        - item
      x-oaiMeta:
        name: response.output_item.added
        group: realtime
        example: |
          {
              "event_id": "event_3334",
              "type": "response.output_item.added",
              "response_id": "resp_001",
              "output_index": 0,
              "item": {
                  "id": "msg_007",
                  "object": "realtime.item",
                  "type": "message",
                  "status": "in_progress",
                  "role": "assistant",
                  "content": []
              }
          }
    RealtimeServerEventResponseOutputItemDone:
      type: object
      description: >
        Returned when an Item is done streaming. Also emitted when a Response
        is 

        interrupted, incomplete, or cancelled.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.output_item.done
          description: The event type, must be `response.output_item.done`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the Response to which the item belongs.
        output_index:
          type: integer
          description: The index of the output item in the Response.
        item:
          $ref: "#/components/schemas/RealtimeConversationItem"
      required:
        - event_id
        - type
        - response_id
        - output_index
        - item
      x-oaiMeta:
        name: response.output_item.done
        group: realtime
        example: |
          {
              "event_id": "event_3536",
              "type": "response.output_item.done",
              "response_id": "resp_001",
              "output_index": 0,
              "item": {
                  "id": "msg_007",
                  "object": "realtime.item",
                  "type": "message",
                  "status": "completed",
                  "role": "assistant",
                  "content": [
                      {
                          "type": "text",
                          "text": "Sure, I can help with that."
                      }
                  ]
              }
          }
    RealtimeServerEventResponseTextDelta:
      type: object
      description: Returned when the text value of a "text" content part is updated.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.text.delta
          description: The event type, must be `response.text.delta`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        content_index:
          type: integer
          description: The index of the content part in the item's content array.
        delta:
          type: string
          description: The text delta.
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - content_index
        - delta
      x-oaiMeta:
        name: response.text.delta
        group: realtime
        example: |
          {
              "event_id": "event_4142",
              "type": "response.text.delta",
              "response_id": "resp_001",
              "item_id": "msg_007",
              "output_index": 0,
              "content_index": 0,
              "delta": "Sure, I can h"
          }
    RealtimeServerEventResponseTextDone:
      type: object
      description: >
        Returned when the text value of a "text" content part is done streaming.
        Also

        emitted when a Response is interrupted, incomplete, or cancelled.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - response.text.done
          description: The event type, must be `response.text.done`.
          x-stainless-const: true
        response_id:
          type: string
          description: The ID of the response.
        item_id:
          type: string
          description: The ID of the item.
        output_index:
          type: integer
          description: The index of the output item in the response.
        content_index:
          type: integer
          description: The index of the content part in the item's content array.
        text:
          type: string
          description: The final text content.
      required:
        - event_id
        - type
        - response_id
        - item_id
        - output_index
        - content_index
        - text
      x-oaiMeta:
        name: response.text.done
        group: realtime
        example: |
          {
              "event_id": "event_4344",
              "type": "response.text.done",
              "response_id": "resp_001",
              "item_id": "msg_007",
              "output_index": 0,
              "content_index": 0,
              "text": "Sure, I can help with that."
          }
    RealtimeServerEventSessionCreated:
      type: object
      description: >
        Returned when a Session is created. Emitted automatically when a new 

        connection is established as the first server event. This event will
        contain 

        the default Session configuration.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - session.created
          description: The event type, must be `session.created`.
          x-stainless-const: true
        session:
          $ref: "#/components/schemas/RealtimeSession"
      required:
        - event_id
        - type
        - session
      x-oaiMeta:
        name: session.created
        group: realtime
        example: |
          {
              "event_id": "event_1234",
              "type": "session.created",
              "session": {
                  "id": "sess_001",
                  "object": "realtime.session",
                  "model": "gpt-4o-realtime-preview-2024-12-17",
                  "modalities": ["text", "audio"],
                  "instructions": "...model instructions here...",
                  "voice": "sage",
                  "input_audio_format": "pcm16",
                  "output_audio_format": "pcm16",
                  "input_audio_transcription": null,
                  "turn_detection": {
                      "type": "server_vad",
                      "threshold": 0.5,
                      "prefix_padding_ms": 300,
                      "silence_duration_ms": 200
                  },
                  "tools": [],
                  "tool_choice": "auto",
                  "temperature": 0.8,
                  "max_response_output_tokens": "inf"
              }
          }
    RealtimeServerEventSessionUpdated:
      type: object
      description: >
        Returned when a session is updated with a `session.update` event,
        unless 

        there is an error.
      properties:
        event_id:
          type: string
          description: The unique ID of the server event.
        type:
          type: string
          enum:
            - session.updated
          description: The event type, must be `session.updated`.
          x-stainless-const: true
        session:
          $ref: "#/components/schemas/RealtimeSession"
      required:
        - event_id
        - type
        - session
      x-oaiMeta:
        name: session.updated
        group: realtime
        example: |
          {
              "event_id": "event_5678",
              "type": "session.updated",
              "session": {
                  "id": "sess_001",
                  "object": "realtime.session",
                  "model": "gpt-4o-realtime-preview-2024-12-17",
                  "modalities": ["text"],
                  "instructions": "New instructions",
                  "voice": "sage",
                  "input_audio_format": "pcm16",
                  "output_audio_format": "pcm16",
                  "input_audio_transcription": {
                      "model": "whisper-1"
                  },
                  "turn_detection": null,
                  "tools": [],
                  "tool_choice": "none",
                  "temperature": 0.7,
                  "max_response_output_tokens": 200
              }
          }
    RealtimeSession:
      type: object
      description: Realtime session object configuration.
      properties:
        id:
          type: string
          description: |
            Unique identifier for the session object.
        modalities:
          description: |
            The set of modalities the model can respond with. To disable audio,
            set this to ["text"].
          items:
            type: string
            enum:
              - text
              - audio
        model:
          description: |
            The Realtime model used for this session.
          anyOf:
            - type: string
            - type: string
              enum:
                - gpt-4o-realtime-preview
                - gpt-4o-realtime-preview-2024-10-01
                - gpt-4o-realtime-preview-2024-12-17
                - gpt-4o-mini-realtime-preview
                - gpt-4o-mini-realtime-preview-2024-12-17
        instructions:
          type: string
          description: >
            The default system instructions (i.e. system message) prepended to
            model 

            calls. This field allows the client to guide the model on desired 

            responses. The model can be instructed on response content and
            format, 

            (e.g. "be extremely succinct", "act friendly", "here are examples of
            good 

            responses") and on audio behavior (e.g. "talk quickly", "inject
            emotion 

            into your voice", "laugh frequently"). The instructions are not
            guaranteed 

            to be followed by the model, but they provide guidance to the model
            on the 

            desired behavior.


            Note that the server sets default instructions which will be used if
            this 

            field is not set and are visible in the `session.created` event at
            the 

            start of the session.
        voice:
          type: string
          enum:
            - alloy
            - ash
            - ballad
            - coral
            - echo
            - sage
            - shimmer
            - verse
          description: >
            The voice the model uses to respond. Voice cannot be changed during
            the 

            session once the model has responded with audio at least once.
            Current 

            voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`, 

            `shimmer` and `verse`.
        input_audio_format:
          type: string
          enum:
            - pcm16
            - g711_ulaw
            - g711_alaw
          description: >
            The format of input audio. Options are `pcm16`, `g711_ulaw`, or
            `g711_alaw`.

            For `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, 

            single channel (mono), and little-endian byte order.
        output_audio_format:
          type: string
          enum:
            - pcm16
            - g711_ulaw
            - g711_alaw
          description: >
            The format of output audio. Options are `pcm16`, `g711_ulaw`, or
            `g711_alaw`.

            For `pcm16`, output audio is sampled at a rate of 24kHz.
        input_audio_transcription:
          type: object
          description: >
            Configuration for input audio transcription, defaults to off and can
            be 

            set to `null` to turn off once on. Input audio transcription is not
            native 

            to the model, since the model consumes audio directly. Transcription
            runs 

            asynchronously through Whisper and should be treated as rough
            guidance 

            rather than the representation understood by the model.
          properties:
            model:
              type: string
              description: >
                The model to use for transcription, `whisper-1` is the only
                currently 

                supported model.
        turn_detection:
          type: object
          nullable: true
          description: >
            Configuration for turn detection. Can be set to `null` to turn off.
            Server 

            VAD means that the model will detect the start and end of speech
            based on 

            audio volume and respond at the end of user speech.
          properties:
            type:
              type: string
              enum:
                - server_vad
              description: >
                Type of turn detection, only `server_vad` is currently
                supported.
              x-stainless-const: true
            threshold:
              type: number
              description: >
                Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5.
                A 

                higher threshold will require louder audio to activate the
                model, and 

                thus might perform better in noisy environments.
            prefix_padding_ms:
              type: integer
              description: |
                Amount of audio to include before the VAD detected speech (in 
                milliseconds). Defaults to 300ms.
            silence_duration_ms:
              type: integer
              description: >
                Duration of silence to detect speech stop (in milliseconds).
                Defaults 

                to 500ms. With shorter values the model will respond more
                quickly, 

                but may jump in on short pauses from the user.
        tools:
          type: array
          description: Tools (functions) available to the model.
          items:
            type: object
            properties:
              type:
                type: string
                enum:
                  - function
                description: The type of the tool, i.e. `function`.
                x-stainless-const: true
              name:
                type: string
                description: The name of the function.
              description:
                type: string
                description: >
                  The description of the function, including guidance on when
                  and how 

                  to call it, and guidance about what to tell the user when
                  calling 

                  (if anything).
              parameters:
                type: object
                description: Parameters of the function in JSON Schema.
        tool_choice:
          type: string
          description: >
            How the model chooses tools. Options are `auto`, `none`, `required`,
            or 

            specify a function.
        temperature:
          type: number
          description: >
            Sampling temperature for the model, limited to [0.6, 1.2]. Defaults
            to 0.8.
        max_response_output_tokens:
          oneOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls. Provide an integer between 1 and 4096 to
            limit output tokens, or `inf` for the maximum available tokens for a
            given model. Defaults to `inf`.
    RealtimeSessionCreateRequest:
      type: object
      description: Realtime session object configuration.
      properties:
        modalities:
          description: |
            The set of modalities the model can respond with. To disable audio,
            set this to ["text"].
          items:
            type: string
            enum:
              - text
              - audio
        model:
          type: string
          description: |
            The Realtime model used for this session.
          enum:
            - gpt-4o-realtime-preview
            - gpt-4o-realtime-preview-2024-10-01
            - gpt-4o-realtime-preview-2024-12-17
            - gpt-4o-mini-realtime-preview
            - gpt-4o-mini-realtime-preview-2024-12-17
        instructions:
          type: string
          description: >
            The default system instructions (i.e. system message) prepended to
            model 

            calls. This field allows the client to guide the model on desired 

            responses. The model can be instructed on response content and
            format, 

            (e.g. "be extremely succinct", "act friendly", "here are examples of
            good 

            responses") and on audio behavior (e.g. "talk quickly", "inject
            emotion 

            into your voice", "laugh frequently"). The instructions are not
            guaranteed 

            to be followed by the model, but they provide guidance to the model
            on the 

            desired behavior.


            Note that the server sets default instructions which will be used if
            this 

            field is not set and are visible in the `session.created` event at
            the 

            start of the session.
        voice:
          type: string
          enum:
            - alloy
            - ash
            - ballad
            - coral
            - echo
            - sage
            - shimmer
            - verse
          description: >
            The voice the model uses to respond. Voice cannot be changed during
            the 

            session once the model has responded with audio at least once.
            Current 

            voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`, 

            `shimmer` and `verse`.
        input_audio_format:
          type: string
          enum:
            - pcm16
            - g711_ulaw
            - g711_alaw
          description: >
            The format of input audio. Options are `pcm16`, `g711_ulaw`, or
            `g711_alaw`.

            For `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate, 

            single channel (mono), and little-endian byte order.
        output_audio_format:
          type: string
          enum:
            - pcm16
            - g711_ulaw
            - g711_alaw
          description: >
            The format of output audio. Options are `pcm16`, `g711_ulaw`, or
            `g711_alaw`.

            For `pcm16`, output audio is sampled at a rate of 24kHz.
        input_audio_transcription:
          type: object
          description: >
            Configuration for input audio transcription, defaults to off and can
            be  set to `null` to turn off once on. Input audio transcription is
            not native to the model, since the model consumes audio directly.
            Transcription runs  asynchronously through [OpenAI Whisper
            transcription](https://platform.openai.com/docs/api-reference/audio/createTranscription)
            and should be treated as rough guidance rather than the
            representation understood by the model. The client can optionally
            set the language and prompt for transcription, these fields will be
            passed to the Whisper API.
          properties:
            model:
              type: string
              description: >
                The model to use for transcription, `whisper-1` is the only
                currently 

                supported model.
            language:
              type: string
              description: >
                The language of the input audio. Supplying the input language in

                [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)
                (e.g. `en`) format

                will improve accuracy and latency.
            prompt:
              type: string
              description: >
                An optional text to guide the model's style or continue a
                previous audio

                segment. The [prompt](/docs/guides/speech-to-text#prompting)
                should match

                the audio language.
        turn_detection:
          type: object
          description: >
            Configuration for turn detection. Can be set to `null` to turn off.
            Server 

            VAD means that the model will detect the start and end of speech
            based on 

            audio volume and respond at the end of user speech.
          properties:
            type:
              type: string
              description: >
                Type of turn detection, only `server_vad` is currently
                supported.
            threshold:
              type: number
              description: >
                Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5.
                A 

                higher threshold will require louder audio to activate the
                model, and 

                thus might perform better in noisy environments.
            prefix_padding_ms:
              type: integer
              description: |
                Amount of audio to include before the VAD detected speech (in 
                milliseconds). Defaults to 300ms.
            silence_duration_ms:
              type: integer
              description: >
                Duration of silence to detect speech stop (in milliseconds).
                Defaults 

                to 500ms. With shorter values the model will respond more
                quickly, 

                but may jump in on short pauses from the user.
            create_response:
              type: boolean
              default: true
              description: |
                Whether or not to automatically generate a response when VAD is
                enabled. `true` by default.
        tools:
          type: array
          description: Tools (functions) available to the model.
          items:
            type: object
            properties:
              type:
                type: string
                enum:
                  - function
                description: The type of the tool, i.e. `function`.
                x-stainless-const: true
              name:
                type: string
                description: The name of the function.
              description:
                type: string
                description: >
                  The description of the function, including guidance on when
                  and how 

                  to call it, and guidance about what to tell the user when
                  calling 

                  (if anything).
              parameters:
                type: object
                description: Parameters of the function in JSON Schema.
        tool_choice:
          type: string
          description: >
            How the model chooses tools. Options are `auto`, `none`, `required`,
            or 

            specify a function.
        temperature:
          type: number
          description: >
            Sampling temperature for the model, limited to [0.6, 1.2]. Defaults
            to 0.8.
        max_response_output_tokens:
          oneOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls. Provide an integer between 1 and 4096 to
            limit output tokens, or `inf` for the maximum available tokens for a
            given model. Defaults to `inf`.
    RealtimeSessionCreateResponse:
      type: object
      description: >
        A new Realtime session configuration, with an ephermeral key. Default
        TTL

        for keys is one minute.
      properties:
        client_secret:
          type: object
          description: Ephemeral key returned by the API.
          properties:
            value:
              type: string
              description: >
                Ephemeral key usable in client environments to authenticate
                connections

                to the Realtime API. Use this in client-side environments rather
                than

                a standard API token, which should only be used server-side.
            expires_at:
              type: integer
              description: >
                Timestamp for when the token expires. Currently, all tokens
                expire

                after one minute.
          required:
            - value
            - expires_at
        modalities:
          description: |
            The set of modalities the model can respond with. To disable audio,
            set this to ["text"].
          items:
            type: string
            enum:
              - text
              - audio
        instructions:
          type: string
          description: >
            The default system instructions (i.e. system message) prepended to
            model 

            calls. This field allows the client to guide the model on desired 

            responses. The model can be instructed on response content and
            format, 

            (e.g. "be extremely succinct", "act friendly", "here are examples of
            good 

            responses") and on audio behavior (e.g. "talk quickly", "inject
            emotion 

            into your voice", "laugh frequently"). The instructions are not
            guaranteed 

            to be followed by the model, but they provide guidance to the model
            on the 

            desired behavior.


            Note that the server sets default instructions which will be used if
            this 

            field is not set and are visible in the `session.created` event at
            the 

            start of the session.
        voice:
          type: string
          enum:
            - alloy
            - ash
            - ballad
            - coral
            - echo
            - sage
            - shimmer
            - verse
          description: >
            The voice the model uses to respond. Voice cannot be changed during
            the 

            session once the model has responded with audio at least once.
            Current 

            voice options are `alloy`, `ash`, `ballad`, `coral`, `echo` `sage`, 

            `shimmer` and `verse`.
        input_audio_format:
          type: string
          description: >
            The format of input audio. Options are `pcm16`, `g711_ulaw`, or
            `g711_alaw`.
        output_audio_format:
          type: string
          description: >
            The format of output audio. Options are `pcm16`, `g711_ulaw`, or
            `g711_alaw`.
        input_audio_transcription:
          type: object
          description: >
            Configuration for input audio transcription, defaults to off and can
            be 

            set to `null` to turn off once on. Input audio transcription is not
            native 

            to the model, since the model consumes audio directly. Transcription
            runs 

            asynchronously through Whisper and should be treated as rough
            guidance 

            rather than the representation understood by the model.
          properties:
            model:
              type: string
              description: >
                The model to use for transcription, `whisper-1` is the only
                currently 

                supported model.
        turn_detection:
          type: object
          description: >
            Configuration for turn detection. Can be set to `null` to turn off.
            Server 

            VAD means that the model will detect the start and end of speech
            based on 

            audio volume and respond at the end of user speech.
          properties:
            type:
              type: string
              description: >
                Type of turn detection, only `server_vad` is currently
                supported.
            threshold:
              type: number
              description: >
                Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5.
                A 

                higher threshold will require louder audio to activate the
                model, and 

                thus might perform better in noisy environments.
            prefix_padding_ms:
              type: integer
              description: |
                Amount of audio to include before the VAD detected speech (in 
                milliseconds). Defaults to 300ms.
            silence_duration_ms:
              type: integer
              description: >
                Duration of silence to detect speech stop (in milliseconds).
                Defaults 

                to 500ms. With shorter values the model will respond more
                quickly, 

                but may jump in on short pauses from the user.
        tools:
          type: array
          description: Tools (functions) available to the model.
          items:
            type: object
            properties:
              type:
                type: string
                enum:
                  - function
                description: The type of the tool, i.e. `function`.
                x-stainless-const: true
              name:
                type: string
                description: The name of the function.
              description:
                type: string
                description: >
                  The description of the function, including guidance on when
                  and how 

                  to call it, and guidance about what to tell the user when
                  calling 

                  (if anything).
              parameters:
                type: object
                description: Parameters of the function in JSON Schema.
        tool_choice:
          type: string
          description: >
            How the model chooses tools. Options are `auto`, `none`, `required`,
            or 

            specify a function.
        temperature:
          type: number
          description: >
            Sampling temperature for the model, limited to [0.6, 1.2]. Defaults
            to 0.8.
        max_response_output_tokens:
          oneOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls. Provide an integer between 1 and 4096 to
            limit output tokens, or `inf` for the maximum available tokens for a
            given model. Defaults to `inf`.
      required:
        - client_secret
      x-oaiMeta:
        name: The session object
        group: realtime
        example: |
          {
            "id": "sess_001",
            "object": "realtime.session",
            "model": "gpt-4o-realtime-preview-2024-12-17",
            "modalities": ["audio", "text"],
            "instructions": "You are a friendly assistant.",
            "voice": "alloy",
            "input_audio_format": "pcm16",
            "output_audio_format": "pcm16",
            "input_audio_transcription": {
                "model": "whisper-1"
            },
            "turn_detection": null,
            "tools": [],
            "tool_choice": "none",
            "temperature": 0.7,
            "max_response_output_tokens": 200,
            "client_secret": {
              "value": "ek_abc123", 
              "expires_at": 1234567890
            }
          }
    ReasoningEffort:
      type: string
      enum:
        - low
        - medium
        - high
      default: medium
      nullable: true
      description: |
        **o1 and o3-mini models only** 

        Constrains effort on reasoning for 
        [reasoning models](https://platform.openai.com/docs/guides/reasoning).
        Currently supported values are `low`, `medium`, and `high`. Reducing
        reasoning effort can result in faster responses and fewer tokens used
        on reasoning in a response.
    ResponseFormatJsonObject:
      type: object
      properties:
        type:
          type: string
          description: "The type of response format being defined: `json_object`"
          enum:
            - json_object
          x-stainless-const: true
      required:
        - type
    ResponseFormatJsonSchema:
      type: object
      properties:
        type:
          type: string
          description: "The type of response format being defined: `json_schema`"
          enum:
            - json_schema
          x-stainless-const: true
        json_schema:
          type: object
          properties:
            description:
              type: string
              description: >-
                A description of what the response format is for, used by the
                model to determine how to respond in the format.
            name:
              type: string
              description: >-
                The name of the response format. Must be a-z, A-Z, 0-9, or
                contain underscores and dashes, with a maximum length of 64.
            schema:
              $ref: "#/components/schemas/ResponseFormatJsonSchemaSchema"
            strict:
              type: boolean
              nullable: true
              default: false
              description: >-
                Whether to enable strict schema adherence when generating the
                output. If set to true, the model will always follow the exact
                schema defined in the `schema` field. Only a subset of JSON
                Schema is supported when `strict` is `true`. To learn more, read
                the [Structured Outputs guide](/docs/guides/structured-outputs).
          required:
            - name
      required:
        - type
        - json_schema
    ResponseFormatJsonSchemaSchema:
      type: object
      description: The schema for the response format, described as a JSON Schema object.
      additionalProperties: true
    ResponseFormatText:
      type: object
      properties:
        type:
          type: string
          description: "The type of response format being defined: `text`"
          enum:
            - text
          x-stainless-const: true
      required:
        - type
    RunCompletionUsage:
      type: object
      description: >-
        Usage statistics related to the run. This value will be `null` if the
        run is not in a terminal state (i.e. `in_progress`, `queued`, etc.).
      properties:
        completion_tokens:
          type: integer
          description: Number of completion tokens used over the course of the run.
        prompt_tokens:
          type: integer
          description: Number of prompt tokens used over the course of the run.
        total_tokens:
          type: integer
          description: Total number of tokens used (prompt + completion).
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
      nullable: true
    RunObject:
      type: object
      title: A run on a thread
      description: Represents an execution run on a [thread](/docs/api-reference/threads).
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `thread.run`.
          type: string
          enum:
            - thread.run
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the run was created.
          type: integer
        thread_id:
          description: >-
            The ID of the [thread](/docs/api-reference/threads) that was
            executed on as a part of this run.
          type: string
        assistant_id:
          description: >-
            The ID of the [assistant](/docs/api-reference/assistants) used for
            execution of this run.
          type: string
        status:
          description: >-
            The status of the run, which can be either `queued`, `in_progress`,
            `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`,
            `incomplete`, or `expired`.
          type: string
          enum:
            - queued
            - in_progress
            - requires_action
            - cancelling
            - cancelled
            - failed
            - completed
            - incomplete
            - expired
        required_action:
          type: object
          description: >-
            Details on the action required to continue the run. Will be `null`
            if no action is required.
          nullable: true
          properties:
            type:
              description: For now, this is always `submit_tool_outputs`.
              type: string
              enum:
                - submit_tool_outputs
              x-stainless-const: true
            submit_tool_outputs:
              type: object
              description: Details on the tool outputs needed for this run to continue.
              properties:
                tool_calls:
                  type: array
                  description: A list of the relevant tool calls.
                  items:
                    $ref: "#/components/schemas/RunToolCallObject"
              required:
                - tool_calls
          required:
            - type
            - submit_tool_outputs
        last_error:
          type: object
          description: >-
            The last error associated with this run. Will be `null` if there are
            no errors.
          nullable: true
          properties:
            code:
              type: string
              description: >-
                One of `server_error`, `rate_limit_exceeded`, or
                `invalid_prompt`.
              enum:
                - server_error
                - rate_limit_exceeded
                - invalid_prompt
            message:
              type: string
              description: A human-readable description of the error.
          required:
            - code
            - message
        expires_at:
          description: The Unix timestamp (in seconds) for when the run will expire.
          type: integer
          nullable: true
        started_at:
          description: The Unix timestamp (in seconds) for when the run was started.
          type: integer
          nullable: true
        cancelled_at:
          description: The Unix timestamp (in seconds) for when the run was cancelled.
          type: integer
          nullable: true
        failed_at:
          description: The Unix timestamp (in seconds) for when the run failed.
          type: integer
          nullable: true
        completed_at:
          description: The Unix timestamp (in seconds) for when the run was completed.
          type: integer
          nullable: true
        incomplete_details:
          description: >-
            Details on why the run is incomplete. Will be `null` if the run is
            not incomplete.
          type: object
          nullable: true
          properties:
            reason:
              description: >-
                The reason why the run is incomplete. This will point to which
                specific token limit was reached over the course of the run.
              type: string
              enum:
                - max_completion_tokens
                - max_prompt_tokens
        model:
          description: >-
            The model that the [assistant](/docs/api-reference/assistants) used
            for this run.
          type: string
        instructions:
          description: >-
            The instructions that the
            [assistant](/docs/api-reference/assistants) used for this run.
          type: string
        tools:
          description: >-
            The list of tools that the
            [assistant](/docs/api-reference/assistants) used for this run.
          default: []
          type: array
          maxItems: 20
          items:
            oneOf:
              - $ref: "#/components/schemas/AssistantToolsCode"
              - $ref: "#/components/schemas/AssistantToolsFileSearch"
              - $ref: "#/components/schemas/AssistantToolsFunction"
            x-oaiExpandable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
        usage:
          $ref: "#/components/schemas/RunCompletionUsage"
        temperature:
          description: >-
            The sampling temperature used for this run. If not set, defaults to
            1.
          type: number
          nullable: true
        top_p:
          description: >-
            The nucleus sampling value used for this run. If not set, defaults
            to 1.
          type: number
          nullable: true
        max_prompt_tokens:
          type: integer
          nullable: true
          description: >
            The maximum number of prompt tokens specified to have been used over
            the course of the run.
          minimum: 256
        max_completion_tokens:
          type: integer
          nullable: true
          description: >
            The maximum number of completion tokens specified to have been used
            over the course of the run.
          minimum: 256
        truncation_strategy:
          allOf:
            - $ref: "#/components/schemas/TruncationObject"
            - nullable: true
        tool_choice:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiToolChoiceOption"
            - nullable: true
        parallel_tool_calls:
          $ref: "#/components/schemas/ParallelToolCalls"
        response_format:
          allOf:
            - $ref: "#/components/schemas/AssistantsApiResponseFormatOption"
            - nullable: true
      required:
        - id
        - object
        - created_at
        - thread_id
        - assistant_id
        - status
        - required_action
        - last_error
        - expires_at
        - started_at
        - cancelled_at
        - failed_at
        - completed_at
        - model
        - instructions
        - tools
        - metadata
        - usage
        - incomplete_details
        - max_prompt_tokens
        - max_completion_tokens
        - truncation_strategy
        - tool_choice
        - parallel_tool_calls
        - response_format
      x-oaiMeta:
        name: The run object
        beta: true
        example: |
          {
            "id": "run_abc123",
            "object": "thread.run",
            "created_at": 1698107661,
            "assistant_id": "asst_abc123",
            "thread_id": "thread_abc123",
            "status": "completed",
            "started_at": 1699073476,
            "expires_at": null,
            "cancelled_at": null,
            "failed_at": null,
            "completed_at": 1699073498,
            "last_error": null,
            "model": "gpt-4o",
            "instructions": null,
            "tools": [{"type": "file_search"}, {"type": "code_interpreter"}],
            "metadata": {},
            "incomplete_details": null,
            "usage": {
              "prompt_tokens": 123,
              "completion_tokens": 456,
              "total_tokens": 579
            },
            "temperature": 1.0,
            "top_p": 1.0,
            "max_prompt_tokens": 1000,
            "max_completion_tokens": 1000,
            "truncation_strategy": {
              "type": "auto",
              "last_messages": null
            },
            "response_format": "auto",
            "tool_choice": "auto",
            "parallel_tool_calls": true
          }
    RunStepCompletionUsage:
      type: object
      description: >-
        Usage statistics related to the run step. This value will be `null`
        while the run step's status is `in_progress`.
      properties:
        completion_tokens:
          type: integer
          description: Number of completion tokens used over the course of the run step.
        prompt_tokens:
          type: integer
          description: Number of prompt tokens used over the course of the run step.
        total_tokens:
          type: integer
          description: Total number of tokens used (prompt + completion).
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
      nullable: true
    RunStepDeltaObject:
      type: object
      title: Run step delta object
      description: >
        Represents a run step delta i.e. any changed fields on a run step during
        streaming.
      properties:
        id:
          description: >-
            The identifier of the run step, which can be referenced in API
            endpoints.
          type: string
        object:
          description: The object type, which is always `thread.run.step.delta`.
          type: string
          enum:
            - thread.run.step.delta
          x-stainless-const: true
        delta:
          description: The delta containing the fields that have changed on the run step.
          type: object
          properties:
            step_details:
              type: object
              description: The details of the run step.
              oneOf:
                - $ref: >-
                    #/components/schemas/RunStepDeltaStepDetailsMessageCreationObject
                - $ref: "#/components/schemas/RunStepDeltaStepDetailsToolCallsObject"
              x-oaiExpandable: true
      required:
        - id
        - object
        - delta
      x-oaiMeta:
        name: The run step delta object
        beta: true
        example: |
          {
            "id": "step_123",
            "object": "thread.run.step.delta",
            "delta": {
              "step_details": {
                "type": "tool_calls",
                "tool_calls": [
                  {
                    "index": 0,
                    "id": "call_123",
                    "type": "code_interpreter",
                    "code_interpreter": { "input": "", "outputs": [] }
                  }
                ]
              }
            }
          }
    RunStepDeltaStepDetailsMessageCreationObject:
      title: Message creation
      type: object
      description: Details of the message creation by the run step.
      properties:
        type:
          description: Always `message_creation`.
          type: string
          enum:
            - message_creation
          x-stainless-const: true
        message_creation:
          type: object
          properties:
            message_id:
              type: string
              description: The ID of the message that was created by this run step.
      required:
        - type
    RunStepDeltaStepDetailsToolCallsCodeObject:
      title: Code interpreter tool call
      type: object
      description: Details of the Code Interpreter tool call the run step was involved in.
      properties:
        index:
          type: integer
          description: The index of the tool call in the tool calls array.
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          description: >-
            The type of tool call. This is always going to be `code_interpreter`
            for this type of tool call.
          enum:
            - code_interpreter
          x-stainless-const: true
        code_interpreter:
          type: object
          description: The Code Interpreter tool call definition.
          properties:
            input:
              type: string
              description: The input to the Code Interpreter tool call.
            outputs:
              type: array
              description: >-
                The outputs from the Code Interpreter tool call. Code
                Interpreter can output one or more items, including text
                (`logs`) or images (`image`). Each of these are represented by a
                different object type.
              items:
                type: object
                oneOf:
                  - $ref: >-
                      #/components/schemas/RunStepDeltaStepDetailsToolCallsCodeOutputLogsObject
                  - $ref: >-
                      #/components/schemas/RunStepDeltaStepDetailsToolCallsCodeOutputImageObject
                x-oaiExpandable: true
      required:
        - index
        - type
    RunStepDeltaStepDetailsToolCallsCodeOutputImageObject:
      title: Code interpreter image output
      type: object
      properties:
        index:
          type: integer
          description: The index of the output in the outputs array.
        type:
          description: Always `image`.
          type: string
          enum:
            - image
          x-stainless-const: true
        image:
          type: object
          properties:
            file_id:
              description: The [file](/docs/api-reference/files) ID of the image.
              type: string
      required:
        - index
        - type
    RunStepDeltaStepDetailsToolCallsCodeOutputLogsObject:
      title: Code interpreter log output
      type: object
      description: Text output from the Code Interpreter tool call as part of a run step.
      properties:
        index:
          type: integer
          description: The index of the output in the outputs array.
        type:
          description: Always `logs`.
          type: string
          enum:
            - logs
          x-stainless-const: true
        logs:
          type: string
          description: The text output from the Code Interpreter tool call.
      required:
        - index
        - type
    RunStepDeltaStepDetailsToolCallsFileSearchObject:
      title: File search tool call
      type: object
      properties:
        index:
          type: integer
          description: The index of the tool call in the tool calls array.
        id:
          type: string
          description: The ID of the tool call object.
        type:
          type: string
          description: >-
            The type of tool call. This is always going to be `file_search` for
            this type of tool call.
          enum:
            - file_search
          x-stainless-const: true
        file_search:
          type: object
          description: For now, this is always going to be an empty object.
          x-oaiTypeLabel: map
      required:
        - index
        - type
        - file_search
    RunStepDeltaStepDetailsToolCallsFunctionObject:
      type: object
      title: Function tool call
      properties:
        index:
          type: integer
          description: The index of the tool call in the tool calls array.
        id:
          type: string
          description: The ID of the tool call object.
        type:
          type: string
          description: >-
            The type of tool call. This is always going to be `function` for
            this type of tool call.
          enum:
            - function
          x-stainless-const: true
        function:
          type: object
          description: The definition of the function that was called.
          properties:
            name:
              type: string
              description: The name of the function.
            arguments:
              type: string
              description: The arguments passed to the function.
            output:
              type: string
              description: >-
                The output of the function. This will be `null` if the outputs
                have not been
                [submitted](/docs/api-reference/runs/submitToolOutputs) yet.
              nullable: true
      required:
        - index
        - type
    RunStepDeltaStepDetailsToolCallsObject:
      title: Tool calls
      type: object
      description: Details of the tool call.
      properties:
        type:
          description: Always `tool_calls`.
          type: string
          enum:
            - tool_calls
          x-stainless-const: true
        tool_calls:
          type: array
          description: >
            An array of tool calls the run step was involved in. These can be
            associated with one of three types of tools: `code_interpreter`,
            `file_search`, or `function`.
          items:
            oneOf:
              - $ref: >-
                  #/components/schemas/RunStepDeltaStepDetailsToolCallsCodeObject
              - $ref: >-
                  #/components/schemas/RunStepDeltaStepDetailsToolCallsFileSearchObject
              - $ref: >-
                  #/components/schemas/RunStepDeltaStepDetailsToolCallsFunctionObject
            x-oaiExpandable: true
      required:
        - type
    RunStepDetailsMessageCreationObject:
      title: Message creation
      type: object
      description: Details of the message creation by the run step.
      properties:
        type:
          description: Always `message_creation`.
          type: string
          enum:
            - message_creation
          x-stainless-const: true
        message_creation:
          type: object
          properties:
            message_id:
              type: string
              description: The ID of the message that was created by this run step.
          required:
            - message_id
      required:
        - type
        - message_creation
    RunStepDetailsToolCallsCodeObject:
      title: Code Interpreter tool call
      type: object
      description: Details of the Code Interpreter tool call the run step was involved in.
      properties:
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          description: >-
            The type of tool call. This is always going to be `code_interpreter`
            for this type of tool call.
          enum:
            - code_interpreter
          x-stainless-const: true
        code_interpreter:
          type: object
          description: The Code Interpreter tool call definition.
          required:
            - input
            - outputs
          properties:
            input:
              type: string
              description: The input to the Code Interpreter tool call.
            outputs:
              type: array
              description: >-
                The outputs from the Code Interpreter tool call. Code
                Interpreter can output one or more items, including text
                (`logs`) or images (`image`). Each of these are represented by a
                different object type.
              items:
                type: object
                oneOf:
                  - $ref: >-
                      #/components/schemas/RunStepDetailsToolCallsCodeOutputLogsObject
                  - $ref: >-
                      #/components/schemas/RunStepDetailsToolCallsCodeOutputImageObject
                x-oaiExpandable: true
      required:
        - id
        - type
        - code_interpreter
    RunStepDetailsToolCallsCodeOutputImageObject:
      title: Code Interpreter image output
      type: object
      properties:
        type:
          description: Always `image`.
          type: string
          enum:
            - image
          x-stainless-const: true
        image:
          type: object
          properties:
            file_id:
              description: The [file](/docs/api-reference/files) ID of the image.
              type: string
          required:
            - file_id
      required:
        - type
        - image
    RunStepDetailsToolCallsCodeOutputLogsObject:
      title: Code Interpreter log output
      type: object
      description: Text output from the Code Interpreter tool call as part of a run step.
      properties:
        type:
          description: Always `logs`.
          type: string
          enum:
            - logs
          x-stainless-const: true
        logs:
          type: string
          description: The text output from the Code Interpreter tool call.
      required:
        - type
        - logs
    RunStepDetailsToolCallsFileSearchObject:
      title: File search tool call
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call object.
        type:
          type: string
          description: >-
            The type of tool call. This is always going to be `file_search` for
            this type of tool call.
          enum:
            - file_search
          x-stainless-const: true
        file_search:
          type: object
          description: For now, this is always going to be an empty object.
          x-oaiTypeLabel: map
          properties:
            ranking_options:
              $ref: >-
                #/components/schemas/RunStepDetailsToolCallsFileSearchRankingOptionsObject
            results:
              type: array
              description: The results of the file search.
              items:
                $ref: >-
                  #/components/schemas/RunStepDetailsToolCallsFileSearchResultObject
      required:
        - id
        - type
        - file_search
    RunStepDetailsToolCallsFileSearchRankingOptionsObject:
      title: File search tool call ranking options
      type: object
      description: The ranking options for the file search.
      properties:
        ranker:
          type: string
          description: The ranker used for the file search.
          enum:
            - default_2024_08_21
          x-stainless-const: true
        score_threshold:
          type: number
          description: >-
            The score threshold for the file search. All values must be a
            floating point number between 0 and 1.
          minimum: 0
          maximum: 1
      required:
        - ranker
        - score_threshold
    RunStepDetailsToolCallsFileSearchResultObject:
      title: File search tool call result
      type: object
      description: A result instance of the file search.
      x-oaiTypeLabel: map
      properties:
        file_id:
          type: string
          description: The ID of the file that result was found in.
        file_name:
          type: string
          description: The name of the file that result was found in.
        score:
          type: number
          description: >-
            The score of the result. All values must be a floating point number
            between 0 and 1.
          minimum: 0
          maximum: 1
        content:
          type: array
          description: >-
            The content of the result that was found. The content is only
            included if requested via the include query parameter.
          items:
            type: object
            properties:
              type:
                type: string
                description: The type of the content.
                enum:
                  - text
                x-stainless-const: true
              text:
                type: string
                description: The text content of the file.
      required:
        - file_id
        - file_name
        - score
    RunStepDetailsToolCallsFunctionObject:
      type: object
      title: Function tool call
      properties:
        id:
          type: string
          description: The ID of the tool call object.
        type:
          type: string
          description: >-
            The type of tool call. This is always going to be `function` for
            this type of tool call.
          enum:
            - function
          x-stainless-const: true
        function:
          type: object
          description: The definition of the function that was called.
          properties:
            name:
              type: string
              description: The name of the function.
            arguments:
              type: string
              description: The arguments passed to the function.
            output:
              type: string
              description: >-
                The output of the function. This will be `null` if the outputs
                have not been
                [submitted](/docs/api-reference/runs/submitToolOutputs) yet.
              nullable: true
          required:
            - name
            - arguments
            - output
      required:
        - id
        - type
        - function
    RunStepDetailsToolCallsObject:
      title: Tool calls
      type: object
      description: Details of the tool call.
      properties:
        type:
          description: Always `tool_calls`.
          type: string
          enum:
            - tool_calls
          x-stainless-const: true
        tool_calls:
          type: array
          description: >
            An array of tool calls the run step was involved in. These can be
            associated with one of three types of tools: `code_interpreter`,
            `file_search`, or `function`.
          items:
            oneOf:
              - $ref: "#/components/schemas/RunStepDetailsToolCallsCodeObject"
              - $ref: "#/components/schemas/RunStepDetailsToolCallsFileSearchObject"
              - $ref: "#/components/schemas/RunStepDetailsToolCallsFunctionObject"
            x-oaiExpandable: true
      required:
        - type
        - tool_calls
    RunStepObject:
      type: object
      title: Run steps
      description: |
        Represents a step in execution of a run.
      properties:
        id:
          description: >-
            The identifier of the run step, which can be referenced in API
            endpoints.
          type: string
        object:
          description: The object type, which is always `thread.run.step`.
          type: string
          enum:
            - thread.run.step
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the run step was created.
          type: integer
        assistant_id:
          description: >-
            The ID of the [assistant](/docs/api-reference/assistants) associated
            with the run step.
          type: string
        thread_id:
          description: The ID of the [thread](/docs/api-reference/threads) that was run.
          type: string
        run_id:
          description: >-
            The ID of the [run](/docs/api-reference/runs) that this run step is
            a part of.
          type: string
        type:
          description: >-
            The type of run step, which can be either `message_creation` or
            `tool_calls`.
          type: string
          enum:
            - message_creation
            - tool_calls
        status:
          description: >-
            The status of the run step, which can be either `in_progress`,
            `cancelled`, `failed`, `completed`, or `expired`.
          type: string
          enum:
            - in_progress
            - cancelled
            - failed
            - completed
            - expired
        step_details:
          type: object
          description: The details of the run step.
          oneOf:
            - $ref: "#/components/schemas/RunStepDetailsMessageCreationObject"
            - $ref: "#/components/schemas/RunStepDetailsToolCallsObject"
          x-oaiExpandable: true
        last_error:
          type: object
          description: >-
            The last error associated with this run step. Will be `null` if
            there are no errors.
          nullable: true
          properties:
            code:
              type: string
              description: One of `server_error` or `rate_limit_exceeded`.
              enum:
                - server_error
                - rate_limit_exceeded
            message:
              type: string
              description: A human-readable description of the error.
          required:
            - code
            - message
        expired_at:
          description: >-
            The Unix timestamp (in seconds) for when the run step expired. A
            step is considered expired if the parent run is expired.
          type: integer
          nullable: true
        cancelled_at:
          description: The Unix timestamp (in seconds) for when the run step was cancelled.
          type: integer
          nullable: true
        failed_at:
          description: The Unix timestamp (in seconds) for when the run step failed.
          type: integer
          nullable: true
        completed_at:
          description: The Unix timestamp (in seconds) for when the run step completed.
          type: integer
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
        usage:
          $ref: "#/components/schemas/RunStepCompletionUsage"
      required:
        - id
        - object
        - created_at
        - assistant_id
        - thread_id
        - run_id
        - type
        - status
        - step_details
        - last_error
        - expired_at
        - cancelled_at
        - failed_at
        - completed_at
        - metadata
        - usage
      x-oaiMeta:
        name: The run step object
        beta: true
        example: |
          {
            "id": "step_abc123",
            "object": "thread.run.step",
            "created_at": 1699063291,
            "run_id": "run_abc123",
            "assistant_id": "asst_abc123",
            "thread_id": "thread_abc123",
            "type": "message_creation",
            "status": "completed",
            "cancelled_at": null,
            "completed_at": 1699063291,
            "expired_at": null,
            "failed_at": null,
            "last_error": null,
            "step_details": {
              "type": "message_creation",
              "message_creation": {
                "message_id": "msg_abc123"
              }
            },
            "usage": {
              "prompt_tokens": 123,
              "completion_tokens": 456,
              "total_tokens": 579
            }
          }
    RunStepStreamEvent:
      oneOf:
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.step.created
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunStepObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run step](/docs/api-reference/run-steps/step-object)
            is created.
          x-oaiMeta:
            dataDescription: "`data` is a [run step](/docs/api-reference/run-steps/step-object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.step.in_progress
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunStepObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run step](/docs/api-reference/run-steps/step-object)
            moves to an `in_progress` state.
          x-oaiMeta:
            dataDescription: "`data` is a [run step](/docs/api-reference/run-steps/step-object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.step.delta
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunStepDeltaObject"
          required:
            - event
            - data
          description: >-
            Occurs when parts of a [run
            step](/docs/api-reference/run-steps/step-object) are being streamed.
          x-oaiMeta:
            dataDescription: >-
              `data` is a [run step
              delta](/docs/api-reference/assistants-streaming/run-step-delta-object)
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.step.completed
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunStepObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run step](/docs/api-reference/run-steps/step-object)
            is completed.
          x-oaiMeta:
            dataDescription: "`data` is a [run step](/docs/api-reference/run-steps/step-object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.step.failed
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunStepObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run step](/docs/api-reference/run-steps/step-object)
            fails.
          x-oaiMeta:
            dataDescription: "`data` is a [run step](/docs/api-reference/run-steps/step-object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.step.cancelled
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunStepObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run step](/docs/api-reference/run-steps/step-object)
            is cancelled.
          x-oaiMeta:
            dataDescription: "`data` is a [run step](/docs/api-reference/run-steps/step-object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.step.expired
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunStepObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run step](/docs/api-reference/run-steps/step-object)
            expires.
          x-oaiMeta:
            dataDescription: "`data` is a [run step](/docs/api-reference/run-steps/step-object)"
    RunStreamEvent:
      oneOf:
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.created
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: Occurs when a new [run](/docs/api-reference/runs/object) is created.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.queued
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run](/docs/api-reference/runs/object) moves to a
            `queued` status.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.in_progress
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run](/docs/api-reference/runs/object) moves to an
            `in_progress` status.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.requires_action
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run](/docs/api-reference/runs/object) moves to a
            `requires_action` status.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.completed
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: Occurs when a [run](/docs/api-reference/runs/object) is completed.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.incomplete
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run](/docs/api-reference/runs/object) ends with
            status `incomplete`.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.failed
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: Occurs when a [run](/docs/api-reference/runs/object) fails.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.cancelling
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: >-
            Occurs when a [run](/docs/api-reference/runs/object) moves to a
            `cancelling` status.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.cancelled
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: Occurs when a [run](/docs/api-reference/runs/object) is cancelled.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
        - type: object
          properties:
            event:
              type: string
              enum:
                - thread.run.expired
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/RunObject"
          required:
            - event
            - data
          description: Occurs when a [run](/docs/api-reference/runs/object) expires.
          x-oaiMeta:
            dataDescription: "`data` is a [run](/docs/api-reference/runs/object)"
    RunToolCallObject:
      type: object
      description: Tool call objects
      properties:
        id:
          type: string
          description: >-
            The ID of the tool call. This ID must be referenced when you submit
            the tool outputs in using the [Submit tool outputs to
            run](/docs/api-reference/runs/submitToolOutputs) endpoint.
        type:
          type: string
          description: >-
            The type of tool call the output is required for. For now, this is
            always `function`.
          enum:
            - function
          x-stainless-const: true
        function:
          type: object
          description: The function definition.
          properties:
            name:
              type: string
              description: The name of the function.
            arguments:
              type: string
              description: >-
                The arguments that the model expects you to pass to the
                function.
          required:
            - name
            - arguments
      required:
        - id
        - type
        - function
    StaticChunkingStrategy:
      type: object
      additionalProperties: false
      properties:
        max_chunk_size_tokens:
          type: integer
          minimum: 100
          maximum: 4096
          description: >-
            The maximum number of tokens in each chunk. The default value is
            `800`. The minimum value is `100` and the maximum value is `4096`.
        chunk_overlap_tokens:
          type: integer
          description: >
            The number of tokens that overlap between chunks. The default value
            is `400`.


            Note that the overlap must not exceed half of
            `max_chunk_size_tokens`.
      required:
        - max_chunk_size_tokens
        - chunk_overlap_tokens
    StaticChunkingStrategyRequestParam:
      type: object
      title: Static Chunking Strategy
      additionalProperties: false
      properties:
        type:
          type: string
          description: Always `static`.
          enum:
            - static
          x-stainless-const: true
        static:
          $ref: "#/components/schemas/StaticChunkingStrategy"
      required:
        - type
        - static
    StaticChunkingStrategyResponseParam:
      type: object
      title: Static Chunking Strategy
      additionalProperties: false
      properties:
        type:
          type: string
          description: Always `static`.
          enum:
            - static
          x-stainless-const: true
        static:
          $ref: "#/components/schemas/StaticChunkingStrategy"
      required:
        - type
        - static
    SubmitToolOutputsRunRequest:
      type: object
      additionalProperties: false
      properties:
        tool_outputs:
          description: A list of tools for which the outputs are being submitted.
          type: array
          items:
            type: object
            properties:
              tool_call_id:
                type: string
                description: >-
                  The ID of the tool call in the `required_action` object within
                  the run object the output is being submitted for.
              output:
                type: string
                description: >-
                  The output of the tool call to be submitted to continue the
                  run.
        stream:
          type: boolean
          nullable: true
          description: >
            If `true`, returns a stream of events that happen during the Run as
            server-sent events, terminating when the Run enters a terminal state
            with a `data: [DONE]` message.
      required:
        - tool_outputs
    ThreadObject:
      type: object
      title: Thread
      description: >-
        Represents a thread that contains
        [messages](/docs/api-reference/messages).
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `thread`.
          type: string
          enum:
            - thread
          x-stainless-const: true
        created_at:
          description: The Unix timestamp (in seconds) for when the thread was created.
          type: integer
        tool_resources:
          type: object
          description: >
            A set of resources that are made available to the assistant's tools
            in this thread. The resources are specific to the type of tool. For
            example, the `code_interpreter` tool requires a list of file IDs,
            while the `file_search` tool requires a list of vector store IDs.
          properties:
            code_interpreter:
              type: object
              properties:
                file_ids:
                  type: array
                  description: >
                    A list of [file](/docs/api-reference/files) IDs made
                    available to the `code_interpreter` tool. There can be a
                    maximum of 20 files associated with the tool.
                  default: []
                  maxItems: 20
                  items:
                    type: string
            file_search:
              type: object
              properties:
                vector_store_ids:
                  type: array
                  description: >
                    The [vector store](/docs/api-reference/vector-stores/object)
                    attached to this thread. There can be a maximum of 1 vector
                    store attached to the thread.
                  maxItems: 1
                  items:
                    type: string
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
      required:
        - id
        - object
        - created_at
        - tool_resources
        - metadata
      x-oaiMeta:
        name: The thread object
        beta: true
        example: |
          {
            "id": "thread_abc123",
            "object": "thread",
            "created_at": 1698107661,
            "metadata": {}
          }
    ThreadStreamEvent:
      oneOf:
        - type: object
          properties:
            enabled:
              type: boolean
              description: Whether to enable input audio transcription.
            event:
              type: string
              enum:
                - thread.created
              x-stainless-const: true
            data:
              $ref: "#/components/schemas/ThreadObject"
          required:
            - event
            - data
          description: >-
            Occurs when a new [thread](/docs/api-reference/threads/object) is
            created.
          x-oaiMeta:
            dataDescription: "`data` is a [thread](/docs/api-reference/threads/object)"
    TranscriptionSegment:
      type: object
      properties:
        id:
          type: integer
          description: Unique identifier of the segment.
        seek:
          type: integer
          description: Seek offset of the segment.
        start:
          type: number
          format: float
          description: Start time of the segment in seconds.
        end:
          type: number
          format: float
          description: End time of the segment in seconds.
        text:
          type: string
          description: Text content of the segment.
        tokens:
          type: array
          items:
            type: integer
          description: Array of token IDs for the text content.
        temperature:
          type: number
          format: float
          description: Temperature parameter used for generating the segment.
        avg_logprob:
          type: number
          format: float
          description: >-
            Average logprob of the segment. If the value is lower than -1,
            consider the logprobs failed.
        compression_ratio:
          type: number
          format: float
          description: >-
            Compression ratio of the segment. If the value is greater than 2.4,
            consider the compression failed.
        no_speech_prob:
          type: number
          format: float
          description: >-
            Probability of no speech in the segment. If the value is higher than
            1.0 and the `avg_logprob` is below -1, consider this segment silent.
      required:
        - id
        - seek
        - start
        - end
        - text
        - tokens
        - temperature
        - avg_logprob
        - compression_ratio
        - no_speech_prob
    TranscriptionWord:
      type: object
      properties:
        word:
          type: string
          description: The text content of the word.
        start:
          type: number
          format: float
          description: Start time of the word in seconds.
        end:
          type: number
          format: float
          description: End time of the word in seconds.
      required:
        - word
        - start
        - end
    TruncationObject:
      type: object
      title: Thread Truncation Controls
      description: >-
        Controls for how a thread will be truncated prior to the run. Use this
        to control the intial context window of the run.
      properties:
        type:
          type: string
          description: >-
            The truncation strategy to use for the thread. The default is
            `auto`. If set to `last_messages`, the thread will be truncated to
            the n most recent messages in the thread. When set to `auto`,
            messages in the middle of the thread will be dropped to fit the
            context length of the model, `max_prompt_tokens`.
          enum:
            - auto
            - last_messages
        last_messages:
          type: integer
          description: >-
            The number of most recent messages from the thread when constructing
            the context for the run.
          minimum: 1
          nullable: true
      required:
        - type
    UpdateVectorStoreRequest:
      type: object
      additionalProperties: false
      properties:
        name:
          description: The name of the vector store.
          type: string
          nullable: true
        expires_after:
          allOf:
            - $ref: "#/components/schemas/VectorStoreExpirationAfter"
            - nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
    Upload:
      type: object
      title: Upload
      description: |
        The Upload object can accept byte chunks in the form of Parts.
      properties:
        id:
          type: string
          description: >-
            The Upload unique identifier, which can be referenced in API
            endpoints.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the Upload was created.
        filename:
          type: string
          description: The name of the file to be uploaded.
        bytes:
          type: integer
          description: The intended number of bytes to be uploaded.
        purpose:
          type: string
          description: >-
            The intended purpose of the file. [Please refer
            here](/docs/api-reference/files/object#files/object-purpose) for
            acceptable values.
        status:
          type: string
          description: The status of the Upload.
          enum:
            - pending
            - completed
            - cancelled
            - expired
        expires_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the Upload was created.
        object:
          type: string
          description: The object type, which is always "upload".
          enum:
            - upload
          x-stainless-const: true
        file:
          allOf:
            - $ref: "#/components/schemas/OpenAIFile"
            - nullable: true
              description: The ready File object after the Upload is completed.
      required:
        - bytes
        - created_at
        - expires_at
        - filename
        - id
        - purpose
        - status
      x-oaiMeta:
        name: The upload object
        example: |
          {
            "id": "upload_abc123",
            "object": "upload",
            "bytes": 2147483648,
            "created_at": 1719184911,
            "filename": "training_examples.jsonl",
            "purpose": "fine-tune",
            "status": "completed",
            "expires_at": 1719127296,
            "file": {
              "id": "file-xyz321",
              "object": "file",
              "bytes": 2147483648,
              "created_at": 1719186911,
              "filename": "training_examples.jsonl",
              "purpose": "fine-tune",
            }
          }
    UploadPart:
      type: object
      title: UploadPart
      description: >
        The upload Part represents a chunk of bytes we can add to an Upload
        object.
      properties:
        id:
          type: string
          description: >-
            The upload Part unique identifier, which can be referenced in API
            endpoints.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the Part was created.
        upload_id:
          type: string
          description: The ID of the Upload object that this Part was added to.
        object:
          type: string
          description: The object type, which is always `upload.part`.
          enum:
            - upload.part
          x-stainless-const: true
      required:
        - created_at
        - id
        - object
        - upload_id
      x-oaiMeta:
        name: The upload part object
        example: |
          {
              "id": "part_def456",
              "object": "upload.part",
              "created_at": 1719186911,
              "upload_id": "upload_abc123"
          }
    UsageAudioSpeechesResult:
      type: object
      description: The aggregated audio speeches usage details of the specific time bucket.
      properties:
        object:
          type: string
          enum:
            - organization.usage.audio_speeches.result
          x-stainless-const: true
        characters:
          type: integer
          description: The number of characters processed.
        num_model_requests:
          type: integer
          description: The count of requests made to the model.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped usage result.
        user_id:
          type: string
          nullable: true
          description: >-
            When `group_by=user_id`, this field provides the user ID of the
            grouped usage result.
        api_key_id:
          type: string
          nullable: true
          description: >-
            When `group_by=api_key_id`, this field provides the API key ID of
            the grouped usage result.
        model:
          type: string
          nullable: true
          description: >-
            When `group_by=model`, this field provides the model name of the
            grouped usage result.
      required:
        - object
        - characters
        - num_model_requests
      x-oaiMeta:
        name: Audio speeches usage object
        example: |
          {
              "object": "organization.usage.audio_speeches.result",
              "characters": 45,
              "num_model_requests": 1,
              "project_id": "proj_abc",
              "user_id": "user-abc",
              "api_key_id": "key_abc",
              "model": "tts-1"
          }
    UsageAudioTranscriptionsResult:
      type: object
      description: >-
        The aggregated audio transcriptions usage details of the specific time
        bucket.
      properties:
        object:
          type: string
          enum:
            - organization.usage.audio_transcriptions.result
          x-stainless-const: true
        seconds:
          type: integer
          description: The number of seconds processed.
        num_model_requests:
          type: integer
          description: The count of requests made to the model.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped usage result.
        user_id:
          type: string
          nullable: true
          description: >-
            When `group_by=user_id`, this field provides the user ID of the
            grouped usage result.
        api_key_id:
          type: string
          nullable: true
          description: >-
            When `group_by=api_key_id`, this field provides the API key ID of
            the grouped usage result.
        model:
          type: string
          nullable: true
          description: >-
            When `group_by=model`, this field provides the model name of the
            grouped usage result.
      required:
        - object
        - seconds
        - num_model_requests
      x-oaiMeta:
        name: Audio transcriptions usage object
        example: |
          {
              "object": "organization.usage.audio_transcriptions.result",
              "seconds": 10,
              "num_model_requests": 1,
              "project_id": "proj_abc",
              "user_id": "user-abc",
              "api_key_id": "key_abc",
              "model": "tts-1"
          }
    UsageCodeInterpreterSessionsResult:
      type: object
      description: >-
        The aggregated code interpreter sessions usage details of the specific
        time bucket.
      properties:
        object:
          type: string
          enum:
            - organization.usage.code_interpreter_sessions.result
          x-stainless-const: true
        num_sessions:
          type: integer
          description: The number of code interpreter sessions.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped usage result.
      required:
        - object
        - num_sessions
      x-oaiMeta:
        name: Code interpreter sessions usage object
        example: |
          {
              "object": "organization.usage.code_interpreter_sessions.result",
              "num_sessions": 1,
              "project_id": "proj_abc"
          }
    UsageCompletionsResult:
      type: object
      description: The aggregated completions usage details of the specific time bucket.
      properties:
        object:
          type: string
          enum:
            - organization.usage.completions.result
          x-stainless-const: true
        input_tokens:
          type: integer
          description: >-
            The aggregated number of text input tokens used, including cached
            tokens. For customers subscribe to scale tier, this includes scale
            tier tokens.
        input_cached_tokens:
          type: integer
          description: >-
            The aggregated number of text input tokens that has been cached from
            previous requests. For customers subscribe to scale tier, this
            includes scale tier tokens.
        output_tokens:
          type: integer
          description: >-
            The aggregated number of text output tokens used. For customers
            subscribe to scale tier, this includes scale tier tokens.
        input_audio_tokens:
          type: integer
          description: >-
            The aggregated number of audio input tokens used, including cached
            tokens.
        output_audio_tokens:
          type: integer
          description: The aggregated number of audio output tokens used.
        num_model_requests:
          type: integer
          description: The count of requests made to the model.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped usage result.
        user_id:
          type: string
          nullable: true
          description: >-
            When `group_by=user_id`, this field provides the user ID of the
            grouped usage result.
        api_key_id:
          type: string
          nullable: true
          description: >-
            When `group_by=api_key_id`, this field provides the API key ID of
            the grouped usage result.
        model:
          type: string
          nullable: true
          description: >-
            When `group_by=model`, this field provides the model name of the
            grouped usage result.
        batch:
          type: boolean
          nullable: true
          description: >-
            When `group_by=batch`, this field tells whether the grouped usage
            result is batch or not.
      required:
        - object
        - input_tokens
        - output_tokens
        - num_model_requests
      x-oaiMeta:
        name: Completions usage object
        example: |
          {
              "object": "organization.usage.completions.result",
              "input_tokens": 5000,
              "output_tokens": 1000,
              "input_cached_tokens": 4000,
              "input_audio_tokens": 300,
              "output_audio_tokens": 200,
              "num_model_requests": 5,
              "project_id": "proj_abc",
              "user_id": "user-abc",
              "api_key_id": "key_abc",
              "model": "gpt-4o-mini-2024-07-18",
              "batch": false
          }
    UsageEmbeddingsResult:
      type: object
      description: The aggregated embeddings usage details of the specific time bucket.
      properties:
        object:
          type: string
          enum:
            - organization.usage.embeddings.result
          x-stainless-const: true
        input_tokens:
          type: integer
          description: The aggregated number of input tokens used.
        num_model_requests:
          type: integer
          description: The count of requests made to the model.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped usage result.
        user_id:
          type: string
          nullable: true
          description: >-
            When `group_by=user_id`, this field provides the user ID of the
            grouped usage result.
        api_key_id:
          type: string
          nullable: true
          description: >-
            When `group_by=api_key_id`, this field provides the API key ID of
            the grouped usage result.
        model:
          type: string
          nullable: true
          description: >-
            When `group_by=model`, this field provides the model name of the
            grouped usage result.
      required:
        - object
        - input_tokens
        - num_model_requests
      x-oaiMeta:
        name: Embeddings usage object
        example: |
          {
              "object": "organization.usage.embeddings.result",
              "input_tokens": 20,
              "num_model_requests": 2,
              "project_id": "proj_abc",
              "user_id": "user-abc",
              "api_key_id": "key_abc",
              "model": "text-embedding-ada-002-v2"
          }
    UsageImagesResult:
      type: object
      description: The aggregated images usage details of the specific time bucket.
      properties:
        object:
          type: string
          enum:
            - organization.usage.images.result
          x-stainless-const: true
        images:
          type: integer
          description: The number of images processed.
        num_model_requests:
          type: integer
          description: The count of requests made to the model.
        source:
          type: string
          nullable: true
          description: >-
            When `group_by=source`, this field provides the source of the
            grouped usage result, possible values are `image.generation`,
            `image.edit`, `image.variation`.
        size:
          type: string
          nullable: true
          description: >-
            When `group_by=size`, this field provides the image size of the
            grouped usage result.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped usage result.
        user_id:
          type: string
          nullable: true
          description: >-
            When `group_by=user_id`, this field provides the user ID of the
            grouped usage result.
        api_key_id:
          type: string
          nullable: true
          description: >-
            When `group_by=api_key_id`, this field provides the API key ID of
            the grouped usage result.
        model:
          type: string
          nullable: true
          description: >-
            When `group_by=model`, this field provides the model name of the
            grouped usage result.
      required:
        - object
        - images
        - num_model_requests
      x-oaiMeta:
        name: Images usage object
        example: |
          {
              "object": "organization.usage.images.result",
              "images": 2,
              "num_model_requests": 2,
              "size": "1024x1024",
              "source": "image.generation",
              "project_id": "proj_abc",
              "user_id": "user-abc",
              "api_key_id": "key_abc",
              "model": "dall-e-3"
          }
    UsageModerationsResult:
      type: object
      description: The aggregated moderations usage details of the specific time bucket.
      properties:
        object:
          type: string
          enum:
            - organization.usage.moderations.result
          x-stainless-const: true
        input_tokens:
          type: integer
          description: The aggregated number of input tokens used.
        num_model_requests:
          type: integer
          description: The count of requests made to the model.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped usage result.
        user_id:
          type: string
          nullable: true
          description: >-
            When `group_by=user_id`, this field provides the user ID of the
            grouped usage result.
        api_key_id:
          type: string
          nullable: true
          description: >-
            When `group_by=api_key_id`, this field provides the API key ID of
            the grouped usage result.
        model:
          type: string
          nullable: true
          description: >-
            When `group_by=model`, this field provides the model name of the
            grouped usage result.
      required:
        - object
        - input_tokens
        - num_model_requests
      x-oaiMeta:
        name: Moderations usage object
        example: |
          {
              "object": "organization.usage.moderations.result",
              "input_tokens": 20,
              "num_model_requests": 2,
              "project_id": "proj_abc",
              "user_id": "user-abc",
              "api_key_id": "key_abc",
              "model": "text-moderation"
          }
    UsageResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - page
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/UsageTimeBucket"
        has_more:
          type: boolean
        next_page:
          type: string
      required:
        - object
        - data
        - has_more
        - next_page
    UsageTimeBucket:
      type: object
      properties:
        object:
          type: string
          enum:
            - bucket
          x-stainless-const: true
        start_time:
          type: integer
        end_time:
          type: integer
        result:
          type: array
          items:
            oneOf:
              - $ref: "#/components/schemas/UsageCompletionsResult"
              - $ref: "#/components/schemas/UsageEmbeddingsResult"
              - $ref: "#/components/schemas/UsageModerationsResult"
              - $ref: "#/components/schemas/UsageImagesResult"
              - $ref: "#/components/schemas/UsageAudioSpeechesResult"
              - $ref: "#/components/schemas/UsageAudioTranscriptionsResult"
              - $ref: "#/components/schemas/UsageVectorStoresResult"
              - $ref: "#/components/schemas/UsageCodeInterpreterSessionsResult"
              - $ref: "#/components/schemas/CostsResult"
      required:
        - object
        - start_time
        - end_time
        - result
    UsageVectorStoresResult:
      type: object
      description: The aggregated vector stores usage details of the specific time bucket.
      properties:
        object:
          type: string
          enum:
            - organization.usage.vector_stores.result
          x-stainless-const: true
        usage_bytes:
          type: integer
          description: The vector stores usage in bytes.
        project_id:
          type: string
          nullable: true
          description: >-
            When `group_by=project_id`, this field provides the project ID of
            the grouped usage result.
      required:
        - object
        - usage_bytes
      x-oaiMeta:
        name: Vector stores usage object
        example: |
          {
              "object": "organization.usage.vector_stores.result",
              "usage_bytes": 1024,
              "project_id": "proj_abc"
          }
    User:
      type: object
      description: Represents an individual `user` within an organization.
      properties:
        object:
          type: string
          enum:
            - organization.user
          description: The object type, which is always `organization.user`
          x-stainless-const: true
        id:
          type: string
          description: The identifier, which can be referenced in API endpoints
        name:
          type: string
          description: The name of the user
        email:
          type: string
          description: The email address of the user
        role:
          type: string
          enum:
            - owner
            - reader
          description: "`owner` or `reader`"
        added_at:
          type: integer
          description: The Unix timestamp (in seconds) of when the user was added.
      required:
        - object
        - id
        - name
        - email
        - role
        - added_at
      x-oaiMeta:
        name: The user object
        example: |
          {
              "object": "organization.user",
              "id": "user_abc",
              "name": "First Last",
              "email": "user@example.com",
              "role": "owner",
              "added_at": 1711471533
          }
    UserDeleteResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - organization.user.deleted
          x-stainless-const: true
        id:
          type: string
        deleted:
          type: boolean
      required:
        - object
        - id
        - deleted
    UserListResponse:
      type: object
      properties:
        object:
          type: string
          enum:
            - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: "#/components/schemas/User"
        first_id:
          type: string
        last_id:
          type: string
        has_more:
          type: boolean
      required:
        - object
        - data
        - first_id
        - last_id
        - has_more
    UserRoleUpdateRequest:
      type: object
      properties:
        role:
          type: string
          enum:
            - owner
            - reader
          description: "`owner` or `reader`"
      required:
        - role
    VectorStoreExpirationAfter:
      type: object
      title: Vector store expiration policy
      description: The expiration policy for a vector store.
      properties:
        anchor:
          description: >-
            Anchor timestamp after which the expiration policy applies.
            Supported anchors: `last_active_at`.
          type: string
          enum:
            - last_active_at
          x-stainless-const: true
        days:
          description: >-
            The number of days after the anchor time that the vector store will
            expire.
          type: integer
          minimum: 1
          maximum: 365
      required:
        - anchor
        - days
    VectorStoreFileBatchObject:
      type: object
      title: Vector store file batch
      description: A batch of files attached to a vector store.
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `vector_store.file_batch`.
          type: string
          enum:
            - vector_store.files_batch
          x-stainless-const: true
        created_at:
          description: >-
            The Unix timestamp (in seconds) for when the vector store files
            batch was created.
          type: integer
        vector_store_id:
          description: >-
            The ID of the [vector
            store](/docs/api-reference/vector-stores/object) that the
            [File](/docs/api-reference/files) is attached to.
          type: string
        status:
          description: >-
            The status of the vector store files batch, which can be either
            `in_progress`, `completed`, `cancelled` or `failed`.
          type: string
          enum:
            - in_progress
            - completed
            - cancelled
            - failed
        file_counts:
          type: object
          properties:
            in_progress:
              description: The number of files that are currently being processed.
              type: integer
            completed:
              description: The number of files that have been processed.
              type: integer
            failed:
              description: The number of files that have failed to process.
              type: integer
            cancelled:
              description: The number of files that where cancelled.
              type: integer
            total:
              description: The total number of files.
              type: integer
          required:
            - in_progress
            - completed
            - cancelled
            - failed
            - total
      required:
        - id
        - object
        - created_at
        - vector_store_id
        - status
        - file_counts
      x-oaiMeta:
        name: The vector store files batch object
        beta: true
        example: |
          {
            "id": "vsfb_123",
            "object": "vector_store.files_batch",
            "created_at": 1698107661,
            "vector_store_id": "vs_abc123",
            "status": "completed",
            "file_counts": {
              "in_progress": 0,
              "completed": 100,
              "failed": 0,
              "cancelled": 0,
              "total": 100
            }
          }
    VectorStoreFileObject:
      type: object
      title: Vector store files
      description: A list of files attached to a vector store.
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `vector_store.file`.
          type: string
          enum:
            - vector_store.file
          x-stainless-const: true
        usage_bytes:
          description: >-
            The total vector store usage in bytes. Note that this may be
            different from the original file size.
          type: integer
        created_at:
          description: >-
            The Unix timestamp (in seconds) for when the vector store file was
            created.
          type: integer
        vector_store_id:
          description: >-
            The ID of the [vector
            store](/docs/api-reference/vector-stores/object) that the
            [File](/docs/api-reference/files) is attached to.
          type: string
        status:
          description: >-
            The status of the vector store file, which can be either
            `in_progress`, `completed`, `cancelled`, or `failed`. The status
            `completed` indicates that the vector store file is ready for use.
          type: string
          enum:
            - in_progress
            - completed
            - cancelled
            - failed
        last_error:
          type: object
          description: >-
            The last error associated with this vector store file. Will be
            `null` if there are no errors.
          nullable: true
          properties:
            code:
              type: string
              description: One of `server_error` or `rate_limit_exceeded`.
              enum:
                - server_error
                - unsupported_file
                - invalid_file
            message:
              type: string
              description: A human-readable description of the error.
          required:
            - code
            - message
        chunking_strategy:
          type: object
          description: The strategy used to chunk the file.
          oneOf:
            - $ref: "#/components/schemas/StaticChunkingStrategyResponseParam"
            - $ref: "#/components/schemas/OtherChunkingStrategyResponseParam"
          x-oaiExpandable: true
      required:
        - id
        - object
        - usage_bytes
        - created_at
        - vector_store_id
        - status
        - last_error
      x-oaiMeta:
        name: The vector store file object
        beta: true
        example: |
          {
            "id": "file-abc123",
            "object": "vector_store.file",
            "usage_bytes": 1234,
            "created_at": 1698107661,
            "vector_store_id": "vs_abc123",
            "status": "completed",
            "last_error": null,
            "chunking_strategy": {
              "type": "static",
              "static": {
                "max_chunk_size_tokens": 800,
                "chunk_overlap_tokens": 400
              }
            }
          }
    VectorStoreObject:
      type: object
      title: Vector store
      description: >-
        A vector store is a collection of processed files can be used by the
        `file_search` tool.
      properties:
        id:
          description: The identifier, which can be referenced in API endpoints.
          type: string
        object:
          description: The object type, which is always `vector_store`.
          type: string
          enum:
            - vector_store
          x-stainless-const: true
        created_at:
          description: >-
            The Unix timestamp (in seconds) for when the vector store was
            created.
          type: integer
        name:
          description: The name of the vector store.
          type: string
        usage_bytes:
          description: The total number of bytes used by the files in the vector store.
          type: integer
        file_counts:
          type: object
          properties:
            in_progress:
              description: The number of files that are currently being processed.
              type: integer
            completed:
              description: The number of files that have been successfully processed.
              type: integer
            failed:
              description: The number of files that have failed to process.
              type: integer
            cancelled:
              description: The number of files that were cancelled.
              type: integer
            total:
              description: The total number of files.
              type: integer
          required:
            - in_progress
            - completed
            - failed
            - cancelled
            - total
        status:
          description: >-
            The status of the vector store, which can be either `expired`,
            `in_progress`, or `completed`. A status of `completed` indicates
            that the vector store is ready for use.
          type: string
          enum:
            - expired
            - in_progress
            - completed
        expires_after:
          $ref: "#/components/schemas/VectorStoreExpirationAfter"
        expires_at:
          description: >-
            The Unix timestamp (in seconds) for when the vector store will
            expire.
          type: integer
          nullable: true
        last_active_at:
          description: >-
            The Unix timestamp (in seconds) for when the vector store was last
            active.
          type: integer
          nullable: true
        metadata:
          $ref: "#/components/schemas/Metadata"
      required:
        - id
        - object
        - usage_bytes
        - created_at
        - status
        - last_active_at
        - name
        - file_counts
        - metadata
      x-oaiMeta:
        name: The vector store object
        beta: true
        example: |
          {
            "id": "vs_123",
            "object": "vector_store",
            "created_at": 1698107661,
            "usage_bytes": 123456,
            "last_active_at": 1698107661,
            "name": "my_vector_store",
            "status": "completed",
            "file_counts": {
              "in_progress": 0,
              "completed": 100,
              "cancelled": 0,
              "failed": 0,
              "total": 100
            },
            "metadata": {},
            "last_used_at": 1698107661
          }
  securitySchemes:
    ApiKeyAuth:
      type: http
      scheme: bearer
security:
  - ApiKeyAuth: []
x-oaiMeta:
  navigationGroups:
    - id: endpoints
      title: Endpoints
    - id: assistants
      title: Assistants
      beta: true
    - id: administration
      title: Administration
    - id: realtime
      title: Realtime
      beta: true
    - id: legacy
      title: Legacy
  groups:
    - id: audio
      title: Audio
      description: |
        Learn how to turn audio into text or text into audio.

        Related guide: [Speech to text](/docs/guides/speech-to-text)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createSpeech
          path: createSpeech
        - type: endpoint
          key: createTranscription
          path: createTranscription
        - type: endpoint
          key: createTranslation
          path: createTranslation
        - type: object
          key: CreateTranscriptionResponseJson
          path: json-object
        - type: object
          key: CreateTranscriptionResponseVerboseJson
          path: verbose-json-object
    - id: chat
      title: Chat
      description: >
        Given a list of messages comprising a conversation, the model will
        return a response.

        Related guide: [Chat Completions](/docs/guides/text-generation)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createChatCompletion
          path: create
        - type: object
          key: CreateChatCompletionResponse
          path: object
        - type: object
          key: CreateChatCompletionStreamResponse
          path: streaming
    - id: embeddings
      title: Embeddings
      description: >
        Get a vector representation of a given input that can be easily consumed
        by machine learning models and algorithms.

        Related guide: [Embeddings](/docs/guides/embeddings)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createEmbedding
          path: create
        - type: object
          key: Embedding
          path: object
    - id: fine-tuning
      title: Fine-tuning
      description: >
        Manage fine-tuning jobs to tailor a model to your specific training
        data.

        Related guide: [Fine-tune models](/docs/guides/fine-tuning)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createFineTuningJob
          path: create
        - type: endpoint
          key: listPaginatedFineTuningJobs
          path: list
        - type: endpoint
          key: listFineTuningEvents
          path: list-events
        - type: endpoint
          key: listFineTuningJobCheckpoints
          path: list-checkpoints
        - type: endpoint
          key: retrieveFineTuningJob
          path: retrieve
        - type: endpoint
          key: cancelFineTuningJob
          path: cancel
        - type: object
          key: FineTuneChatRequestInput
          path: chat-input
        - type: object
          key: FineTunePreferenceRequestInput
          path: preference-input
        - type: object
          key: FineTuneCompletionRequestInput
          path: completions-input
        - type: object
          key: FineTuningJob
          path: object
        - type: object
          key: FineTuningJobEvent
          path: event-object
        - type: object
          key: FineTuningJobCheckpoint
          path: checkpoint-object
    - id: batch
      title: Batch
      description: >
        Create large batches of API requests for asynchronous processing. The
        Batch API returns completions within 24 hours for a 50% discount.

        Related guide: [Batch](/docs/guides/batch)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createBatch
          path: create
        - type: endpoint
          key: retrieveBatch
          path: retrieve
        - type: endpoint
          key: cancelBatch
          path: cancel
        - type: endpoint
          key: listBatches
          path: list
        - type: object
          key: Batch
          path: object
        - type: object
          key: BatchRequestInput
          path: request-input
        - type: object
          key: BatchRequestOutput
          path: request-output
    - id: files
      title: Files
      description: >
        Files are used to upload documents that can be used with features like
        [Assistants](/docs/api-reference/assistants),
        [Fine-tuning](/docs/api-reference/fine-tuning), and [Batch
        API](/docs/guides/batch).
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createFile
          path: create
        - type: endpoint
          key: listFiles
          path: list
        - type: endpoint
          key: retrieveFile
          path: retrieve
        - type: endpoint
          key: deleteFile
          path: delete
        - type: endpoint
          key: downloadFile
          path: retrieve-contents
        - type: object
          key: OpenAIFile
          path: object
    - id: uploads
      title: Uploads
      description: |
        Allows you to upload large files in multiple parts.
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createUpload
          path: create
        - type: endpoint
          key: addUploadPart
          path: add-part
        - type: endpoint
          key: completeUpload
          path: complete
        - type: endpoint
          key: cancelUpload
          path: cancel
        - type: object
          key: Upload
          path: object
        - type: object
          key: UploadPart
          path: part-object
    - id: images
      title: Images
      description: >
        Given a prompt and/or an input image, the model will generate a new
        image.

        Related guide: [Image generation](/docs/guides/images)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createImage
          path: create
        - type: endpoint
          key: createImageEdit
          path: createEdit
        - type: endpoint
          key: createImageVariation
          path: createVariation
        - type: object
          key: Image
          path: object
    - id: models
      title: Models
      description: >
        List and describe the various models available in the API. You can refer
        to the [Models](/docs/models) documentation to understand what models
        are available and the differences between them.
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: listModels
          path: list
        - type: endpoint
          key: retrieveModel
          path: retrieve
        - type: endpoint
          key: deleteModel
          path: delete
        - type: object
          key: Model
          path: object
    - id: moderations
      title: Moderations
      description: >
        Given text and/or image inputs, classifies if those inputs are
        potentially harmful across several categories.

        Related guide: [Moderations](/docs/guides/moderation)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createModeration
          path: create
        - type: object
          key: CreateModerationResponse
          path: object
    - id: assistants
      title: Assistants
      beta: true
      description: |
        Build assistants that can call models and use tools to perform tasks.

        [Get started with the Assistants API](/docs/assistants)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createAssistant
          path: createAssistant
        - type: endpoint
          key: listAssistants
          path: listAssistants
        - type: endpoint
          key: getAssistant
          path: getAssistant
        - type: endpoint
          key: modifyAssistant
          path: modifyAssistant
        - type: endpoint
          key: deleteAssistant
          path: deleteAssistant
        - type: object
          key: AssistantObject
          path: object
    - id: threads
      title: Threads
      beta: true
      description: |
        Create threads that assistants can interact with.

        Related guide: [Assistants](/docs/assistants/overview)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createThread
          path: createThread
        - type: endpoint
          key: getThread
          path: getThread
        - type: endpoint
          key: modifyThread
          path: modifyThread
        - type: endpoint
          key: deleteThread
          path: deleteThread
        - type: object
          key: ThreadObject
          path: object
    - id: messages
      title: Messages
      beta: true
      description: |
        Create messages within threads

        Related guide: [Assistants](/docs/assistants/overview)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createMessage
          path: createMessage
        - type: endpoint
          key: listMessages
          path: listMessages
        - type: endpoint
          key: getMessage
          path: getMessage
        - type: endpoint
          key: modifyMessage
          path: modifyMessage
        - type: endpoint
          key: deleteMessage
          path: deleteMessage
        - type: object
          key: MessageObject
          path: object
    - id: runs
      title: Runs
      beta: true
      description: |
        Represents an execution run on a thread.

        Related guide: [Assistants](/docs/assistants/overview)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createRun
          path: createRun
        - type: endpoint
          key: createThreadAndRun
          path: createThreadAndRun
        - type: endpoint
          key: listRuns
          path: listRuns
        - type: endpoint
          key: getRun
          path: getRun
        - type: endpoint
          key: modifyRun
          path: modifyRun
        - type: endpoint
          key: submitToolOuputsToRun
          path: submitToolOutputs
        - type: endpoint
          key: cancelRun
          path: cancelRun
        - type: object
          key: RunObject
          path: object
    - id: run-steps
      title: Run steps
      beta: true
      description: |
        Represents the steps (model and tool calls) taken during the run.

        Related guide: [Assistants](/docs/assistants/overview)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: listRunSteps
          path: listRunSteps
        - type: endpoint
          key: getRunStep
          path: getRunStep
        - type: object
          key: RunStepObject
          path: step-object
    - id: vector-stores
      title: Vector stores
      beta: true
      description: |
        Vector stores are used to store files for use by the `file_search` tool.

        Related guide: [File Search](/docs/assistants/tools/file-search)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createVectorStore
          path: create
        - type: endpoint
          key: listVectorStores
          path: list
        - type: endpoint
          key: getVectorStore
          path: retrieve
        - type: endpoint
          key: modifyVectorStore
          path: modify
        - type: endpoint
          key: deleteVectorStore
          path: delete
        - type: object
          key: VectorStoreObject
          path: object
    - id: vector-stores-files
      title: Vector store files
      beta: true
      description: |
        Vector store files represent files inside a vector store.

        Related guide: [File Search](/docs/assistants/tools/file-search)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createVectorStoreFile
          path: createFile
        - type: endpoint
          key: listVectorStoreFiles
          path: listFiles
        - type: endpoint
          key: getVectorStoreFile
          path: getFile
        - type: endpoint
          key: deleteVectorStoreFile
          path: deleteFile
        - type: object
          key: VectorStoreFileObject
          path: file-object
    - id: vector-stores-file-batches
      title: Vector store file batches
      beta: true
      description: >
        Vector store file batches represent operations to add multiple files to
        a vector store.

        Related guide: [File Search](/docs/assistants/tools/file-search)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createVectorStoreFileBatch
          path: createBatch
        - type: endpoint
          key: getVectorStoreFileBatch
          path: getBatch
        - type: endpoint
          key: cancelVectorStoreFileBatch
          path: cancelBatch
        - type: endpoint
          key: listFilesInVectorStoreBatch
          path: listBatchFiles
        - type: object
          key: VectorStoreFileBatchObject
          path: batch-object
    - id: assistants-streaming
      title: Streaming
      beta: true
      description: >
        Stream the result of executing a Run or resuming a Run after submitting
        tool outputs.

        You can stream events from the [Create Thread and
        Run](/docs/api-reference/runs/createThreadAndRun),

        [Create Run](/docs/api-reference/runs/createRun), and [Submit Tool
        Outputs](/docs/api-reference/runs/submitToolOutputs)

        endpoints by passing `"stream": true`. The response will be a
        [Server-Sent
        events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events)
        stream.

        Our Node and Python SDKs provide helpful utilities to make streaming
        easy. Reference the

        [Assistants API quickstart](/docs/assistants/overview) to learn more.
      navigationGroup: assistants
      sections:
        - type: object
          key: MessageDeltaObject
          path: message-delta-object
        - type: object
          key: RunStepDeltaObject
          path: run-step-delta-object
        - type: object
          key: AssistantStreamEvent
          path: events
    - id: administration
      title: Administration
      description: >
        Programmatically manage your organization.

        The Audit Logs endpoint provides a log of all actions taken in the
        organization for security and monitoring purposes.

        To access these endpoints please generate an Admin API Key through the
        [API Platform Organization overview](/organization/admin-keys). Admin
        API keys cannot be used for non-administration endpoints.

        For best practices on setting up your organization, please refer to this
        [guide](/docs/guides/production-best-practices#setting-up-your-organization)
      navigationGroup: administration
    - id: admin-api-keys
      title: Admin API Keys
      description: >
        The **Usage API** provides detailed insights into your activity across
        the OpenAI API. It also includes a separate [Costs
        endpoint](/docs/api-reference/usage/costs), which offers visibility into
        your spend, breaking down consumption by invoice line items and project
        IDs.

        While the Usage API delivers granular usage data, it may not always
        reconcile perfectly with the Costs due to minor differences in how usage
        and spend are recorded. For financial purposes, we recommend using the
        [Costs endpoint](/docs/api-reference/usage/costs) or the [Costs
        tab](/settings/organization/usage) in the Usage Dashboard, which will
        reconcile back to your billing invoice.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: admin-api-keys-list
          path: list
        - type: endpoint
          key: admin-api-keys-create
          path: create
        - type: endpoint
          key: admin-api-keys-get
          path: listget
        - type: endpoint
          key: admin-api-keys-delete
          path: delete
    - id: invite
      title: Invites
      description: Invite and manage invitations for an organization.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-invites
          path: list
        - type: endpoint
          key: inviteUser
          path: create
        - type: endpoint
          key: retrieve-invite
          path: retrieve
        - type: endpoint
          key: delete-invite
          path: delete
        - type: object
          key: Invite
          path: object
    - id: users
      title: Users
      description: |
        Manage users and their role in an organization.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-users
          path: list
        - type: endpoint
          key: modify-user
          path: modify
        - type: endpoint
          key: retrieve-user
          path: retrieve
        - type: endpoint
          key: delete-user
          path: delete
        - type: object
          key: User
          path: object
    - id: projects
      title: Projects
      description: >
        Manage the projects within an orgnanization includes creation, updating,
        and archiving or projects.

        The Default project cannot be archived.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-projects
          path: list
        - type: endpoint
          key: create-project
          path: create
        - type: endpoint
          key: retrieve-project
          path: retrieve
        - type: endpoint
          key: modify-project
          path: modify
        - type: endpoint
          key: archive-project
          path: archive
        - type: object
          key: Project
          path: object
    - id: project-users
      title: Project users
      description: >
        Manage users within a project, including adding, updating roles, and
        removing users.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-users
          path: list
        - type: endpoint
          key: create-project-user
          path: creeate
        - type: endpoint
          key: retrieve-project-user
          path: retrieve
        - type: endpoint
          key: modify-project-user
          path: modify
        - type: endpoint
          key: delete-project-user
          path: delete
        - type: object
          key: ProjectUser
          path: object
    - id: project-service-accounts
      title: Project service accounts
      description: >
        Manage service accounts within a project. A service account is a bot
        user that is not associated with a user.

        If a user leaves an organization, their keys and membership in projects
        will no longer work. Service accounts

        do not have this limitation. However, service accounts can also be
        deleted from a project.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-service-accounts
          path: list
        - type: endpoint
          key: create-project-service-account
          path: create
        - type: endpoint
          key: retrieve-project-service-account
          path: retrieve
        - type: endpoint
          key: delete-project-service-account
          path: delete
        - type: object
          key: ProjectServiceAccount
          path: object
    - id: project-api-keys
      title: Project API keys
      description: >
        Manage API keys for a given project. Supports listing and deleting keys
        for users.

        This API does not allow issuing keys for users, as users need to
        authorize themselves to generate keys.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-api-keys
          path: list
        - type: endpoint
          key: retrieve-project-api-key
          path: retrieve
        - type: endpoint
          key: delete-project-api-key
          path: delete
        - type: object
          key: ProjectApiKey
          path: object
    - id: project-rate-limits
      title: Project rate limits
      description: >
        Manage rate limits per model for projects. Rate limits may be configured
        to be equal to or lower than the organization's rate limits.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-rate-limits
          path: list
        - type: endpoint
          key: update-project-rate-limits
          path: update
        - type: object
          key: ProjectRateLimit
          path: object
    - id: audit-logs
      title: Audit logs
      description: >
        Logs of user actions and configuration changes within this organization.

        To log events, you must activate logging in the [Organization
        Settings](/settings/organization/general).

        Once activated, for security reasons, logging cannot be deactivated.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-audit-logs
          path: list
        - type: object
          key: AuditLog
          path: object
    - id: usage
      title: Usage
      description: >
        The **Usage API** provides detailed insights into your activity across
        the OpenAI API. It also includes a separate [Costs
        endpoint](/docs/api-reference/usage/costs), which offers visibility into
        your spend, breaking down consumption by invoice line items and project
        IDs.


        While the Usage API delivers granular usage data, it may not always
        reconcile perfectly with the Costs due to minor differences in how usage
        and spend are recorded. For financial purposes, we recommend using the
        [Costs endpoint](/docs/api-reference/usage/costs) or the [Costs
        tab](/settings/organization/usage) in the Usage Dashboard, which will
        reconcile back to your billing invoice.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: usage-completions
          path: completions
        - type: object
          key: UsageCompletionsResult
          path: completions_object
        - type: endpoint
          key: usage-embeddings
          path: embeddings
        - type: object
          key: UsageEmbeddingsResult
          path: embeddings_object
        - type: endpoint
          key: usage-moderations
          path: moderations
        - type: object
          key: UsageModerationsResult
          path: moderations_object
        - type: endpoint
          key: usage-images
          path: images
        - type: object
          key: UsageImagesResult
          path: images_object
        - type: endpoint
          key: usage-audio-speeches
          path: audio_speeches
        - type: object
          key: UsageAudioSpeechesResult
          path: audio_speeches_object
        - type: endpoint
          key: usage-audio-transcriptions
          path: audio_transcriptions
        - type: object
          key: UsageAudioTranscriptionsResult
          path: audio_transcriptions_object
        - type: endpoint
          key: usage-vector-stores
          path: vector_stores
        - type: object
          key: UsageVectorStoresResult
          path: vector_stores_object
        - type: endpoint
          key: usage-code-interpreter-sessions
          path: code_interpreter_sessions
        - type: object
          key: UsageCodeInterpreterSessionsResult
          path: code_interpreter_sessions_object
        - type: endpoint
          key: usage-costs
          path: costs
        - type: object
          key: CostsResult
          path: costs_object
    - id: realtime
      title: Realtime
      beta: true
      description: |
        Communicate with a GPT-4o class model in real time using WebRTC or 
        WebSockets. Supports text and audio inputs and ouputs, along with audio
        transcriptions.
        [Learn more about the Realtime API](/docs/guides/realtime).
      navigationGroup: realtime
    - id: realtime-sessions
      title: Session tokens
      description: >
        REST API endpoint to generate ephemeral session tokens for use in
        client-side

        applications.
      navigationGroup: realtime
      sections:
        - type: endpoint
          key: create-realtime-session
          path: create
        - type: object
          key: RealtimeSessionCreateResponse
          path: session_object
    - id: realtime-client-events
      title: Client events
      description: >
        These are events that the OpenAI Realtime WebSocket server will accept
        from the client.
      navigationGroup: realtime
      sections:
        - type: object
          key: RealtimeClientEventSessionUpdate
          path: <auto>
        - type: object
          key: RealtimeClientEventInputAudioBufferAppend
          path: <auto>
        - type: object
          key: RealtimeClientEventInputAudioBufferCommit
          path: <auto>
        - type: object
          key: RealtimeClientEventInputAudioBufferClear
          path: <auto>
        - type: object
          key: RealtimeClientEventConversationItemCreate
          path: <auto>
        - type: object
          key: RealtimeClientEventConversationItemTruncate
          path: <auto>
        - type: object
          key: RealtimeClientEventConversationItemDelete
          path: <auto>
        - type: object
          key: RealtimeClientEventResponseCreate
          path: <auto>
        - type: object
          key: RealtimeClientEventResponseCancel
          path: <auto>
    - id: realtime-server-events
      title: Server events
      description: >
        These are events emitted from the OpenAI Realtime WebSocket server to
        the client.
      navigationGroup: realtime
      sections:
        - type: object
          key: RealtimeServerEventError
          path: <auto>
        - type: object
          key: RealtimeServerEventSessionCreated
          path: <auto>
        - type: object
          key: RealtimeServerEventSessionUpdated
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationCreated
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemCreated
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemInputAudioTranscriptionCompleted
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemInputAudioTranscriptionFailed
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemTruncated
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemDeleted
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferCommitted
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferCleared
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferSpeechStarted
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferSpeechStopped
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseCreated
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseOutputItemAdded
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseOutputItemDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseContentPartAdded
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseContentPartDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseTextDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseTextDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseAudioTranscriptDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseAudioTranscriptDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseAudioDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseAudioDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseFunctionCallArgumentsDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseFunctionCallArgumentsDone
          path: <auto>
        - type: object
          key: RealtimeServerEventRateLimitsUpdated
          path: <auto>
    - id: completions
      title: Completions
      legacy: true
      navigationGroup: legacy
      description: >
        Given a prompt, the model will return one or more predicted completions
        along with the probabilities of alternative tokens at each position.
        Most developer should use our [Chat Completions
        API](/docs/guides/text-generation#text-generation-models) to leverage
        our best and newest models.
      sections:
        - type: endpoint
          key: createCompletion
          path: create
        - type: object
          key: CreateCompletionResponse
          path: object

================
File: Sources/GeneratedOpenAIClient/package-lock.json.license
================
This source file is part of the Stanford Biodesign Digital Health ENGAGE-HF open-source project

SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Sources/GeneratedOpenAIClient/package.json
================
{
  "name": "openapi-preprocessor",
  "version": "1.0.0",
  "description": "A script to preprocess and clean the OpenAPI specification file for SpeziLLM usage.",
  "main": "preprocess-openapi-spec.js",
  "scripts": {
    "preprocess": "node preprocess-openapi-spec.js"
  },
  "dependencies": {
    "js-yaml": "^4.1.0"
  },
  "config": {
    "openapiFile": "openapi.yaml"
  },
  "author": "Philipp Zagar",
  "license": "MIT"
}

================
File: Sources/GeneratedOpenAIClient/package.json.license
================
This source file is part of the Stanford Biodesign Digital Health ENGAGE-HF open-source project

SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Sources/GeneratedOpenAIClient/preprocess-openapi-spec.js
================
let openapiDoc = yaml.load(fs.readFileSync(openapiPath, "utf8"));
function removeDeprecated(obj) {
  if (Array.isArray(obj)) {
    return obj.map(removeDeprecated).filter(Boolean);
    for (const [key, value] of Object.entries(obj)) {
      const cleanedValue = removeDeprecated(value);
    return Object.keys(newObj).length ? newObj : undefined; // Remove empty objects
function removeSpecificOneOf(obj) {
    return obj.map(removeSpecificOneOf).filter(Boolean);
    Object.keys(obj).forEach((key) => {
      obj[key] = removeSpecificOneOf(obj[key]);
    if ("oneOf" in obj && Array.isArray(obj.oneOf)) {
      if (JSON.stringify(obj.oneOf) === JSON.stringify(requiredBlocks)) {
function fixRequiredSessions(obj) {
  if (Array.isArray(requiredFields)) {
    obj.components.schemas.UsageCodeInterpreterSessionsResult.required = requiredFields.map((field) =>
function fixRequiredStatus(obj) {
  if (schema?.required && Array.isArray(schema.required)) {
      schema.required = schema.required.filter((field) => field !== "status");
function removeInvalidRefs(obj) {
    return obj.map(removeInvalidRefs).filter(Boolean);
    if (obj.$ref && removedSchemas.has(obj.$ref)) return undefined; // Remove invalid references
      const cleanedValue = removeInvalidRefs(value);
if (openapiDoc.paths) openapiDoc.paths = removeDeprecated(openapiDoc.paths);
const removedSchemas = new Set();
  for (const [key, value] of Object.entries(openapiDoc.components.schemas)) {
      removedSchemas.add(`#/components/schemas/${key}`);
      openapiDoc.components[section] = removeDeprecated(openapiDoc.components[section]);
openapiDoc = removeSpecificOneOf(openapiDoc);
openapiDoc = removeInvalidRefs(openapiDoc);
openapiDoc = fixRequiredSessions(openapiDoc);
openapiDoc = fixRequiredStatus(openapiDoc);
const cleanedYaml = yaml.dump(openapiDoc, {
fs.writeFileSync(openapiPath, finalOutput, "utf8");
console.log("✅ Successfully preprocessed OpenAPI spec and prepended Spezi copyright header.");

================
File: Sources/GeneratedOpenAIClient/README.md
================
<!--
                  
This source file is part of the Stanford Spezi open source project

SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT
             
-->

# Generated OpenAI client

This SPM target uses the [`swift-openapi-generator`](https://github.com/apple/swift-openapi-generator) to generate Swift client code from the [OpenAI OpenAPI specification](https://github.com/openai/openai-openapi) and provides the generated code to other SpeziLLM targets. The generator's configuration is defined in `openapi-generator-config.yaml`, while the specification is located in `openapi.yaml`.

### Why Preprocessing is Needed

The OpenAI OpenAPI specification contains the following issues that require preprocessing before code generation:

- **Incorrect `required` Property**: A non-existent property is incorrectly marked as `required` (see [`openai-openapi#421`](https://github.com/openai/openai-openapi/issues/421)).
- **Unsupported `oneOf` Syntax**: The `swift-openapi-generator` does not fully support `oneOf` with `required` properties (see [`swift-openapi-generator#739`](https://github.com/apple/swift-openapi-generator/issues/739)).
- **Deprecation Warnings**: `deprecated` markings in the OpenAPI spec trigger warnings in the generated Swift code (see [`swift-openapi-generator#106`](https://github.com/apple/swift-openapi-generator/issues/106) and [`swift-openapi-generator#715`](https://github.com/apple/swift-openapi-generator/issues/715)).

Without preprocessing, these issues result in unnecessary warnings during the Swift code generation and in the resulting Swift client code.

### Running the Preprocessing Script

After updating the used [OpenAI OpenAPI specification](https://github.com/openai/openai-openapi) in SpeziLLM, run the preprocessing script to prepare the spec for use in SpeziLLM.

#### Prerequisits:

Make sure Node.js and npm are installed on your system, either via `brew install node` on macOS or `sudo apt install nodejs npm` on Ubuntu / Linux.
Also, ensure that the OpenAI OpenAPI spec is located in the `openapi.yaml` file.

#### **Steps:**
1. Navigate to the generated client directory:

```sh
cd Sources/GeneratedOpenAIClient
```


2. Install dependencies (if not already installed):

```sh
npm install
```


3. Run the preprocessing script:

```sh
npm run preprocess
```


After running the script, the specification will be correctly formatted (including the Spezi copyright header) and ready for Swift client code generation.

================
File: Sources/SpeziLLM/Helpers/BundleDescription+Bundle.swift
================
    public static func atURL(from bundle: Bundle) -> LocalizedStringResource.BundleDescription {

================
File: Sources/SpeziLLM/Helpers/LLMContext+Append.swift
================
    public mutating func append(assistantOutput output: String, complete: Bool = false, overwrite: Bool = false) {
    public mutating func append(userInput input: String, id: UUID = .init(), date: Date = .now) {
    public mutating func append(systemMessage systemPrompt: String, insertAtStart: Bool = true) {
    public mutating func append(forFunction functionName: String, withID functionID: String, response functionResponse: String) {
    public mutating func append(functionCalls: [LLMContextEntity.ToolCall]) {
    public mutating func completeAssistantStreaming() {

================
File: Sources/SpeziLLM/Helpers/LLMContext+Chat.swift
================
    @MainActor public var chat: Chat {
    static let assistantToolCall = ChatEntity.HiddenMessageType(name: "assistantToolCall")
    static let system = ChatEntity.HiddenMessageType(name: "system")
    static let function = ChatEntity.HiddenMessageType(name: "function")

================
File: Sources/SpeziLLM/Helpers/LLMContext+Init.swift
================
    public init(systemMessages: [String]) {
    public mutating func reset() {

================
File: Sources/SpeziLLM/Mock/LLMMockPlatform.swift
================
public actor LLMMockPlatform: LLMPlatform {
    @MainActor public let state: LLMPlatformState = .idle
    public init() {}
    nonisolated public func callAsFunction(with: LLMMockSchema) -> LLMMockSession {

================
File: Sources/SpeziLLM/Mock/LLMMockSchema.swift
================
public struct LLMMockSchema: LLMSchema {
    public let injectIntoContext = false
    public init() {}

================
File: Sources/SpeziLLM/Mock/LLMMockSession.swift
================
public final class LLMMockSession: LLMSession, @unchecked Sendable {
    let platform: LLMMockPlatform
    let schema: LLMMockSchema
    @ObservationIgnored private var task: Task<(), Never>?
    @MainActor public var state: LLMState = .uninitialized
    @MainActor public var context: LLMContext = []
    init(_ platform: LLMMockPlatform, schema: LLMMockSchema) {
    public func generate() async throws -> AsyncThrowingStream<String, any Error> {
            let tokens = ["Mock ", "Message ", "from ", "SpeziLLM!"]
    public func cancel() {
    private func injectAndYield(_ piece: String, on continuation: AsyncThrowingStream<String, any Error>.Continuation) async {
    deinit {

================
File: Sources/SpeziLLM/Models/LLMContext.swift
================


================
File: Sources/SpeziLLM/Models/LLMContextEntity.swift
================
public struct LLMContextEntity: Codable, Equatable, Hashable, Identifiable, Sendable {
    public struct ToolCall: Codable, Equatable, Hashable, Sendable {
        public let id: String
        public let name: String
        public let arguments: String
        public init(id: String, name: String, arguments: String) {
    public enum Role: Codable, Equatable, Hashable, Sendable {
        package var rawValue: String {
    public let role: Role
    public let content: String
    public let complete: Bool
    public let id: UUID
    public let date: Date
    public init<Content: StringProtocol>(

================
File: Sources/SpeziLLM/Models/LLMError.swift
================
public enum LLMDefaultError: LLMError {
    public var errorDescription: String? {
    public var recoverySuggestion: String? {
    public var failureReason: String? {
public protocol LLMError: LocalizedError, Equatable {}

================
File: Sources/SpeziLLM/Models/LLMState.swift
================
public enum LLMState: CustomStringConvertible, Equatable, Sendable {
    public var description: String {

================
File: Sources/SpeziLLM/Models/LLMState+OperationState.swift
================
    public var representation: ViewState {

================
File: Sources/SpeziLLM/Resources/Localizable.xcstrings
================
{
  "sourceLanguage" : "en",
  "strings" : {
    "LLM_STATE_ERROR" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM state error."
          }
        }
      }
    },
    "LLM_STATE_GENERATING" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM state generating."
          }
        }
      }
    },
    "LLM_STATE_LOADING" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM state loading."
          }
        }
      }
    },
    "LLM_STATE_READY" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM state ready."
          }
        }
      }
    },
    "LLM_STATE_UNINITIALIZED" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM state uninitialized."
          }
        }
      }
    },
    "LLM_UNKNOWN_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Unknown LLM generation error."
          }
        }
      }
    },
    "LLM_UNKNOWN_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "An unknown error has occurred during LLM inference."
          }
        }
      }
    },
    "LLM_UNKNOWN_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please retry the inference and restart the application."
          }
        }
      }
    }
  },
  "version" : "1.0"
}

================
File: Sources/SpeziLLM/Resources/Localizable.xcstrings.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Sources/SpeziLLM/SpeziLLM.docc/SpeziLLM.md
================
# ``SpeziLLM``

<!--
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#       
-->

Provides base Large Language Model (LLM) execution infrastructure within the Spezi ecosystem.

## Overview

The ``SpeziLLM`` target provides the base infrastructure for Large Language Model (LLM) execution within the Swift-based Spezi ecosystem. It contains necessary abstractions of LLMs that can be reused in an arbitrary context as well as a runner component handling the actual inference of the Language Model.

## Setup

### Add Spezi LLM as a Dependency

You need to add the SpeziLLM Swift package to
[your app in Xcode](https://developer.apple.com/documentation/xcode/adding-package-dependencies-to-your-app#) or
[Swift package](https://developer.apple.com/documentation/xcode/creating-a-standalone-swift-package-with-xcode#Add-a-dependency-on-another-Swift-package).

> Important: If your application is not yet configured to use Spezi, follow the [Spezi setup article](https://swiftpackageindex.com/stanfordspezi/spezi/documentation/spezi/initial-setup) to set up the core Spezi infrastructure.

## Spezi LLM Components

The two main components of ``SpeziLLM`` are the LLM abstractions which are composed of the ``LLMSchema``, ``LLMSession``, and ``LLMPlatform`` as well as the ``LLMRunner`` execution capability. The following section highlights the usage of these parts.

### LLM abstractions

``SpeziLLM`` provides three main parts abstracting LLMs:
- ``LLMSchema``: Configuration of the to-be-used LLM, containing all information necessary for the creation of an executable ``LLMSession``.
- ``LLMSession``: Executable version of the LLM containing context and state as defined by the ``LLMSchema``.
- ``LLMPlatform``: Responsible for turning the received ``LLMSchema`` to an executable ``LLMSession``.

These protocols provides an abstraction layer for the usage of Large Language Models within the Spezi ecosystem,
regardless of the execution locality (local or remote) or the specific model type.
Developers can use these protocols to conform their LLM interface implementations to a standard which is consistent throughout the Spezi ecosystem.

The actual inference logic as well as state is held within the ``LLMSession``. It requires implementation of the ``LLMSession/generate()`` as well as ``LLMSession/cancel()`` functions, starting and cancelling the inference by the LLM respectively.
In addition, it contains the ``LLMSession/context`` in which the entire conversational history with the LLM is held as well as the ``LLMSession/state`` describing the current execution state of the session. 

> Important: Any of the three aforementioned LLM abstractions shouldn't be used on it's own but always together with the ``LLMRunner``.
    Please refer to the ``LLMRunner`` documentation for a complete code example.

### LLM runner

The ``LLMRunner`` is a Spezi `Module` accessible via the SwiftUI `Environment` that handles the execution of Language Models in the Spezi ecosystem, regardless of their execution locality (represented by the ``LLMPlatform``) or the specific model type. 
A ``LLMRunner`` is responsible for turning a ``LLMSchema`` towards an executable and stateful ``LLMSession`` by using the underlying ``LLMPlatform``.

The ``LLMRunner`` is configured with the supported ``LLMPlatform``s, enabling the runner to delegate the LLM execution to the correct ``LLMPlatform``.

#### Setup

Before usage, the ``LLMRunner`` needs to be initialized in the Spezi `Configuration` with the supported ``LLMPlatform``s.

```swift
class LocalLLMAppDelegate: SpeziAppDelegate {
    override var configuration: Configuration {
        Configuration {
            // Configure the runner responsible for executing LLMs.
            LLMRunner {
                // State the `LLMPlatform`s supported by the `LLMRunner`.
                LLMMockPlatform()
            }
        }
    }
}
```

#### Usage

The code section below showcases a complete, bare-bone code example on how to use the ``LLMRunner`` with the ``LLMSchema``.
The example is structured as a SwiftUI `View` with a `Button` to trigger LLM inference via the ``LLMMockSchema``. The generated output stream is displayed in a `Text` field.

- Tip: SpeziLLM provides the `@LLMSessionProvider` property wrapper (`View/LLMSessionProvider`) that drastically simplifies the state management of using the ``LLMSchema`` with the ``LLMRunner``. Refer to the docs below for more information.

```swift
struct LLMDemoView: View {
    // The runner responsible for executing the LLM.
    @Environment(LLMRunner.self) var runner

    // The LLM in execution, as defined by the ``LLMSchema``.
    @State var llmSession: LLMMockSession?
    @State var responseText = ""

    var body: some View {
        VStack {
            Button {
                Task {
                    try await executePrompt(prompt: "Hello LLM!")
                }
            } label: {
                Text("Start LLM inference")
            }
                .disabled(if: llmSession)

            Text(responseText)
        }
            .task {
                // Instantiate the `LLMSchema` to an `LLMSession` via the `LLMRunner`.
                self.llmSession = runner(with: LLMMockSchema())
            }
    }

    func executePrompt(prompt: String) async throws {
        await MainActor.run {
            llmSession?.context.append(userInput: prompt)
        }

        // Performing the LLM inference, returning a stream of outputs.
        guard let stream = try await llmSession?.generate() else {
            return
        }

        for try await token in stream {
            responseText.append(token)
        }
   }
}
```

As show in the example above, a simple LLM inference task with the ``LLMSession`` quickly becomes complex.
That's why SpeziLLM provides the `@LLMSessionProvider` property wrapper (`View/LLMSessionProvider`) that enables the convenient instantiation of the passed ``LLMSchema`` (defining the LLM) to a to-be-used ``LLMSession`` (LLM in execution).
The instantiation is done by the ``LLMRunner`` which determines the correct ``LLMPlatform`` for the ``LLMSchema`` to run on.
An example of using the `@LLMSessionProvider` property wrapper can be found below:

```swift
struct LLMDemoView: View {
    // Use the convenience property wrapper to instantiate the `LLMMockSession`
    @LLMSessionProvider(schema: LLMMockSchema()) var llm: LLMMockSession
    @State var responseText = ""

    var body: some View {
        VStack {
            Button {
                Task { @MainActor in
                    llm.context.append(userInput: "Hello!")

                    for try await token in try await llm.generate() {
                        responseText.append(token)
                    }
                }
            } label: {
                Text("Start LLM inference")
            }
                .disabled(llm.state.representation == .processing)

            Text(responseText)
        }
    }
}
```

### LLM Chat View

The ``LLMChatView`` and ``LLMChatViewSchema`` present basic chat views that enable users to chat with a Spezi LLM in a typical chat-like fashion. The input can be either typed out via the iOS keyboard or provided as voice input and transcribed into written text.
The ``LLMChatViewSchema`` takes an ``LLMSchema`` instance to define which LLM in what configuration should be used for the text inference.
The ``LLMChatView`` is passed an ``LLMSession`` that represents the LLM in execution containing state and context, and an optional `ChatView/ChatExportFormat` that defines the format of the to-be-exported `SpeziChat/Chat` (can be any of `.pdf`, `.text`, `.json`).

> Tip: The ``LLMChatView`` and ``LLMChatViewSchema`` build on top of the [SpeziChat package](https://swiftpackageindex.com/stanfordspezi/spezichat/documentation).
    For more details, please refer to the DocC documentation of the [`ChatView`](https://swiftpackageindex.com/stanfordspezi/spezichat/documentation/spezichat/chatview).

> Tip: By default, the ``LLMChatView`` presents no share button in the toolbar that exports the current `SpeziChat/Chat`. To add this element or change the export functionality, pass the desired export format for the `exportFormat` parameter in ``LLMChatView/init(session:exportFormat:)``.

#### Usage

An example usage of the ``LLMChatViewSchema`` can be seen in the following example.
The example uses the ``LLMMockSchema`` as the passed ``LLMSchema`` instance in order to provide a mock output generation stream.
Keep in mind that one cannot access the underlying context or state of the ``LLMSession`` when using the ``LLMChatViewSchema``.

```swift
struct LLMDemoChatView: View {
    var body: some View {
        LLMChatViewSchema(with: LLMMockSchema())
    }
}
```

An example of using the lower-level ``LLMChatView`` can be seen in the following example.
Here, the user has full control over the ``LLMSession`` and can access the context or state of the LLM.
SpeziLLM provides the `@LLMSessionProvider` property wrapper (`View/LLMSessionProvider`) that enables the convenient instantiation of the passed ``LLMSchema`` (defining the LLM) to a to-be-used ``LLMSession`` (LLM in execution).

```swift
struct LLMDemoChatView: View {
    // Use the convenience property wrapper to instantiate the `LLMMockSession`
    @LLMSessionProvider(schema: LLMMockSchema()) var llm: LLMMockSession

    var body: some View {
        LLMChatView(session: $llm)
    }
}
```

## Topics

### LLM abstraction

- ``LLMSchema``
- ``LLMSession``
- ``LLMState``
- ``LLMError``

### LLM Execution

- ``LLMRunner``
- ``LLMPlatform``

### Views

- ``LLMChatView``
- ``LLMChatViewSchema``

### Mocks

- ``LLMMockPlatform``
- ``LLMMockSchema``
- ``LLMMockSession``

================
File: Sources/SpeziLLM/Views/LLMChatView.swift
================
public struct LLMChatView<Session: LLMSession>: View {
    @Binding private var llm: Session
    @MainActor private var inputDisabled: Bool {
    private let exportFormat: ChatView.ChatExportFormat?
    public var body: some View {
                            let stream = try await llm.generate()
    public init(
    @Previewable @State var llm = LLMMockSession(.init(), schema: .init())

================
File: Sources/SpeziLLM/Views/LLMChatViewDisabledModifier.swift
================
private struct LLMChatViewDisabledModifier<L: LLMSession>: ViewModifier {
    let llm: L?
    func body(content: Content) -> some View {
    public func disabled<L: LLMSession>(if llm: L?) -> some View {

================
File: Sources/SpeziLLM/Views/LLMChatViewSchema.swift
================
public struct LLMChatViewSchema<Schema: LLMSchema>: View {
    @LLMSessionProvider<Schema> var llm: Schema.Platform.Session
    private let exportFormat: ChatView.ChatExportFormat?
    public var body: some View {
    public init(

================
File: Sources/SpeziLLM/LLMPlatform.swift
================
public protocol LLMPlatform: Module, EnvironmentAccessible {
    var schemaId: ObjectIdentifier {

================
File: Sources/SpeziLLM/LLMPlatformBuilder.swift
================
public enum LLMPlatformBuilder: DependencyCollectionBuilder {
    public static func buildExpression<L: LLMPlatform>(_ expression: @escaping @autoclosure () -> L) -> DependencyCollection {

================
File: Sources/SpeziLLM/LLMPlatformState.swift
================
public enum LLMPlatformState: Sendable {

================
File: Sources/SpeziLLM/LLMRunner.swift
================
    public enum State {
    @Dependency private var llmPlatformModules: [any Module]
    var llmPlatforms: [ObjectIdentifier: any LLMPlatform] = [:]
    @MainActor public var state: State {
        var state: State = .idle
    public func configure() {
    public func callAsFunction<L: LLMSchema>(with llmSchema: L) -> L.Platform.Session {
    public func oneShot<L: LLMSchema>(with llmSchema: L, context: LLMContext) async throws -> AsyncThrowingStream<String, any Error> {
        let llmSession = callAsFunction(with: llmSchema)
        var output = ""

================
File: Sources/SpeziLLM/LLMSchema.swift
================
public protocol LLMSchema: Sendable {

================
File: Sources/SpeziLLM/LLMSession.swift
================
public protocol LLMSession: AnyObject, Sendable {
    public func finishGenerationWithError<E: LLMError>(_ error: E, on continuation: AsyncThrowingStream<String, any Error>.Continuation) async {
    public func checkCancellation(on continuation: AsyncThrowingStream<String, any Error>.Continuation) async -> Bool {

================
File: Sources/SpeziLLM/LLMSessionProvider.swift
================
public struct _LLMSessionProvider<Schema: LLMSchema>: DynamicProperty {     // swiftlint:disable:this type_name
    class Box<T> {
        var value: T
        init(_ value: T) {
    @Environment(LLMRunner.self) private var runner
    @State private var llmBox: Box<Schema.Platform.Session?>
    private let schema: Schema
    public var wrappedValue: Schema.Platform.Session {
    @MainActor public var projectedValue: Binding<Schema.Platform.Session> {
    public init(schema: Schema) {
    public func update() {

================
File: Sources/SpeziLLMFog/Configuration/LLMFogModelParameters.swift
================
public struct LLMFogModelParameters: Sendable {
    public enum ResponseFormat {
        var openAiRepresentation: Components.Schemas.CreateChatCompletionRequest.response_formatPayload {
    let responseFormat: Components.Schemas.CreateChatCompletionRequest.response_formatPayload?
    let temperature: Double?
    let topP: Double?
    let stopSequence: [String]
    let maxOutputLength: Int?
    let seed: Int?
    let presencePenalty: Double?
    let frequencyPenalty: Double?
    public init(

================
File: Sources/SpeziLLMFog/Configuration/LLMFogParameters.swift
================
public struct LLMFogParameters: Sendable {
    public enum FogModelType: String, Sendable {
    let modelType: String
    let systemPrompts: [String]
    let authToken: @Sendable () async -> String?
    public init(

================
File: Sources/SpeziLLMFog/Configuration/LLMFogPlatformConfiguration.swift
================
public struct LLMFogPlatformConfiguration: Sendable {
    let host: String
    let caCertificate: URL?
    let taskPriority: TaskPriority
    let concurrentStreams: Int
    let timeout: TimeInterval
    let mDnsBrowsingTimeout: Duration
    public init(

================
File: Sources/SpeziLLMFog/Connection/AuthMiddleware.swift
================
struct AuthMiddleware: ClientMiddleware {
    private let hostHeaderKey: HTTPField.Name = {
    private let authToken: @Sendable () async -> String?
    private let expectedHost: String?
    init(authToken: @Sendable @escaping () async -> String?, expectedHost: String?) {
    func intercept(
        var request = request

================
File: Sources/SpeziLLMFog/Connection/URLSession+CertVerification.swift
================
final class TransportCertificateValidationDelegate: NSObject, URLSessionDelegate {
    private let caCertificate: SecCertificate?
    private let expectedHost: String?
    private let logger: Logger
    init(caCertificate: SecCertificate?, expectedHost: String?, logger: Logger) {
    func urlSession(
        let anchorCertificates: [SecCertificate] = [caCertificate]
        let policy = SecPolicyCreateSSL(true, expectedHost as CFString)
        var error: CFError?

================
File: Sources/SpeziLLMFog/Helpers/Chat+OpenAI.swift
================
    var openAIRepresentation: Role {

================
File: Sources/SpeziLLMFog/Resources/Localizable.xcstrings
================
{
  "sourceLanguage" : "en",
  "strings" : {
    "LLM_CONNECTIVITY_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Connectivity Error with the Fog Node."
          }
        }
      }
    },
    "LLM_CONNECTIVITY_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The network connection to the Fog Node servers couldn't be established, most probably the device doesn't have a proper connection to the Fog Node."
          }
        }
      }
    },
    "LLM_CONNECTIVITY_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please ensure that the device has a stable internet connection and the Fog Node is running correctly."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Generation Error occurred during Fog LLM inference."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The Fog LLM API responded with an error during the output generation."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please retry the input query and ensure that the Fog Node is running correctly."
          }
        }
      }
    },
    "LLM_INVALID_TOKEN_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Unauthorized access to the Fog LLM resource."
          }
        }
      }
    },
    "LLM_INVALID_TOKEN_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The specified token is not valid / not authorized to access the Fog LLM resource."
          }
        }
      }
    },
    "LLM_INVALID_TOKEN_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ensure that the specified token is valid and has the potentially required claims to the LLM resource."
          }
        }
      }
    },
    "LLM_MISSING_CA_CERT_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Couldn't establish a secure connection to the fog node."
          }
        }
      }
    },
    "LLM_MISSING_CA_CERT_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The root CA certificate that signed the Fog LLM certificates is missing."
          }
        }
      }
    },
    "LLM_MISSING_CA_CERT_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ensure that SpeziLLMFog is configured with the valid root CA certificate used to sign the Fog node certificates."
          }
        }
      }
    },
    "LLM_MODEL_ACCESS_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM model type could not be accessed on fog node."
          }
        }
      }
    },
    "LLM_MODEL_ACCESS_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The specified LLM model type is not present on the LLM Fog node."
          }
        }
      }
    },
    "LLM_MODEL_ACCESS_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Download the specified LLM model on the LLM Fog node."
          }
        }
      }
    },
    "LLM_NO_MDNS_SERVICE_FOUND_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "No LLM Fog node was found in the local network."
          }
        }
      }
    },
    "LLM_NO_MDNS_SERVICE_FOUND_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Either, the LLM Fog node is not properly running or the device has network connectivity issues."
          }
        }
      }
    },
    "LLM_NO_MDNS_SERVICE_FOUND_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ensure proper connectivity within the local network and the correct running of the LLM Fog node."
          }
        }
      }
    },
    "LLM_SERIVE_DISCOVERY_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Error during fog service discovery."
          }
        }
      }
    },
    "LLM_SERIVE_DISCOVERY_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "An error occurred during browsing for fog services within the local network."
          }
        }
      }
    },
    "LLM_SERIVE_DISCOVERY_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ensure proper network connectivity so that the fog service can be discovered."
          }
        }
      }
    },
    "LLM_UNKNOWN_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "An unknown Fog LLM error has occured."
          }
        }
      }
    },
    "LLM_UNKNOWN_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "During service discovery of the LLM fog node, an unknown error occured."
          }
        }
      }
    },
    "LLM_UNKNOWN_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ensure proper network connectivity and device configuration so that the fog service can be discovered."
          }
        }
      }
    }
  },
  "version" : "1.0"
}

================
File: Sources/SpeziLLMFog/Resources/Localizable.xcstrings.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Sources/SpeziLLMFog/SpeziLLMFog.docc/SpeziLLMFog.md
================
# ``SpeziLLMFog``

<!--
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#       
-->

Discover and dispatch Large Language Models (LLMs) inference jobs to Fog node resources within the local network.

## Overview

A module that allows you to interact with Fog node-based Large Language Models (LLMs) in the local network within your Spezi application.
``SpeziLLMFog`` automatically discovers LLM computing resources within the local network, establishes a connection to these [Fog nodes](https://en.wikipedia.org/wiki/Fog_computing), and then dispatches LLM inference jobs to these nodes. The response is then streamed back to ``SpeziLLMFog`` and surfaced to the user. The fog nodes advertise their services via [mDNS](https://en.wikipedia.org/wiki/Multicast_DNS), enabling clients to discover all fog nodes serving a specific host within the local network.
``SpeziLLMFog`` provides a pure Swift-based API for interacting with the Fog LLMs, building on top of the infrastructure of the [SpeziLLM target](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm).

## Setup

### Add Spezi LLM as a Dependency

You need to add the SpeziLLM Swift package to
[your app in Xcode](https://developer.apple.com/documentation/xcode/adding-package-dependencies-to-your-app#) or
[Swift package](https://developer.apple.com/documentation/xcode/creating-a-standalone-swift-package-with-xcode#Add-a-dependency-on-another-Swift-package).

> Important: If your application is not yet configured to use Spezi, follow the [Spezi setup article](https://swiftpackageindex.com/stanfordspezi/spezi/documentation/spezi/initial-setup) to set up the core Spezi infrastructure.

## Spezi LLM Fog Components

The core components of the ``SpeziLLMFog`` target are the ``LLMFogSchema``, ``LLMFogSession`` as well as ``LLMFogPlatform``. These components enable users to automatically discover Fog LLM resources via [mDNS](https://en.wikipedia.org/wiki/Multicast_DNS) and dispatch jobs to these nodes which are then performing the LLM inference on open-source LLMs like Llama2 or Gemma.

> Important: ``SpeziLLMFog`` requires a `SpeziLLMFogNode` within the local network hosted on some computing resource that actually performs the inference requests. ``SpeziLLMFog`` provides the `SpeziLLMFogNode` Docker-based package that enables an out-of-the-box setup of these fog nodes. See the `FogNode` directory on the root level of the SPM package as well as the respective `README.md` for more details.

### LLM Fog

``LLMFogSchema`` offers a variety of configuration possibilities that are supported by the Fog LLM APIs (mirroring the OpenAI API implementation), such as the model type, the system prompt, the temperature of the model, and many more. These options can be set via the ``LLMFogSchema/init(parameters:modelParameters:injectIntoContext:)`` initializer and the ``LLMFogParameters`` and ``LLMFogModelParameters``.

This ``LLMFogSchema`` is then turned into an in-execution ``LLMFogSession`` by the `LLMRunner` via the ``LLMFogPlatform``. The ``LLMFogSession`` is the executable version of a Fog LLM containing context and state as defined by the ``LLMFogSchema``.
As the to-be-used models are running on a Fog node within the local network, the respective LLM computing resource (so the fog node) is discovered upon setup of the ``LLMFogSession``, meaning a ``LLMFogSession`` is bound to a specific fog node after initialization.

- Important: The Fog LLM abstractions shouldn't be used on it's own but always used together with the Spezi `LLMRunner`.

#### Setup

In order to use Fog LLMs within the Spezi ecosystem, the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) needs to be initialized in the Spezi `Configuration` with the `LLMFogPlatform`. Only after, the `LLMRunner` can be used for inference with Fog LLMs. See the [SpeziLLM documentation](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) for more details.
The `LLMFogPlatform` needs to be initialized with the custom root CA certificate that was used to sign the fog node web service certificate (see the `FogNode/README.md` documentation for more information). Copy the root CA certificate from the fog node as resource to the application using `SpeziLLMFog` and use it to initialize the `LLMFogPlatform` within the Spezi `Configuration`.

```swift
class LLMFogAppDelegate: SpeziAppDelegate {
    private nonisolated static var caCertificateUrl: URL {
        // Return local file URL of root CA certificate in the `.crt` format
    }

    override var configuration: Configuration {
        Configuration {
            LLMRunner {
                LLMFogPlatform(configuration: .init(caCertificate: Self.caCertificateUrl))
            }
        }
    }
}
```

- Important: For development purposes, one is able to configure the fog node in the development mode, meaning no TLS connection (resulting in no need for custom certificates). See the `FogNode/README.md` for more details regarding server-side (so fog node) instructions.
On the client-side within Spezi, one has to pass `nil` for the `caCertificate` parameter of the ``LLMFogPlatform`` as shown above. If used in development mode, no custom CA certificate is required, ensuring a smooth and straightforward development process.

#### Usage

The code example below showcases the interaction with a Fog LLM through the the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner), which is injected into the SwiftUI `Environment` via the `Configuration` shown above.

The ``LLMFogSchema`` defines the type and configurations of the to-be-executed ``LLMFogSession``. This transformation is done via the [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) that uses the ``LLMFogPlatform``. The inference via ``LLMFogSession/generate()`` returns an `AsyncThrowingStream` that yields all generated `String` pieces.
The ``LLMFogSession`` automatically discovers all available LLM fog nodes within the local network upon setup and the dispatches the LLM inference jobs to the fog computing resource, streaming back the response and surfaces it to the user.

The ``LLMFogSession`` contains the ``LLMFogSession/context`` property which holds the entire history of the model interactions. This includes the system prompt, user input, but also assistant responses.
Ensure the property always contains all necessary information, as the ``LLMFogSession/generate()`` function executes the inference based on the ``LLMFogSession/context``.

- Important: The ``LLMFogSchema`` accepts a closure that returns an authorization token that is passed with every request to the Fog node in the `Bearer` HTTP field via the ``LLMFogParameters/init(modelType:systemPrompt:authToken:)``. The token is created via the closure upon every LLM inference request, as the ``LLMFogSession`` may be long lasting and the token could therefore expire. Ensure that the closure appropriately caches the token in order to prevent unnecessary token refresh roundtrips to external systems.

```swift
struct LLMFogDemoView: View {
    @Environment(LLMRunner.self) var runner
    @State var responseText = ""

    var body: some View {
        Text(responseText)
            .task {
                // Instantiate the `LLMFogSchema` to an `LLMFogSession` via the `LLMRunner`.
                let llmSession: LLMFogSession = runner(
                    with: LLMFogSchema(
                        parameters: .init(
                            modelType: .llama7B,
                            systemPrompt: "You're a helpful assistant that answers questions from users.",
                            authToken: { 
                                // Return authorization token as `String` or `nil` if no token is required by the Fog node.
                            }
                        )
                    )
                )

                do {
                    for try await token in try await llmSession.generate() {
                        responseText.append(token)
                    }
                } catch {
                    // Handle errors here. E.g., you can use `ViewState` and `viewStateAlert` from SpeziViews.
                }
            }
    }
}
```

## Topics

### LLM Fog abstraction

- ``LLMFogSchema``
- ``LLMFogSession``

### LLM Execution

- ``LLMFogPlatform``
- ``LLMFogPlatformConfiguration``

### LLM Configuration

- ``LLMFogParameters``
- ``LLMFogModelParameters``

### Misc

- ``LLMFogError``

================
File: Sources/SpeziLLMFog/LLMFogError.swift
================
public enum LLMFogError: LLMError {
    public var errorDescription: String? {
    public var recoverySuggestion: String? {
    public var failureReason: String? {
    private static let modelNotFoundRegex = "model '([\\w:]+)' not found, try pulling it first"
    func handleErrorCode(statusCode: Int, message: String?) -> LLMFogError {

================
File: Sources/SpeziLLMFog/LLMFogPlatform.swift
================
public actor LLMFogPlatform: LLMPlatform {
    private let semaphore: AsyncSemaphore
    let configuration: LLMFogPlatformConfiguration
    @MainActor public var state: LLMPlatformState = .idle
    public init(configuration: LLMFogPlatformConfiguration) {
    public nonisolated func callAsFunction(with llmSchema: LLMFogSchema) -> LLMFogSession {
    func exclusiveAccess() async throws {
    func signal() async {
        let otherTasksWaiting = semaphore.signal()

================
File: Sources/SpeziLLMFog/LLMFogSchema.swift
================
public struct LLMFogSchema: LLMSchema, @unchecked Sendable {
    let parameters: LLMFogParameters
    let modelParameters: LLMFogModelParameters
    public let injectIntoContext: Bool
    public init(

================
File: Sources/SpeziLLMFog/LLMFogSession.swift
================
public final class LLMFogSession: LLMSession, @unchecked Sendable {
    static let logger = Logger(subsystem: "edu.stanford.spezi", category: "SpeziLLMFog")
    let platform: LLMFogPlatform
    let schema: LLMFogSchema
    @ObservationIgnored private var tasks: Set<Task<(), Never>> = []
    @ObservationIgnored private var lock = NSLock()
    @ObservationIgnored var wrappedClient: Client?
    @ObservationIgnored var discoveredServiceAddress: String?
    @MainActor public var state: LLMState = .uninitialized
    @MainActor public var context: LLMContext = []
    var fogNodeClient: Client {
    init(_ platform: LLMFogPlatform, schema: LLMFogSchema) {
    public func generate() async throws -> AsyncThrowingStream<String, any Error> {
        let task = Task(priority: platform.configuration.taskPriority) {
    public func setup(
    public func cancel() {
    deinit {

================
File: Sources/SpeziLLMFog/LLMFogSession+Configuration.swift
================
    private var openAIContext: [Components.Schemas.ChatCompletionRequestMessage] {
    var openAIChatQuery: Operations.createChatCompletion.Input {
    private func getChatMessage(_ contextEntity: LLMContextEntity) -> Components.Schemas.ChatCompletionRequestMessage? {

================
File: Sources/SpeziLLMFog/LLMFogSession+Generation.swift
================
    func _generate( // swiftlint:disable:this identifier_name function_body_length
            let response = try await fogNodeClient.createChatCompletion(openAIChatQuery)
                var errorMessage: String?
                let llmError = handleErrorCode(statusCode: statusCode, message: errorMessage)
            let chatStream = try response.ok.body.text_event_hyphen_stream

================
File: Sources/SpeziLLMFog/LLMFogSession+Setup.swift
================
    func _setup(continuation: AsyncThrowingStream<String, any Error>.Continuation) async -> Bool {
        var caCertificate: SecCertificate?
        let fogServiceAddress: String
        let host: String
        let urlString = """
                let session = URLSession(
    private func resolveFogService(secureTraffic: Bool = true) async throws -> String {
        let browser = NWBrowser(
        let connection = NWConnection(to: discoveredEndpoint, using: .tcp)
        let resolvedService = try await withCheckedThrowingContinuation { continuation in
                        let ipAddress: String? = switch host {

================
File: Sources/SpeziLLMLocal/Configuration/LLMLocalModel.swift
================
public enum LLMLocalModel {
    public var hubID: String {

================
File: Sources/SpeziLLMLocal/Configuration/LLMLocalParameters.swift
================
public struct LLMLocalParameters: Sendable {
    public enum Defaults {
        public static let defaultSystemPrompt: String = {
    let systemPrompt: String?
    let maxOutputLength: Int
    let extraEOSTokens: Set<String>
    let displayEveryNTokens: Int
    let seed: UInt64?
    let chatTemplate: String?
    public init(

================
File: Sources/SpeziLLMLocal/Configuration/LLMLocalPlatformConfiguration.swift
================
public struct LLMLocalPlatformConfiguration: Sendable {
    public struct MemoryLimit: Sendable {
        let limit: Int
        let relaxed: Bool
        public init(limit: Int, relaxed: Bool = false) {
    let cacheLimit: Int?
    let memoryLimit: MemoryLimit?
    let taskPriority: TaskPriority
    public init(

================
File: Sources/SpeziLLMLocal/Configuration/LLMLocalSamplingParameters.swift
================
public struct LLMLocalSamplingParameters: Sendable {
    let topP: Float
    let temperature: Float
    let penaltyRepeat: Float?
    let repetitionContextSize: Int
    public init(

================
File: Sources/SpeziLLMLocal/Helpers/LLMContext+FormattedChat.swift
================
    package var formattedChat: [[String: String]] {

================
File: Sources/SpeziLLMLocal/Helpers/LLMModel+numParameters.swift
================
    public func numParameters() -> Int {

================
File: Sources/SpeziLLMLocal/Resources/Localizable.xcstrings
================
{
  "sourceLanguage" : "en",
  "strings" : {
    "LLM_CONTEXT_SIZE_MISMATCH_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The configured LLM context size mismatched with the model."
          }
        }
      }
    },
    "LLM_CONTEXT_SIZE_MISMATCH_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM was trained on a context size that is smaller than the configured LLM context size."
          }
        }
      }
    },
    "LLM_CONTEXT_SIZE_MISMATCH_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please ensure that the configured LLM context size is smaller or equal to the context the model was trained upon."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Generation of LLM output failed."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "An unknown error has occurred during the generation of the output."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ensure that the device has enough free computing and memory resources. Try restarting the application."
          }
        }
      }
    },
    "LLM_ILLEGAL_CONTEXT_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM context is in an illegal state."
          }
        }
      }
    },
    "LLM_ILLEGAL_CONTEXT_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM context doesn't contain the required messages such as system prompt and a user message. These messages are required before triggering an inference."
          }
        }
      }
    },
    "LLM_ILLEGAL_CONTEXT_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please retry the query or restart the applicaiton."
          }
        }
      }
    },
    "LLM_MLX_NOT_SUPPORTED_WORKAROUND" : {
      "extractionState" : "manual",
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Here are two recommended workarounds:\n1. Add the Mac (Designed for iPad) destination to your target in Xcode.\n- SpeziLLMLocal requires MLX which requires Apple silicon, with `Mac (Designed for iPad)` you build an iPad application that will run on macOS.\n- The UI may present with differences to iOS, but this will allow you to build an iOS binary that runs with a fully featured Metal GPU.\n\n2. Make a multiplatform application that can run on macOS, iOS and iPadOS.\n- With SwiftUI it is possible to do most of your development in a macOS application and fine tune it for iOS by running it on an actual device.\n\nYou can also use the simulator for developing UI features but local LLM execution is not possible."
          }
        }
      }
    },
    "LLM_MODEL_NOT_FOUND_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM file not found."
          }
        }
      }
    },
    "LLM_MODEL_NOT_FOUND_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The specified LLM file could not be found on the device."
          }
        }
      }
    },
    "LLM_MODEL_NOT_FOUND_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ensure that the LLM file is downloaded successfully and properly stored on the device. Try restarting the application."
          }
        }
      }
    },
    "LLM_MODEL_NOT_READY_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM not ready yet."
          }
        }
      }
    },
    "LLM_MODEL_NOT_READY_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM needs to be initialized before usage."
          }
        }
      }
    },
    "LLM_MODEL_NOT_READY_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Make sure that the application initialized the LLM properly. Try restarting the application."
          }
        }
      }
    },
    "SPEZI_LLM_LOCAL_SYSTEM_PROMPT" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe and still concise. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."
          }
        }
      }
    }
  },
  "version" : "1.0"
}

================
File: Sources/SpeziLLMLocal/Resources/Localizable.xcstrings.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Sources/SpeziLLMLocal/SpeziLLMLocal.docc/SpeziLLMLocal.md
================
# ``SpeziLLMLocal``

<!--
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2024 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#       
-->

Provides local Language Model execution capabilities on-device.

## Overview


The ``SpeziLLMLocal`` target enables the usage of locally executed Language Models (LLMs) directly on-device, without the need for any kind of internet connection and no data every leaving the local device. The underlying technology used for the LLM inference is [`mlx-swift`](https://github.com/ml-explore/mlx-swift). ``SpeziLLMLocal`` provides a pure Swift-based API for interacting with the locally executed model, building on top of the infrastructure of the [SpeziLLM target](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm).

## Setup

### Add Spezi LLM as a Dependency

You need to add the SpeziLLM Swift package to
[your app in Xcode](https://developer.apple.com/documentation/xcode/adding-package-dependencies-to-your-app#) or
[Swift package](https://developer.apple.com/documentation/xcode/creating-a-standalone-swift-package-with-xcode#Add-a-dependency-on-another-Swift-package).

> Important: If your application is not yet configured to use Spezi, follow the [Spezi setup article](https://swiftpackageindex.com/stanfordspezi/spezi/documentation/spezi/initial-setup) to set up the core Spezi infrastructure.
 
> Important: Spezi LLM Local is not compatible with simulators. The underlying [`mlx-swift`](https://github.com/ml-explore/mlx-swift) requires a modern Metal MTLGPUFamily and the simulator does not provide that.

> Important: To use the LLM local target, some LLMs require adding the *Increase Memory Limit* entitlement to the project.

## Spezi LLM Local Components

The core components of the ``SpeziLLMLocal`` target are ``LLMLocalSchema``, ``LLMLocalSession`` as well as ``LLMLocalPlatform``. They heavily utilize [mlx-swift-examples](https://github.com/ml-explore/mlx-swift-examples) to perform the inference of the Language Model. ``LLMLocalSchema`` defines the type and configuration of the LLM, ``LLMLocalSession`` represents the ``LLMLocalSchema`` in execution while ``LLMLocalPlatform`` is the LLM execution platform.

> Important: To execute a LLM locally, the model file must be present on the local device.

> Tip: In order to download the model file of the Language model to the local device, SpeziLLM provides the [SpeziLLMLocalDownload](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmlocaldownload) target which provides model download and storage functionalities.

``LLMLocalSchema`` offers a variety of configuration possibilities, such as the used model file, the context window, the maximum output size or the batch size. These options can be set via the ``LLMLocalSchema/init(model:parameters:samplingParameters:injectIntoContext:)`` initializer and the ``LLMLocalParameters``, and ``LLMLocalSamplingParameters`` types.

- Important: ``LLMLocalSchema``, ``LLMLocalSession`` as well as ``LLMLocalPlatform`` shouldn't be used on it's own but always used together with the Spezi `LLMRunner`!

### Setup

In order to use local LLMs within Spezi, the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) needs to be initialized in the Spezi `Configuration` with the ``LLMLocalPlatform``. Only after, the `LLMRunner` can be used to execute LLMs locally.
See the [SpeziLLM documentation](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) for more details.

```swift
class LocalLLMAppDelegate: SpeziAppDelegate {
    override var configuration: Configuration {
        Configuration {
            LLMRunner {
                LLMLocalPlatform()
            }
        }
    }
}
```

### Usage

The code example below showcases the interaction with the local LLM abstractions through the the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner), which is injected into the SwiftUI `Environment` via the `Configuration` shown above.

The ``LLMLocalSchema`` defines the type and configurations of the to-be-executed ``LLMLocalSession``. This transformation is done via the [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) that uses the ``LLMLocalPlatform``. The inference via ``LLMLocalSession/generate()`` returns an `AsyncThrowingStream` that yields all generated `String` pieces.

The ``LLMLocalSession`` contains the ``LLMLocalSession/context`` property which holds the entire history of the model interactions. This includes the system prompt, user input, but also assistant responses.
Ensure the property always contains all necessary information, as the ``LLMLocalSession/generate()`` function executes the inference based on the ``LLMLocalSession/context``.

> Important: The local LLM abstractions should only be used together with the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner)!

```swift
struct LLMLocalDemoView: View {
    @Environment(LLMRunner.self) var runner
    @State var responseText = ""

    var body: some View {
        Text(responseText)
            .task {
                // Instantiate the `LLMLocalSchema` to an `LLMLocalSession` via the `LLMRunner`.
                let llmSession: LLMLocalSession = runner(
                    with: LLMLocalSchema(
                        model: .llama3_1_8B_4bit
                    )
                )

                do {
                    for try await token in try await llmSession.generate() {
                        responseText.append(token)
                    }
                } catch {
                    // Handle errors here. E.g., you can use `ViewState` and `viewStateAlert` from SpeziViews.
                }
            }
    }
}
```

## Topics

### LLM Local abstraction

- ``LLMLocalSchema``
- ``LLMLocalSession``

### LLM Execution

- ``LLMLocalPlatform``
- ``LLMLocalPlatformConfiguration``

### LLM Configuration

- ``LLMLocalParameters``
- ``LLMLocalSamplingParameters``

### Misc

- ``LLMLocalError``

================
File: Sources/SpeziLLMLocal/LLMLocalError.swift
================
public enum LLMLocalError: LLMError {
    public var errorDescription: String? {
    public var recoverySuggestion: String? {
    public var failureReason: String? {

================
File: Sources/SpeziLLMLocal/LLMLocalPlatform.swift
================
public actor LLMLocalPlatform: LLMPlatform, DefaultInitializable {
    let configuration: LLMLocalPlatformConfiguration
    @MainActor public var state: LLMPlatformState = .idle
    public init(configuration: LLMLocalPlatformConfiguration) {
    public init() {
    public nonisolated func configure() {
    public nonisolated func callAsFunction(with llmSchema: LLMLocalSchema) -> LLMLocalSession {
    deinit {

================
File: Sources/SpeziLLMLocal/LLMLocalSchema.swift
================
public struct LLMLocalSchema: LLMSchema {
    let parameters: LLMLocalParameters
    let samplingParameters: LLMLocalSamplingParameters
    public let injectIntoContext: Bool
    internal let configuration: ModelConfiguration
    public init(
    internal init(

================
File: Sources/SpeziLLMLocal/LLMLocalSession.swift
================
public final class LLMLocalSession: LLMSession, @unchecked Sendable {
    static let logger = Logger(subsystem: "edu.stanford.spezi", category: "SpeziLLMLocal")
    let platform: LLMLocalPlatform
    var schema: LLMLocalSchema
    @ObservationIgnored private var modelExist: Bool {
    @ObservationIgnored private var task: Task<(), Never>?
    @MainActor public var state: LLMState = .uninitialized
    @MainActor public var context: LLMContext = []
    @MainActor public var customContext: [[String: String]] = []
    @MainActor public var numParameters: Int?
    @MainActor public var modelConfiguration: ModelRegistry?
    @MainActor public var modelContainer: ModelContainer?
    init(_ platform: LLMLocalPlatform, schema: LLMLocalSchema) {
    public func setup() async throws {
    public func generate() async throws -> AsyncThrowingStream<String, any Error> {
    public func cancel() {
    deinit {

================
File: Sources/SpeziLLMLocal/LLMLocalSession+Generate.swift
================
    private var generationParameters: GenerateParameters {
    internal func _generate(continuation: AsyncThrowingStream<String, any Error>.Continuation) async {
        let messages = if await !self.customContext.isEmpty {
            let result = try await modelContainer.perform { modelContext in
                let result = try MLXLMCommon.generate(
    private func prepareModelInput(messages: [[String: String]], modelContainer: ModelContainer) async throws -> LMInput {
                let tokens = try modelContext.tokenizer.applyChatTemplate(messages: messages, chatTemplate: chatTemplate)
    private func processTokens(
            let lastTokens = Array(tokens.suffix(schema.parameters.displayEveryNTokens))
            let text = modelContext.tokenizer.decode(tokens: lastTokens)
    private func processRemainingTokens(
        let remainingTokens = result.tokens.count % schema.parameters.displayEveryNTokens
        let lastTokens = Array(result.tokens.suffix(remainingTokens))
    private func handleError(_ message: String, error: LLMLocalError, continuation: AsyncThrowingStream<String, any Error>.Continuation) async {
    private func _mockGenerate(continuation: AsyncThrowingStream<String, any Error>.Continuation) async {
        let tokens = [

================
File: Sources/SpeziLLMLocal/LLMLocalSession+Setup.swift
================
    private func verifyModelDownload() -> Bool {
        let repo = Hub.Repo(id: self.schema.configuration.name)
        let url = HubApi.shared.localRepoLocation(repo)
        let modelFileExtension = ".safetensors"
            let contents = try FileManager.default.contentsOfDirectory(atPath: url.path())
    internal func _setup(continuation: AsyncThrowingStream<String, any Error>.Continuation?) async -> Bool {
            let modelContainer = try await LLMModelFactory.shared.loadContainer(configuration: self.schema.configuration)
            let numParams = await modelContainer.perform { modelContext in
    private func _mockSetup(continuation: AsyncThrowingStream<String, any Error>.Continuation?) async -> Bool {

================
File: Sources/SpeziLLMLocal/LLMLocalSession+Update.swift
================
    public func update(

================
File: Sources/SpeziLLMLocalDownload/Resources/Localizable.xcstrings
================
{
  "sourceLanguage" : "en",
  "strings" : {
    "Downloaded %@%% of 100%%." : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Downloaded %@%% of 100%%."
          }
        }
      }
    },
    "LLM_ALREADY_DOWNLOADED_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The Large Language Model is already present on the device."
          }
        }
      }
    },
    "LLM_DOWNLOAD_BUTTON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Download LLM"
          }
        }
      }
    },
    "LLM_DOWNLOAD_FAILED_ERROR" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "An error occurred during the download of the LLM."
          }
        }
      }
    },
    "LLM_DOWNLOAD_NEXT_BUTTON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Finish setup"
          }
        }
      }
    },
    "LLM_DOWNLOAD_SUBTITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "In order to locally execute a LLM, first download a model."
          }
        }
      }
    },
    "LLM_DOWNLOAD_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM Download"
          }
        }
      }
    },
    "LLM_DOWNLOADED_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The Large Language Model is downloaded to the device."
          }
        }
      }
    },
    "LLM_DOWNLOADING_PROGRESS_TEXT" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Downloading..."
          }
        }
      }
    }
  },
  "version" : "1.0"
}

================
File: Sources/SpeziLLMLocalDownload/Resources/Localizable.xcstrings.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Sources/SpeziLLMLocalDownload/SpeziLLMLocalDownload.docc/SpeziLLMLocalDownload.md
================
# ``SpeziLLMLocalDownload``

<!--
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#       
-->

Provides download and storage functionality for Large Language Models (LLMs).

## Overview

The ``SpeziLLMLocalDownload`` target provides download and storage functionality for Large Language Models (LLMs) to the local device in the Spezi ecosystem. As Language Models typically have big file sizes and therefore long transmission times, the download process has to be properly managed with care. ``SpeziLLMLocalDownload`` offers reusable view- and manager components for developers to easily achieve their desired local LLM setup process in an application.

## Setup

### Add Spezi LLM as a Dependency

You need to add the SpeziLLM Swift package to
[your app in Xcode](https://developer.apple.com/documentation/xcode/adding-package-dependencies-to-your-app#) or
[Swift package](https://developer.apple.com/documentation/xcode/creating-a-standalone-swift-package-with-xcode#Add-a-dependency-on-another-Swift-package).

> Important: If your application is not yet configured to use Spezi, follow the [Spezi setup article](https://swiftpackageindex.com/stanfordspezi/spezi/documentation/spezi/initial-setup) to set up the core Spezi infrastructure.

## Spezi LLM Local Download Components

The two main components of ``SpeziLLMLocalDownload`` are the ``LLMLocalDownloadView`` providing an out-of-the-box onboarding view to download large models and the ``LLMLocalDownloadManager`` that contains all the logic for the model download and local storage.

### Download View

The ``LLMLocalDownloadView`` provides an out-of-the-box onboarding view for downloading locally executed [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) LLMs to the device.
It can be combined with the [SpeziOnboarding](https://swiftpackageindex.com/stanfordspezi/spezionboarding/documentation) [`OnboardingStack`](https://swiftpackageindex.com/stanfordspezi/spezionboarding/documentation/spezionboarding/onboardingstack) to create an easy onboarding flow within the application.
The ``LLMLocalDownloadView`` automatically checks if the model already exists on disk, and if not, offers the start of the download via a button click. The download process itself includes the presentation of a percentage progress view in order to give the user a better feeling for the download progress.

The ``LLMLocalDownloadView/init(model:downloadDescription:action:)-4a14v`` initializer accepts a download description displayed in the view, the remote download `URL` of the LLM, the local storage `URL` of the downloaded model, as well as an action closure to move onto the next (onboarding) step.

The heavy lifting of downloading and storing the model is done by the ``LLMLocalDownloadManager`` which exposes the current downloading state view the ``LLMLocalDownloadManager/state`` property of type ``LLMLocalDownloadManager/DownloadState``.

#### Usage

The code example below creates an onboarding flow via the [SpeziOnboarding](https://swiftpackageindex.com/stanfordspezi/spezionboarding/documentation) [`OnboardingStack`](https://swiftpackageindex.com/stanfordspezi/spezionboarding/documentation/spezionboarding/onboardingstack) that downloads and stores an Language Model on the local device via the use of the ``LLMLocalDownloadView``.

```swift
struct LLMLocalDownloadApp: View {
    @State private var path = NavigationPath()

    var body: some View {
        NavigationStack(path: $path) {
            LLMLocalOnboardingDownloadView()
        }
    }
}
```

```swift
struct LLMLocalOnboardingDownloadView: View {
    @Environment(OnboardingNavigationPath.self) private var onboardingNavigationPath

    var body: some View {
        LLMLocalDownloadView(
            model: .llama3_8B_4bit,
            downloadDescription: "The Llama3 8B model will be downloaded"
        ) {
            onboardingNavigationPath.nextStep()
        }
    }
}
```

### Download Manager

The ``LLMLocalDownloadManager`` manages the download and storage of Large Language Models to the local device.

One configures the ``LLMLocalDownloadManager`` via the ``LLMLocalDownloadManager/init(model:)`` initializer,
passing a download `URL` as well as a local storage `URL` to the ``LLMLocalDownloadManager``.
The download of a model is started via ``LLMLocalDownloadManager/startDownload()`` and can be cancelled (early) via ``LLMLocalDownloadManager/cancelDownload()``.

The current state of the ``LLMLocalDownloadManager`` is exposed via the ``LLMLocalDownloadManager/state`` property which
is of type ``LLMLocalDownloadManager/DownloadState``, containing states such as ``LLMLocalDownloadManager/DownloadState/downloading(progress:)`` which includes the progress of the download or ``LLMLocalDownloadManager/DownloadState/downloaded`` which indicates that the download has finished.

## Topics

### Views

- ``LLMLocalDownloadView``

### Operations

- ``LLMLocalDownloadManager``

================
File: Sources/SpeziLLMLocalDownload/LLMLocalDownloadManager.swift
================
public final class LLMLocalDownloadManager: NSObject {
    public enum DownloadState: Equatable {
    @ObservationIgnored private var downloadTask: Task<(), Never>?
    @MainActor public var state: DownloadState = .idle
    private let model: LLMLocalModel
    @ObservationIgnored public var modelExist: Bool {
    public init(model: LLMLocalModel) {
    public static func modelExist(model: LLMLocalModel) -> Bool {
        let repo = Hub.Repo(id: model.hubID)
        let url = HubApi.shared.localRepoLocation(repo)
        let modelFileExtension = ".safetensors"
            let contents = try FileManager.default.contentsOfDirectory(atPath: url.path())
    public func startDownload() async {
    public func cancelDownload() async {
    private func downloadWithHub() async throws {
        func mutate(progress: Progress) {
        let modelFiles = ["*.safetensors", "config.json"]

================
File: Sources/SpeziLLMLocalDownload/LLMLocalDownloadManager+OperationState.swift
================
    public var representation: ViewState {

================
File: Sources/SpeziLLMLocalDownload/LLMLocalDownloadView.swift
================
public struct LLMLocalDownloadView: View {
    @State private var downloadManager: LLMLocalDownloadManager
    private let action: () async throws -> Void
    private let downloadDescription: Text
    @State private var viewState: ViewState = .idle
    public var body: some View {
    @MainActor @ViewBuilder private var informationView: some View {
    @MainActor private var downloadButton: some View {
    @MainActor private var downloadProgressView: some View {
    @MainActor private var isDownloading: Bool {
    @MainActor private var downloadProgress: Double {
    private var modelExist: Bool {
    public init(
    public init<S: StringProtocol>(

================
File: Sources/SpeziLLMOpenAI/Configuration/LLMOpenAIModelParameters.swift
================
public struct LLMOpenAIModelParameters: Sendable {
    public enum ResponseFormat {
        var openAiRepresentation: Components.Schemas.CreateChatCompletionRequest.response_formatPayload {
    let responseFormat: Components.Schemas.CreateChatCompletionRequest.response_formatPayload?
    let temperature: Double?
    let topP: Double?
    let completionsPerOutput: Int?
    let stopSequence: [String]
    let maxOutputLength: Int?
    let seed: Int?
    let presencePenalty: Double?
    let frequencyPenalty: Double?
    let logitBias: Components.Schemas.CreateChatCompletionRequest.logit_biasPayload
    let user: String?
    public init(

================
File: Sources/SpeziLLMOpenAI/Configuration/LLMOpenAIParameters.swift
================
public struct LLMOpenAIParameters: Sendable {
    public enum ModelType: String, Sendable {
    public enum Defaults {
        public static let defaultOpenAISystemPrompt: String = {
    let modelType: String
    let systemPrompts: [String]
    let modelAccessTest: Bool
    let overwritingToken: String?
    public init(

================
File: Sources/SpeziLLMOpenAI/Configuration/LLMOpenAIPlatformConfiguration.swift
================
public struct LLMOpenAIPlatformConfiguration: Sendable {
    let taskPriority: TaskPriority
    let concurrentStreams: Int
    let apiToken: String?
    let timeout: TimeInterval
    public init(

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/Helpers/_LLMFunctionCollection.swift
================
public struct _LLMFunctionCollection {  // swiftlint:disable:this type_name
    var functions: [String: any LLMFunction] = [:]
    init(functions: [any LLMFunction]) {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/Helpers/LLMFunctionBuilder.swift
================
public enum LLMFunctionBuilder {
    public static func buildExpression<L: LLMFunction>(_ expression: L) -> [L] {
    public static func buildBlock(_ children: [any LLMFunction]...) -> [any LLMFunction] {
    public static func buildOptional(_ component: [any LLMFunction]?) -> [any LLMFunction] {
    public static func buildEither(first: [any LLMFunction]) -> [any LLMFunction] {
    public static func buildEither(second: [any LLMFunction]) -> [any LLMFunction] {
    public static func buildArray(_ components: [[any LLMFunction]]) -> [any LLMFunction] {
    public static func buildLimitedAvailability(_ component: [any LLMFunction]) -> [any LLMFunction] {
    public static func buildFinalResult(_ component: [any LLMFunction]) -> _LLMFunctionCollection {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/Helpers/LLMFunctionParameterCodingKey.swift
================
struct LLMFunctionParameterCodingKey: CodingKey {
    let stringValue: String
    var intValue: Int?
    init(stringValue: String) {
    init?(intValue: Int) {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/Helpers/LLMFunctionParameterIntermediateRepresentation.swift
================
enum LLMFunctionParameterIntermediary: Codable {
    static let encoder = JSONEncoder()
    var topLayerJSONRepresentation: [String: Data] {
        let container = try decoder.singleValueContainer()
    func encode(to encoder: any Encoder) throws {
        var container = encoder.singleValueContainer()

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/Helpers/LLMFunctionParameterItemSchema+ObjectInit.swift
================
    public struct Property: Sendable {
        public enum PropertyType: String, Sendable {
        let name: String
        let type: PropertyType
        let description: String
        public init(name: String, type: PropertyType, description: String) {
    public init(_ objectProperties: Property...) throws {
    public init(_ objectProperties: [Property]) throws {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/Helpers/LLMFunctionParameterPropertySchema+Init.swift
================
    public init(type: Property.PropertyType) throws {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunction.swift
================
public protocol LLMFunction {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameter.swift
================
public protocol LLMFunctionParameter: Decodable {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterArray.swift
================
public protocol LLMFunctionParameterArrayElement: Decodable {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterEnum.swift
================
public protocol LLMFunctionParameterEnum: CaseIterable, RawRepresentable, Decodable {}

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterSchemaCollector.swift
================
protocol LLMFunctionParameterSchemaCollector {
    var schemaValueCollectors: [String: any LLMFunctionParameterSchemaCollector] {
    var schema: LLMFunctionParameterSchema {
            let requiredPropertyNames = Array(
            let properties = schemaValueCollectors.compactMapValues { $0.schema }
            var functionParameterSchema: LLMFunctionParameterSchema = .init()

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterValueCollector.swift
================
protocol LLMFunctionParameterValueCollector {
    var isOptional: Bool {
    func retrieve(from data: Data) throws {
    var parameterValueCollectors: [String: any LLMFunctionParameterValueCollector] {
    func retrieveProperties<Value>(ofType type: Value.Type) -> [String: Value] {
        let mirror = Mirror(reflecting: self)
    func injectParameters(from parameterData: Data) throws {
        let topLayerParameterData = try JSONDecoder().decode(
                let missingCodingKey = LLMFunctionParameterCodingKey(stringValue: propertyName)

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterWrapper.swift
================
public class _LLMFunctionParameterWrapper<T: Decodable>: LLMFunctionParameterSchemaCollector { // swiftlint:disable:this type_name
    var logger = Logger(subsystem: "edu.stanford.spezi", category: "SpeziLLMOpenAI")
    private var injectedValue: T?
    var schema: LLMFunctionParameterItemSchema
    public var wrappedValue: T {
    public convenience init(description _: some StringProtocol) where T: LLMFunctionParameter {
    init(schema: LLMFunctionParameterItemSchema) {
    func inject(_ value: T) where T: Decodable {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterWrapper+ArrayTypes.swift
================
    public convenience init(

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterWrapper+CustomTypes.swift
================
    public convenience init(
            let itemSchema = T.Element.itemSchema.value
            let itemSchema = T.Wrapped.Element.itemSchema.value

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterWrapper+Enum.swift
================
    public convenience init(

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterWrapper+Models.swift
================
    public enum Format: String {
        public var rawValue: String {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterWrapper+NilValue.swift
================
protocol NilValueProtocol {
    func nilValue<Value>(_ value: Value.Type) -> Value {

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterWrapper+OptionalTypes.swift
================
    public convenience init(

================
File: Sources/SpeziLLMOpenAI/FunctionCalling/LLMFunctionParameterWrapper+PrimitiveTypes.swift
================
    convenience init(
    public convenience init(

================
File: Sources/SpeziLLMOpenAI/Helpers/Chat+OpenAI.swift
================
    var openAIRepresentation: Role {

================
File: Sources/SpeziLLMOpenAI/Helpers/LLMOpenAIConstants.swift
================
enum LLMOpenAIConstants {
    static let credentialsUsername = "OpenAIGPT"

================
File: Sources/SpeziLLMOpenAI/Helpers/LLMOpenAIStreamResult.swift
================
struct LLMOpenAIStreamResult {
    struct FunctionCall {
        var id: String?
        var name: String?
        var arguments: String?
        init(name: String? = nil, id: String? = nil, arguments: String? = nil) {
    var deltaContent: String?
    var role: Role?
    var finishReason: FinishReason?
    var functionCall: [FunctionCall]
    var currentFunctionCallIndex = -1
    init(deltaContent: String? = nil, role: Role? = nil, finishReason: FinishReason? = nil, functionCall: [FunctionCall] = []) {
    mutating func append(choice: Components.Schemas.CreateChatCompletionStreamResponse.choicesPayloadPayload) -> Self {
        var newFunctionCall = functionCall[currentFunctionCallIndex]

================
File: Sources/SpeziLLMOpenAI/Onboarding/LLMOpenAIAPITokenOnboardingStep.swift
================
public struct LLMOpenAIAPITokenOnboardingStep: View {
    @Environment(LLMOpenAITokenSaver.self) private var tokenSaver
    private let actionText: String
    private let action: () -> Void
    public var body: some View {
        @Bindable var tokenSaver = tokenSaver
    public init(
    public init<ActionText: StringProtocol>(

================
File: Sources/SpeziLLMOpenAI/Onboarding/LLMOpenAIModelOnboardingStep.swift
================
public struct LLMOpenAIModelOnboardingStep: View {
    public enum Default {
        public static let models: [LLMOpenAIParameters.ModelType] = [
    @State private var modelSelection: LLMOpenAIParameters.ModelType
    private let actionText: String
    private let action: (LLMOpenAIParameters.ModelType) -> Void
    private let models: [LLMOpenAIParameters.ModelType]
    public var body: some View {
    public init(
    public init<ActionText: StringProtocol>(

================
File: Sources/SpeziLLMOpenAI/Resources/Localizable.xcstrings
================
{
  "sourceLanguage" : "en",
  "strings" : {
    "LLM_CONNECTIVITY_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Connectivity Error with the OpenAI API."
          }
        }
      }
    },
    "LLM_CONNECTIVITY_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The network connection to the OpenAI servers couldn't be established, most probably the device doesn't have an internet connection."
          }
        }
      }
    },
    "LLM_CONNECTIVITY_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please ensure that the device has a stable internet connection."
          }
        }
      }
    },
    "LLM_FUNCTION_CALL_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM Function Call has failed."
          }
        }
      }
    },
    "LLM_FUNCTION_CALL_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM Function Call executed on the device has thrown an error."
          }
        }
      }
    },
    "LLM_FUNCTION_CALL_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please retry the input query or restart the application."
          }
        }
      }
    },
    "LLM_FUNCTION_CALL_SCHEMA_EXTRACTION_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Function Call schema definition could not be extracted."
          }
        }
      }
    },
    "LLM_FUNCTION_CALL_SCHEMA_EXTRACTION_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM Function Call schema definition could not be extracted from the defined function call."
          }
        }
      }
    },
    "LLM_FUNCTION_CALL_SCHEMA_EXTRACTION_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please retry the input query or restart the application."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Generation Error occurred during OpenAI inference."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The OpenAI API responded with an error during the output generation."
          }
        }
      }
    },
    "LLM_GENERATION_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please retry the input query."
          }
        }
      }
    },
    "LLM_INSUFFICIENT_QUOTA_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI Quota limit reached."
          }
        }
      }
    },
    "LLM_INSUFFICIENT_QUOTA_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The specified OpenAI API key has reached the quota limit of the associated OpenAI account."
          }
        }
      }
    },
    "LLM_INSUFFICIENT_QUOTA_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please increase the OpenAI quota limits or try again later."
          }
        }
      }
    },
    "LLM_INVALID_FUNCTION_ARGUMENTS_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM Function Call could not be executed."
          }
        }
      }
    },
    "LLM_INVALID_FUNCTION_ARGUMENTS_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The parameters specified by the LLM Function Call do not match the argument sent by OpenAI. "
          }
        }
      }
    },
    "LLM_INVALID_FUNCTION_ARGUMENTS_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please ensure that the specified OpenAI Function Call schema is reflected within the SpeziLLM Function Call definition."
          }
        }
      }
    },
    "LLM_INVALID_FUNCTION_CALL_NAME_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM called an unknown function."
          }
        }
      }
    },
    "LLM_INVALID_FUNCTION_CALL_NAME_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM has tried to call a function that doesn't exist."
          }
        }
      }
    },
    "LLM_INVALID_FUNCTION_CALL_NAME_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please retry the input query."
          }
        }
      }
    },
    "LLM_INVALID_TOKEN_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI API Key invalid."
          }
        }
      }
    },
    "LLM_INVALID_TOKEN_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The specified OpenAI API key is not valid."
          }
        }
      }
    },
    "LLM_INVALID_TOKEN_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please ensure that the configured OpenAI API key is valid and able to access OpenAI models."
          }
        }
      }
    },
    "LLM_MISSING_TOKEN_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI API Key missing."
          }
        }
      }
    },
    "LLM_MISSING_TOKEN_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI API Key wasn't set before using the APIs generational capabilities."
          }
        }
      }
    },
    "LLM_MISSING_TOKEN_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Ensure that the API Key is set before dispatching the first inference."
          }
        }
      }
    },
    "LLM_MODEL_ACCESS_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI GPT Model couldn't be accessed."
          }
        }
      }
    },
    "LLM_MODEL_ACCESS_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The specified GPT model in combination with the API key couldn't be accessed. Ensure that the API key can access the specified model."
          }
        }
      }
    },
    "LLM_MODEL_ACCESS_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please ensure that the specified API key has access to the configured GPT model."
          }
        }
      }
    },
    "LLM_STORAGE_ERROR_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Token could not be stored in a secure storage."
          }
        }
      }
    },
    "LLM_STORAGE_ERROR_FAILURE_REASON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The configured OpenAI API key could not be stored in the secure enclave."
          }
        }
      }
    },
    "LLM_STORAGE_ERROR_RECOVERY_SUGGESTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please try to restart the application or the phone."
          }
        }
      }
    },
    "OPENAI_API_KEY_PROMPT" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI-API-Schlüssel"
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI API Key"
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Clave API de OpenAI"
          }
        }
      }
    },
    "OPENAI_API_KEY_SAVE_BUTTON" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Weiter"
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Next"
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Siguiente"
          }
        }
      }
    },
    "OPENAI_API_KEY_SUBTITLE" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Bitte geben Sie Ihren OpenAI-API-Schlüssel ein."
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Please enter your OpenAI API key."
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Por favor, introduzca su clave API de OpenAI."
          }
        }
      }
    },
    "OPENAI_API_KEY_SUBTITLE_HINT" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Sie können Ihre OpenAI-API-Schlüssel [im API-Schlüsselbereich Ihres OpenAI-Kontos erstellen und überprüfen.](https://platform.openai.com/account/api-keys)"
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "You can create and inspect your OpenAI API keys [in the API keys section of your OpenAI Account](https://platform.openai.com/account/api-keys)."
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Puede crear e inspeccionar sus claves API de OpenAI [en la sección de claves API de su cuenta de OpenAI.](https://platform.openai.com/account/api-keys)"
          }
        }
      }
    },
    "OPENAI_API_KEY_TITLE" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI-API-Schlüssel"
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI API Key"
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Clave API de OpenAI"
          }
        }
      }
    },
    "OPENAI_MODEL_SELECTION_DESCRIPTION" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI GPT-Modell"
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OpenAI GPT Model"
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Modelo GPT de OpenAI"
          }
        }
      }
    },
    "OPENAI_MODEL_SELECTION_SAVE_BUTTON" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Weiter"
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Next"
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Siguiente"
          }
        }
      }
    },
    "OPENAI_MODEL_SELECTION_SUBTITLE" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Wählen Sie das OpenAI-Modell, das Sie verwenden möchten. Stellen Sie sicher, dass Ihr API-Schlüssel angemessenen Zugriff auf das ausgewählte Modell hat."
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Select the OpenAI model that you want to use. Ensure that your API key has proper access to the model you select."
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Seleccione el modelo de OpenAI que desea utilizar. Asegúrese de que su clave API tenga acceso adecuado al modelo que seleccione."
          }
        }
      }
    },
    "OPENAI_MODEL_SELECTION_TITLE" : {
      "localizations" : {
        "de" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Wählen Sie ein OpenAI-Modell"
          }
        },
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Select an OpenAI Model"
          }
        },
        "es" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Seleccione un Modelo de OpenAI"
          }
        }
      }
    },
    "SPEZI_LLM_OPENAI_SYSTEM_PROMPT" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "You are ChatGPT, a large language model trained by OpenAI, based on the GPT architecture."
          }
        }
      }
    }
  },
  "version" : "1.0"
}

================
File: Sources/SpeziLLMOpenAI/Resources/Localizable.xcstrings.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Sources/SpeziLLMOpenAI/SpeziLLMOpenAI.docc/FunctionCalling.md
================
# Function Calling

<!--
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#       
-->

Function calling with LLMs from OpenAI.

## Overview

The OpenAI GPT-based LLMs provide [function calling capabilities](https://platform.openai.com/docs/guides/function-calling) in order to enable a structured, bidirectional, and reliable communication between the OpenAI LLMs and external tools, such as the Spezi ecosystem. <!-- markdown-link-check-disable-line -->
``SpeziLLMOpenAI`` provides a declarative Domain Specific Language to make LLM function calling as seamless as possible within Spezi.

## Usage

The function calling mechanism reflected within the DSL of ``SpeziLLMOpenAI`` consists of two main components: The ``LLMFunction``, serving as a representative of the to-be executed function, and the ``LLMFunction/Parameter``s, declaring which parameters the ``LLMFunction`` receives from the LLM upon execution.

The crucial properties of a ``LLMFunction`` are the ``LLMFunction/name``, serving as the identifier of the function, as well as the ``LLMFunction/description``, enabling the LLM to understand the purpose of the available function call.
The actual logic that is executed upon a function call by the LLM resides in the ``LLMFunction/execute()`` function.
``LLMFunction/execute()`` returns a `String?`, containing all the information the Spezi application wants to provide to the LLM upon a function call, and is automatically injected into the LLM conversation by ``SpeziLLMOpenAI``.

The ``LLMFunction/Parameter`` property wrapper (`@Parameter`) can be used within an ``LLMFunction`` to declare that the function takes a number of arguments of specific type.
As the function is called by the LLM, the function parameters that are sent by the LLM are automatically injected into the ``LLMFunction`` by ``SpeziLLMOpenAI``.
The wrapper contains various initializers for the respective wrapped types of the parameter, such as `Int`, `Float`, `Double`, `Bool` or `String`, as well as `Optional`, `array`, and `enum` data types.
For these types, ``SpeziLLMOpenAI`` is able to automatically synthezise the OpenAI function parameter schema from the declared ``LLMFunction/Parameter``s.

> Tip: In case developers want to manually define schema's for custom and complex types, please refer to ``LLMFunctionParameter``, ``LLMFunctionParameterEnum``, and ``LLMFunctionParameterArrayElement``.

The available ``LLMFunction``s are then declared via ``LLMOpenAI/init(parameters:modelParameters:_:)`, enabling the LLM to pick relevant functions and call them in order to receive more information or execute a specific programatic functionality.

### Example

A full code example of using a ``LLMFunction`` using the ``LLMOpenAISchema`` (configuration of the LLM) can be found below.
As LLMs cannot access real time information, the OpenAI model is provided with a weather ``LLMFunction``, enabling the LLM to fetch up-to-date weather information for a specific location.

```swift
// The defined `LLMFunction` made available to the OpenAI LLM
struct WeatherFunction: LLMFunction {
    static let name: String = "get_current_weather"
    static let description: String = "Get the current weather in a given location"

    @Parameter(description: "The city and state of the to be determined weather, e.g. San Francisco, CA")
    var location: String

    func execute() async throws -> String? {
        "The weather at \(location) is 30 degrees"
    }
}

// Enclosing view to display an LLM chat
struct LLMOpenAIChatTestView: View {
    private let schema = LLMOpenAISchema(
        parameters: .init(
            modelType: .gpt4_o,
            systemPrompt: "You're a helpful assistant that answers questions from users."
        )
    ) {
        WeatherFunction()   // State which LLM functions are made available to the OpenAI LLM
    }

    var body: some View {
        LLMChatView(
            schema: schema
        )
    }
}
```

One can even argue that the LLM should be able to request the weather in a specific measuring scale.
This can be achieved by adding a second parameter, in this case a `String`-based `enum`, to the ``LLMFunction``.
The `enum` type has to conform to the ``LLMFunctionParameterEnum`` protocol and needs to be `String`-based (so the `RawValue` must be a `String`).

```swift
struct LLMOpenAIFunctionWeather: LLMFunction {
    // The `enum`-based type used as a function call parameter
    enum TemperatureUnit: String, LLMFunctionParameterEnum {
        case celsius
        case fahrenheit
    }
    
    static let name: String = "get_current_weather"
    static let description: String = "Get the current weather in a given location"
    
    @Parameter(description: "The city and state of the to be determined weather, e.g. San Francisco, CA")
    var location: String
    @Parameter(description: "The unit of the temperature")
    var unit: TemperatureUnit   // Specify the `enum`-based type as a parameter
    
    func execute() async throws -> String? {
        "The weather at \(location) is 30 degrees \(unit)"
    }
}
```

In addition to that, the ``LLMFunction/Parameter`` also enables the usage of `array`-based types with primitive array element types (such as `Int`s or `String`s) out of the box.
This can be incredibly useful if the specific use case requires the LLM to request lots of data from the client.

An example use case for this feature within the health context can be found in the example below.
The LLM is able to request specific types of health data which is then returned by the function call.

```swift
struct LLMOpenAIFunctionHealthData: LLMFunction {
    static let name: String = "get_health_data"
    static let description: String = "Get the health data of a patient based on health data types."
    
    @Parameter(description: "The types of health data that are requested", enum: ["allergies", "medications"])
    var healthDataTypes: [String]   // Use an `array` of `String`s as parameter
    
    func execute() async throws -> String? {
        var healthData = ""
        
        if healthDataTypes.contains(where: { $0 == "allergies" }) {
            healthData += "The patient has an allergy against nuts. "
        }
        if healthDataTypes.contains(where: { $0 == "medications" }) {
            healthData += "The patient takes painkillers twice a day. "
        }
        
        return healthData
    }
}
```

> Tip: In case one wants to use complex custom objects within the ``LLMFunction/Parameter``, one can use ``LLMFunctionParameter`` for regular types and ``LLMFunctionParameterArrayElement`` for `array`-based types to manually specify the conformance of Swift types to the [OpenAI Function calling schema](https://platform.openai.com/docs/guides/function-calling). See the inline DocC documentation for further information. <!-- markdown-link-check-disable-line -->

## Topics

### LLM functions

- ``LLMFunction``
- ``LLMFunction/Parameter``
- ``LLMFunctionBuilder``

### Function Parameters

- ``LLMFunctionParameter``
- ``LLMFunctionParameterEnum``
- ``LLMFunctionParameterArrayElement``

================
File: Sources/SpeziLLMOpenAI/SpeziLLMOpenAI.docc/SpeziLLMOpenAI.md
================
# ``SpeziLLMOpenAI``

<!--
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
#       
-->

Interact with Large Language Models (LLMs) from OpenAI.

## Overview

A module that allows you to interact with GPT-based Large Language Models (LLMs) from OpenAI within your Spezi application.
``SpeziLLMOpenAI`` provides a pure Swift-based API for interacting with the OpenAI GPT API, building on top of the infrastructure of the [SpeziLLM target](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm).

@Row {
    @Column {
        @Image(source: "LLMOpenAIAPITokenOnboardingStep", alt: "Screenshot displaying the OpenAI API Token Onboarding view from Spezi OpenAI") {
            ``LLMOpenAIAPITokenOnboardingStep``
        }
    }
    @Column {
        @Image(source: "LLMOpenAIModelOnboardingStep", alt: "Screenshot displaying the Open AI Model Selection Onboarding Step"){
            ``LLMOpenAIModelOnboardingStep``
        }
    }
    @Column {
        @Image(source: "ChatView", alt: "Screenshot displaying the usage of the LLMOpenAI with the SpeziChat Chat View."){
            ``LLMOpenAISession``
        }
    }
}

## Setup

### Add Spezi LLM as a Dependency

You need to add the SpeziLLM Swift package to
[your app in Xcode](https://developer.apple.com/documentation/xcode/adding-package-dependencies-to-your-app#) or
[Swift package](https://developer.apple.com/documentation/xcode/creating-a-standalone-swift-package-with-xcode#Add-a-dependency-on-another-Swift-package).

> Important: If your application is not yet configured to use Spezi, follow the [Spezi setup article](https://swiftpackageindex.com/stanfordspezi/spezi/documentation/spezi/initial-setup) to set up the core Spezi infrastructure.

## Spezi LLM OpenAI Components

The core components of the ``SpeziLLMOpenAI`` target are the ``LLMOpenAISchema``, ``LLMOpenAISession`` as well as ``LLMOpenAIPlatform``. They heavily use the OpenAI API to perform textual inference on the GPT-3.5 or GPT-4 models from OpenAI.

> Important: To utilize an LLM from OpenAI, an OpenAI API Key is required. Ensure that the OpenAI account associated with the key has enough resources to access the specified model as well as enough credits to perform the actual inference.

> Tip: In order to collect the OpenAI API Key or model type from the user, ``SpeziLLMOpenAI`` provides the ``LLMOpenAIAPITokenOnboardingStep`` and ``LLMOpenAIModelOnboardingStep`` views which can be used in the onboarding flow of the application.

### LLM OpenAI

``LLMOpenAISchema`` offers a variety of configuration possibilities that are supported by the OpenAI API, such as the model type, the system prompt, the temperature of the model, and many more. These options can be set via the ``LLMOpenAISchema/init(parameters:modelParameters:injectIntoContext:_:)`` initializer and the ``LLMOpenAIParameters`` and ``LLMOpenAIModelParameters``.

- Important: The OpenAI LLM abstractions shouldn't be used on it's own but always used together with the Spezi `LLMRunner`.

#### Setup

In order to use OpenAI LLMs, the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) needs to be initialized in the Spezi `Configuration` with the ``LLMOpenAIPlatform``. Only after, the `LLMRunner` can be used to do inference via OpenAI LLMs.
See the [SpeziLLM documentation](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) for more details.

```swift
import Spezi
import SpeziLLM
import SpeziLLMOpenAI

class LLMOpenAIAppDelegate: SpeziAppDelegate {
    override var configuration: Configuration {
         Configuration {
             LLMRunner {
                LLMOpenAIPlatform()
            }
        }
    }
}
```

#### Usage

The code example below showcases the interaction with the OpenAI LLMs within the Spezi ecosystem through the the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner), which is injected into the SwiftUI `Environment` via the `Configuration` shown above.

The ``LLMOpenAISchema`` defines the type and configurations of the to-be-executed ``LLMOpenAISession``. This transformation is done via the [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) that uses the ``LLMOpenAIPlatform``. The inference via ``LLMOpenAISession/generate()`` returns an `AsyncThrowingStream` that yields all generated `String` pieces.

The ``LLMOpenAISession`` contains the ``LLMOpenAISession/context`` property which holds the entire history of the model interactions. This includes the system prompt, user input, but also assistant responses.
Ensure the property always contains all necessary information, as the ``LLMOpenAISession/generate()`` function executes the inference based on the ``LLMOpenAISession/context``

```swift
import SpeziLLM
import SpeziLLMOpenAI
import SwiftUI

struct LLMOpenAIDemoView: View {
    @Environment(LLMRunner.self) var runner
    @State var responseText = ""

    var body: some View {
        Text(responseText)
            .task {
                // Instantiate the `LLMOpenAISchema` to an `LLMOpenAISession` via the `LLMRunner`.
                let llmSession: LLMOpenAISession = runner(
                    with: LLMOpenAISchema(
                        parameters: .init(
                            modelType: .gpt3_5Turbo,
                            systemPrompt: "You're a helpful assistant that answers questions from users.",
                            overwritingToken: "abc123"
                        )
                    )
                )

                do {
                    for try await token in try await llmSession.generate() {
                        responseText.append(token)
                    }
                } catch {
                    // Handle errors here. E.g., you can use `ViewState` and `viewStateAlert` from SpeziViews.
                }
            }
    }
}
```

#### LLM Function Calling

The OpenAI GPT-based LLMs provide function calling capabilities in order to enable a structured, bidirectional, and reliable communication between the OpenAI LLMs and external tools, such as the Spezi ecosystem.
``SpeziLLMOpenAI`` provides a declarative Domain Specific Language to make LLM function calling as seamless as possible within Spezi.
An extensive documentation can be found in <doc:FunctionCalling>.

### Onboarding Flow

The ``LLMOpenAIAPITokenOnboardingStep`` provides a view that can be used for the user to enter an OpenAI API key during onboarding in your Spezi application. The example below showcases of how to can add an OpenAI onboarding step within an application created from the Spezi Template Application below.

First, create a new view to show the onboarding step:

```swift
import SpeziLLMOpenAI
import SpeziOnboarding
import SwiftUI

struct OpenAIAPIKey: View {
    @Environment(OnboardingNavigationPath.self) private var onboardingNavigationPath: OnboardingNavigationPath
    
    var body: some View {
        LLMOpenAIAPITokenOnboardingStep {
            onboardingNavigationPath.nextStep()
        }
    }
}
```

This view can then be added to the `OnboardingFlow` within the Spezi Template Application:

```swift
import SpeziOnboarding
import SwiftUI

struct OnboardingFlow: View {
    @AppStorage(StorageKeys.onboardingFlowComplete) var completedOnboardingFlow = false
    
    var body: some View {
        OnboardingStack(onboardingFlowComplete: $completedOnboardingFlow) {
            // ... other steps
            OpenAIAPIKey()
            // ... other steps
        }
    }
}
```

Now the OpenAI API Key entry view will appear within your application's onboarding process. The API Key entered will be persisted across application launches.

## Topics

### LLM OpenAI abstraction

- ``LLMOpenAISchema``
- ``LLMOpenAISession``

### LLM Execution

- ``LLMOpenAIPlatform``
- ``LLMOpenAIPlatformConfiguration``

### Onboarding

- ``LLMOpenAIAPITokenOnboardingStep``
- ``LLMOpenAIModelOnboardingStep``
- ``LLMOpenAITokenSaver``

### LLM Configuration

- ``LLMOpenAIParameters``
- ``LLMOpenAIModelParameters``

### Misc

- ``LLMOpenAIError``

================
File: Sources/SpeziLLMOpenAI/LLMOpenAIAuthMiddleware.swift
================
struct AuthMiddleware: ClientMiddleware {
    private let APIKey: String
    init(APIKey: String) { self.APIKey = APIKey }
    func intercept(
        var request = request

================
File: Sources/SpeziLLMOpenAI/LLMOpenAIError.swift
================
public enum LLMOpenAIError: LLMError {
    public var errorDescription: String? {
    public var recoverySuggestion: String? {
    public var failureReason: String? {
    func handleErrorCode(_ statusCode: Int) -> LLMOpenAIError {

================
File: Sources/SpeziLLMOpenAI/LLMOpenAIPlatform.swift
================
public class LLMOpenAIPlatform: LLMPlatform, DefaultInitializable, @unchecked Sendable {
    static let logger = Logger(subsystem: "edu.stanford.spezi", category: "SpeziLLMOpenAI")
    private let semaphore: AsyncSemaphore
    let configuration: LLMOpenAIPlatformConfiguration
    @MainActor public var state: LLMPlatformState = .idle
    @Dependency(LLMOpenAITokenSaver.self) private var tokenSaver
    @Dependency(KeychainStorage.self) private var keychainStorage
    public init(configuration: LLMOpenAIPlatformConfiguration) {
    public required convenience init() {
    public func configure() {
    public func callAsFunction(with llmSchema: LLMOpenAISchema) -> LLMOpenAISession {
    func exclusiveAccess() async throws {
    func signal() async {
        let otherTasksWaiting = semaphore.signal()

================
File: Sources/SpeziLLMOpenAI/LLMOpenAISchema.swift
================
public struct LLMOpenAISchema: LLMSchema, @unchecked Sendable {
    public enum Defaults {
        nonisolated(unsafe) public static let emptyLLMFunctions: _LLMFunctionCollection = .init(functions: [])
    let parameters: LLMOpenAIParameters
    let modelParameters: LLMOpenAIModelParameters
    let functions: [String: any LLMFunction]
    public let injectIntoContext: Bool
    public init(

================
File: Sources/SpeziLLMOpenAI/LLMOpenAISession.swift
================
public final class LLMOpenAISession: LLMSession, @unchecked Sendable {
    static let logger = Logger(subsystem: "edu.stanford.spezi", category: "SpeziLLMOpenAI")
    let platform: LLMOpenAIPlatform
    let schema: LLMOpenAISchema
    let keychainStorage: KeychainStorage
    @ObservationIgnored private var tasks: Set<Task<(), Never>> = []
    @ObservationIgnored private var lock = NSLock()
    @ObservationIgnored var wrappedClient: Client?
    @MainActor public var state: LLMState = .uninitialized
    @MainActor public var context: LLMContext = []
    var openAiClient: Client {
    init(_ platform: LLMOpenAIPlatform, schema: LLMOpenAISchema, keychainStorage: KeychainStorage) {
    public func generate() async throws -> AsyncThrowingStream<String, any Error> {
        let task = Task(priority: platform.configuration.taskPriority) {
    public func cancel() {
    deinit {

================
File: Sources/SpeziLLMOpenAI/LLMOpenAISession+Configuration.swift
================
    private var openAIContext: [Components.Schemas.ChatCompletionRequestMessage] {
    var openAIChatQuery: Operations.createChatCompletion.Input {
            let functions: [Components.Schemas.ChatCompletionTool] = try schema.functions.values.compactMap { function in
    private func getChatMessage(_ contextEntity: LLMContextEntity) -> Components.Schemas.ChatCompletionRequestMessage? {

================
File: Sources/SpeziLLMOpenAI/LLMOpenAISession+Generation.swift
================
    func _generate( // swiftlint:disable:this identifier_name function_body_length cyclomatic_complexity
            var llmStreamResults: [Int: LLMOpenAIStreamResult] = [:]
                let response = try await openAiClient.createChatCompletion(openAIChatQuery)
                    let llmError = handleErrorCode(statusCode)
                let chatStream = try response.ok.body.text_event_hyphen_stream
                    let assistantResults = llmStreamResults.values.filter { llmStreamResult in
            let functionCalls = llmStreamResults.values.compactMap { $0.functionCall }.flatMap { $0 }
            let functionCallContext: [LLMContextEntity.ToolCall] = functionCalls.compactMap { functionCall in
                            let functionCallResponse: String?
                                let defaultResponse = "Function call to \(functionCall.name ?? "") succeeded, function intentionally didn't respond anything."

================
File: Sources/SpeziLLMOpenAI/LLMOpenAISession+Setup.swift
================
    private func initaliseClient(_ continuation: AsyncThrowingStream<String, any Error>.Continuation) async -> Bool {
                        let session = URLSession.shared
    func setup(continuation: AsyncThrowingStream<String, any Error>.Continuation) async -> Bool {
    private func modelAccessTest(continuation: AsyncThrowingStream<String, any Error>.Continuation) async -> Bool {
                let llmError = handleErrorCode(statusCode)

================
File: Sources/SpeziLLMOpenAI/LLMOpenAITokenSaver.swift
================
public class LLMOpenAITokenSaver: Module, EnvironmentAccessible, DefaultInitializable {
    static let logger = Logger(subsystem: "edu.stanford.spezi", category: "SpeziLLMOpenAI")
    @Dependency(KeychainStorage.self) @ObservationIgnored private var keychainStorage
    public var token: String = ""
    public var tokenPresent: Bool {
    public required init() {}
    public func configure() {
    public func store() {
    public func delete() {
    static let openAIKey = CredentialsTag.genericPassword(forService: "openai.com")

================
File: Tests/SpeziLLMTests/LLMOpenAIParameterTests+Array.swift
================
final class LLMOpenAIParameterArrayTests: XCTestCase {
    struct Parameters: Encodable {
        static let shared = Self()
        let intArrayParameter = [1, 2]
        let doubleArrayParameter = [3.1, 4.6]
        let boolArrayParameter = [true, false]
        let stringArrayParameter = ["test1", "test2"]
    struct LLMFunctionTest: LLMFunction {
        static let name: String = "test_array_function"
        static let description: String = "This is a test array LLM function."
        let someInitArg: String
        var intArrayParameter: [Int]
        var doubleArrayParameter: [Double]
        var boolArrayParameter: [Bool]
        var stringArrayParameter: [String]
        init(someInitArg: String) {
        func execute() async throws -> String? {
    let llm = LLMOpenAISchema(
    func testLLMFunctionPrimitiveParameters() async throws {
        let llmFunctionPair = try XCTUnwrap(llm.functions.first)
        let llmFunction = llmFunctionPair.value
        let schemaArrayInt = try XCTUnwrap(llmFunction.schemaValueCollectors["intArrayParameter"])
        var schema = schemaArrayInt.schema.value
        var items = schema["items"] as? [String: Any]
        let schemaArrayDouble = try XCTUnwrap(llmFunction.schemaValueCollectors["doubleArrayParameter"])
        let schemaArrayBool = try XCTUnwrap(llmFunction.schemaValueCollectors["boolArrayParameter"])
        let schemaArrayString = try XCTUnwrap(llmFunction.schemaValueCollectors["stringArrayParameter"])
        let parameterData = try XCTUnwrap(
        let llmFunctionResponse = try await llmFunction.execute()

================
File: Tests/SpeziLLMTests/LLMOpenAIParameterTests+CustomTypes.swift
================
final class LLMOpenAIParameterCustomTypesTests: XCTestCase {
    struct CustomType: LLMFunctionParameterArrayElement, Encodable, Equatable {
        static let itemSchema: LLMFunctionParameterItemSchema = {
        var propertyA: String
        var propertyB: Int
    struct Parameters: Encodable {
        static let shared = Self()
        let customArrayParameter = [
        let customOptionalArrayParameter: [CustomType] = []
    struct LLMFunctionTest: LLMFunction {
        static let name: String = "test_custom_type_function"
        static let description: String = "This is a test custom type LLM function."
        let someInitArg: String
        var customArrayParameter: [CustomType]
        var customOptionalArrayParameter: [CustomType]?     // swiftlint:disable:this discouraged_optional_collection
        init(someInitArg: String) {
        func execute() async throws -> String? {
    let llm = LLMOpenAISchema(
    func testLLMFunctionPrimitiveParameters() async throws {
        let llmFunctionPair = try XCTUnwrap(llm.functions.first)
        let llmFunction = llmFunctionPair.value
        let schemaCustomArray = try XCTUnwrap(llmFunction.schemaValueCollectors["customArrayParameter"])
        var schema = schemaCustomArray.schema.value
        var items = schemaCustomArray.schema.value["items"] as? [String: Any]
        var properties = items?["properties"] as? [String: Any]
        let schemaCustomOptionalArray = try XCTUnwrap(llmFunction.schemaValueCollectors["customOptionalArrayParameter"])
        let parameterData = try XCTUnwrap(
        let llmFunctionResponse = try await llmFunction.execute()

================
File: Tests/SpeziLLMTests/LLMOpenAIParameterTests+Enum.swift
================
final class LLMOpenAIParameterEnumTests: XCTestCase {
    enum CustomEnumType: String, LLMFunctionParameterEnum, Encodable {
    struct Parameters: Encodable {
        static let shared = Self()
        let enumParameter = CustomEnumType.optionB
        let optionalEnumParameter = CustomEnumType.optionA
        let arrayEnumParameter = [CustomEnumType.optionA, CustomEnumType.optionB]
        let optionalArrayEnumParameter = [CustomEnumType.optionB, CustomEnumType.optionA]
    struct LLMFunctionTest: LLMFunction {
        static let name: String = "test_enum_function"
        static let description: String = "This is a test enum LLM function."
        let someInitArg: String
        var enumParameter: CustomEnumType
        var optionalEnumParameter: CustomEnumType?
        var arrayEnumParameter: [CustomEnumType]
        var optionalArrayEnumParameter: [CustomEnumType]?   // swiftlint:disable:this discouraged_optional_collection
        init(someInitArg: String) {
        func execute() async throws -> String? {
    let llm = LLMOpenAISchema(
    func testLLMFunctionPrimitiveParameters() async throws {
        let llmFunctionPair = try XCTUnwrap(llm.functions.first)
        let llmFunction = llmFunctionPair.value
        let schemaEnum = try XCTUnwrap(llmFunction.schemaValueCollectors["enumParameter"])
        var schema = schemaEnum.schema.value
        let schemaOptionalEnum = try XCTUnwrap(llmFunction.schemaValueCollectors["optionalEnumParameter"])
        let schemaArrayEnum = try XCTUnwrap(llmFunction.schemaValueCollectors["arrayEnumParameter"])
        var items = schema["items"] as? [String: Any]
        let schemaOptionalArrayEnum = try XCTUnwrap(llmFunction.schemaValueCollectors["optionalArrayEnumParameter"])
        let parameterData = try XCTUnwrap(
        let llmFunctionResponse = try await llmFunction.execute()

================
File: Tests/SpeziLLMTests/LLMOpenAIParameterTests+InvalidParameters.swift
================
final class LLMOpenAIInvalidParametersTests: XCTestCase {
    struct Parameters: Encodable {
        static let shared = Self()
        let intParameter = 12
    struct LLMFunctionTest: LLMFunction {
        static let name: String = "test_invalid_parameters_function"
        static let description: String = "This is a test invalid parameters LLM function."
        let someInitArg: String
        var randomParameter: String
        init(someInitArg: String) {
        func execute() async throws -> String? {
    let llm = LLMOpenAISchema(
    func testLLMFunctionPrimitiveParameters() async throws {
        let llmFunctionPair = try XCTUnwrap(llm.functions.first)
        let llmFunction = llmFunctionPair.value
        let schemaRandomParameter = try XCTUnwrap(llmFunction.schemaValueCollectors["randomParameter"])
        let schema = schemaRandomParameter.schema.value
        let parameterData = try XCTUnwrap(

================
File: Tests/SpeziLLMTests/LLMOpenAIParameterTests+OptionalTypes.swift
================
final class LLMOpenAIParameterOptionalTypesTests: XCTestCase {
    struct Parameters: Encodable {
        static let shared = Self()
        let intParameter = 123
        let doubleParameter: Double = 12.34
        let boolParameter = true
        let stringParameter = "1234"
        let intArrayParameter = [1, 2, 3]
        let doubleArrayParameter = [12.34, 56.78]
        let boolArrayParameter = [true, false]
        let stringArrayParameter = ["1234", "5678"]
    struct LLMFunctionTest: LLMFunction {
        static let name: String = "test_optional_function"
        static let description: String = "This is a test optional LLM function."
        let someInitArg: String
        var intParameter: Int?
        var doubleParameter: Double?
        var boolParameter: Bool?
        var stringParameter: String?
        var intArrayParameter: [Int]?
        var doubleArrayParameter: [Double]?
        var boolArrayParameter: [Bool]?
        var stringArrayParameter: [String]?
        var arrayNilParameter: [String]?
        init(someInitArg: String) {
        func execute() async throws -> String? {
    let llm = LLMOpenAISchema(
    func testLLMFunctionOptionalParameters() async throws { // swiftlint:disable:this function_body_length
        let llmFunctionPair = try XCTUnwrap(llm.functions.first)
        let llmFunction = llmFunctionPair.value
        let schemaOptionalInt = try XCTUnwrap(llmFunction.schemaValueCollectors["intParameter"])
        var schema = schemaOptionalInt.schema.value
        let schemaOptionalDouble = try XCTUnwrap(llmFunction.schemaValueCollectors["doubleParameter"])
        let schemaOptionalBool = try XCTUnwrap(llmFunction.schemaValueCollectors["boolParameter"])
        let schemaOptionalString = try XCTUnwrap(llmFunction.schemaValueCollectors["stringParameter"])
        let schemaArrayInt = try XCTUnwrap(llmFunction.schemaValueCollectors["intArrayParameter"])
        var items = schema["items"] as? [String: Any]
        let schemaArrayDouble = try XCTUnwrap(llmFunction.schemaValueCollectors["doubleArrayParameter"])
        let schemaArrayBool = try XCTUnwrap(llmFunction.schemaValueCollectors["boolArrayParameter"])
        let schemaArrayString = try XCTUnwrap(llmFunction.schemaValueCollectors["stringArrayParameter"])
        let parameterData = try XCTUnwrap(
        let llmFunctionResponse = try await llmFunction.execute()

================
File: Tests/SpeziLLMTests/LLMOpenAIParameterTests+PrimitiveTypes.swift
================
final class LLMOpenAIParameterPrimitiveTypesTests: XCTestCase {
    struct Parameters: Encodable {
        static let shared = Self()
        let intParameter = 12
        let doubleParameter: Double = 12.34
        let boolParameter = true
        let stringParameter = "1234"
    struct LLMFunctionTest: LLMFunction {
        static let name: String = "test_function"
        static let description: String = "This is a test LLM function."
        let someInitArg: String
        var intParameter: Int
        var doubleParameter: Double
        var boolParameter: Bool
        var stringParameter: String
        init(someInitArg: String) {
        func execute() async throws -> String? {
    let llm = LLMOpenAISchema(
    func testLLMFunctionPrimitiveParameters() async throws {
        let llmFunctionPair = try XCTUnwrap(llm.functions.first)
        let llmFunction = llmFunctionPair.value
        let schemaPrimitiveInt = try XCTUnwrap(llmFunction.schemaValueCollectors["intParameter"])
        let schemaPrimitiveDouble = try XCTUnwrap(llmFunction.schemaValueCollectors["doubleParameter"])
        let schemaPrimitiveBool = try XCTUnwrap(llmFunction.schemaValueCollectors["boolParameter"])
        let schemaPrimitiveString = try XCTUnwrap(llmFunction.schemaValueCollectors["stringParameter"])
        let parameterData = try XCTUnwrap(
        let llmFunctionResponse = try await llmFunction.execute()

================
File: Tests/UITests/TestApp/Assets.xcassets/AccentColor.colorset/Contents.json
================
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: Tests/UITests/TestApp/Assets.xcassets/AccentColor.colorset/Contents.json.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/TestApp/Assets.xcassets/AppIcon.appiconset/Contents.json
================
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: Tests/UITests/TestApp/Assets.xcassets/AppIcon.appiconset/Contents.json.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/TestApp/Assets.xcassets/Contents.json
================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

================
File: Tests/UITests/TestApp/Assets.xcassets/Contents.json.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/TestApp/LLMFog/Account/AccountSetupHeader.swift
================
struct AccountSetupHeader: View {
    @Environment(Account.self) private var account
    @Environment(\.accountSetupState) var setupState
    var body: some View {

================
File: Tests/UITests/TestApp/LLMFog/Account/AccountSheet.swift
================
struct AccountSheet: View {
    @Environment(\.dismiss) var dismiss
    @Environment(Account.self) private var account
    @Environment(\.accountRequired) var accountRequired
    @State var isInSetup = false
    var body: some View {

================
File: Tests/UITests/TestApp/LLMFog/LLMFogChatTestView.swift
================
struct LLMFogChatTestView: View {
    static let schema = LLMFogSchema(
    @State var showOnboarding = false
    @State var presentingAccount = false
    var body: some View {

================
File: Tests/UITests/TestApp/LLMLocal/Helpers/Binding+Negate.swift
================


================
File: Tests/UITests/TestApp/LLMLocal/Helpers/StorageKeys.swift
================
enum StorageKeys {
    static let onboardingFlowComplete = "onboardingFlow.complete"

================
File: Tests/UITests/TestApp/LLMLocal/Onboarding/LLMLocalOnboardingDownloadView.swift
================
struct LLMLocalOnboardingDownloadView: View {
    @Environment(OnboardingNavigationPath.self) private var onboardingNavigationPath
    var body: some View {

================
File: Tests/UITests/TestApp/LLMLocal/Onboarding/LLMLocalOnboardingFlow.swift
================
struct LLMLocalOnboardingFlow: View {
    @AppStorage(StorageKeys.onboardingFlowComplete) private var completedOnboardingFlow = false
    var body: some View {

================
File: Tests/UITests/TestApp/LLMLocal/Onboarding/LLMLocalOnboardingWelcomeView.swift
================
struct LLMLocalOnboardingWelcomeView: View {
    @Environment(OnboardingNavigationPath.self) private var onboardingNavigationPath
    var body: some View {

================
File: Tests/UITests/TestApp/LLMLocal/LLMLocalChatTestView.swift
================
struct LLMLocalChatTestView: View {
    let mockMode: Bool
    var body: some View {
    init(mockMode: Bool = false) {

================
File: Tests/UITests/TestApp/LLMLocal/LLMLocalTestView.swift
================
struct LLMLocalTestView: View {
    @AppStorage(StorageKeys.onboardingFlowComplete) private var completedOnboardingFlow = false
    let mockMode: Bool
    var body: some View {
    init(mockMode: Bool = false) {

================
File: Tests/UITests/TestApp/LLMOpenAI/Functions/LLMOpenAIFunctionHealthData.swift
================
struct LLMOpenAIFunctionHealthData: LLMFunction {
    static let name: String = "get_health_data"
    static let description: String = "Get the health data of a patient based on health data types."
    var healthDataTypes: [String]
    func execute() async throws -> String? {
        var healthData = ""

================
File: Tests/UITests/TestApp/LLMOpenAI/Functions/LLMOpenAIFunctionPerson.swift
================
struct LLMOpenAIFunctionPerson: LLMFunction {
    struct CustomArrayItemType: LLMFunctionParameterArrayElement {
        static let itemSchema: LLMFunctionParameterItemSchema = {
        let firstName: String
        let lastName: String
    static let name: String = "get_age_persons"
    static let description: String = "Gets the age of persons."
    var persons: [CustomArrayItemType]
    func execute() async throws -> String? {

================
File: Tests/UITests/TestApp/LLMOpenAI/Functions/LLMOpenAIFunctionWeather.swift
================
struct LLMOpenAIFunctionWeather: LLMFunction {
    enum TemperatureUnit: String, LLMFunctionParameterEnum {
    static let name: String = "get_current_weather"
    static let description: String = "Get the current weather in a given location"
    var location: String
    var unit: TemperatureUnit
    func execute() async throws -> String? {

================
File: Tests/UITests/TestApp/LLMOpenAI/Onboarding/LLMOpenAIModelOnboarding.swift
================
struct LLMOpenAIModelOnboarding: View {
    @Environment(OnboardingNavigationPath.self) private var path
    @State private var showingAlert = false
    @State private var modelSelection: LLMOpenAIParameters.ModelType?
    var body: some View {

================
File: Tests/UITests/TestApp/LLMOpenAI/Onboarding/LLMOpenAITokenOnboarding.swift
================
struct LLMOpenAITokenOnboarding: View {
    @Environment(OnboardingNavigationPath.self) private var path
    @Environment(\.dismiss) private var dismiss
    var body: some View {

================
File: Tests/UITests/TestApp/LLMOpenAI/LLMOpenAIChatTestView.swift
================
struct LLMOpenAIChatTestView: View {
    static let schema = LLMOpenAISchema(
    @LLMSessionProvider(schema: Self.schema) var llm: LLMOpenAISession
    @State var showOnboarding = false
    @State var muted = true
    var body: some View {

================
File: Tests/UITests/TestApp/LLMOpenAI/LLMOpenAIOnboardingView.swift
================
struct LLMOpenAIOnboardingView: View {
    @Environment(\.dismiss) private var dismiss
    var body: some View {

================
File: Tests/UITests/TestApp/Resources/GoogleService-Info.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>CLIENT_ID</key>
	<string>CLIENT_ID</string>
	<key>REVERSED_CLIENT_ID</key>
	<string>REVERSED_CLIENT_ID</string>
	<key>API_KEY</key>
	<string>API_KEY</string>
	<key>GCM_SENDER_ID</key>
	<string>GCM_SENDER_ID</string>
	<key>PLIST_VERSION</key>
	<string>1</string>
	<key>BUNDLE_ID</key>
	<string>edu.stanford.spezillm.testapp</string>
	<key>PROJECT_ID</key>
	<string>spezillmfog</string>
	<key>STORAGE_BUCKET</key>
	<string>STORAGE_BUCKET</string>
	<key>IS_ADS_ENABLED</key>
	<false/>
	<key>IS_ANALYTICS_ENABLED</key>
	<false/>
	<key>IS_APPINVITE_ENABLED</key>
	<true/>
	<key>IS_GCM_ENABLED</key>
	<true/>
	<key>IS_SIGNIN_ENABLED</key>
	<true/>
	<key>GOOGLE_APP_ID</key>
	<string>1:123456789012:ios:1234567890123456789012</string>
</dict>
</plist>

================
File: Tests/UITests/TestApp/Resources/GoogleService-Info.plist.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/TestApp/Resources/Localizable.xcstrings
================
{
  "sourceLanguage" : "en",
  "strings" : {
    "ACCOUNT_SETUP_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "You may login to your existing account. Or create a new one if you don't have one already."
          }
        }
      }
    },
    "ACCOUNT_SIGNED_IN_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "You are already logged in with the account shown below. Continue or change your account by logging out."
          }
        }
      }
    },
    "ACCOUNT_SUBTITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The authentication is provided via SpeziAccount and the Firebase Account Module."
          }
        }
      }
    },
    "ACCOUNT_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Your Account"
          }
        }
      }
    },
    "LLM_DOWNLOAD_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "By default, the application downloads the Llama 3 8B model in its instruct variation. The size of the model is around 5GB."
          }
        }
      }
    },
    "LLM_FOG_CHAT_VIEW_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM Fog Chat"
          }
        }
      }
    },
    "LLM_LOCAL_CHAT_VIEW_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM Local Chat"
          }
        }
      }
    },
    "LLM_OPENAI_CHAT_ONBOARDING_BUTTON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Onboarding"
          }
        }
      }
    },
    "LLM_OPENAI_CHAT_VIEW_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLM OpenAI Chat"
          }
        }
      }
    },
    "LLM_OPENAI_MODEL_SELECTED" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Model Selected"
          }
        }
      }
    },
    "OK" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "OK"
          }
        }
      }
    },
    "SPEZI_LLM_TEST_NAVIGATION_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "SpeziLLM UI Test"
          }
        }
      }
    },
    "WELCOME_AREA1_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Locally execute LLMs directly on your phone."
          }
        }
      }
    },
    "WELCOME_AREA1_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "LLMs on an iPhone"
          }
        }
      }
    },
    "WELCOME_AREA2_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The LLM execution is provided by a module available through the Swift Package Manager."
          }
        }
      }
    },
    "WELCOME_AREA2_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Swift Package Manager"
          }
        }
      }
    },
    "WELCOME_AREA3_DESCRIPTION" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "This application is built on top of the Stanford Spezi ecosystem."
          }
        }
      }
    },
    "WELCOME_AREA3_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "The Stanford Spezi ecosystem"
          }
        }
      }
    },
    "WELCOME_BUTTON" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Next"
          }
        }
      }
    },
    "WELCOME_SUBTITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "This application demonstrates the local execution of a Large Language Model on an iPhone."
          }
        }
      }
    },
    "WELCOME_TITLE" : {
      "localizations" : {
        "en" : {
          "stringUnit" : {
            "state" : "translated",
            "value" : "Local LLM Execution"
          }
        }
      }
    }
  },
  "version" : "1.0"
}

================
File: Tests/UITests/TestApp/Resources/Localizable.xcstrings.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/TestApp/FeatureFlags.swift
================
enum FeatureFlags: Sendable {
    static let mockMode = ProcessInfo.processInfo.arguments.contains("--mockMode")
    static let resetSecureStorage = ProcessInfo.processInfo.arguments.contains("--resetSecureStorage")
    static let showOnboarding = ProcessInfo.processInfo.arguments.contains("--showOnboarding")

================
File: Tests/UITests/TestApp/TestApp.entitlements
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>keychain-access-groups</key>
	<array/>
</dict>
</plist>

================
File: Tests/UITests/TestApp/TestApp.entitlements.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/TestApp/TestApp.swift
================
struct UITestsApp: App {
    enum Tests: String, CaseIterable, Identifiable {
        var id: RawValue {
        func view(withNavigationPath path: Binding<NavigationPath>) -> some View {
    @ApplicationDelegateAdaptor(TestAppDelegate.self) var appDelegate
    @State private var path = NavigationPath()
    var body: some Scene {

================
File: Tests/UITests/TestApp/TestAppDelegate.swift
================
class TestAppDelegate: SpeziAppDelegate {
    private nonisolated static var caCertificateUrl: URL? {
    override var configuration: Configuration {

================
File: Tests/UITests/TestApp/TestAppTestingSetup.swift
================
private struct TestAppTestingSetup: ViewModifier {
    @Environment(KeychainStorage.self) var keychainStorage
    @AppStorage(StorageKeys.onboardingFlowComplete) private var completedOnboardingFlow = false
    func body(content: Content) -> some View {
    func testingSetup() -> some View {

================
File: Tests/UITests/TestAppUITests/TestAppLLMLocalUITests.swift
================
class TestAppLLMLocalUITests: XCTestCase {
    override func setUp() async throws {
        let app = XCUIApplication()
    func testSpeziLLMLocal() throws {
        let inputTextfield = app.textFields["Message Input Textfield"]

================
File: Tests/UITests/TestAppUITests/TestAppLLMOpenAIUITests.swift
================
class TestAppLLMOpenAIUITests: XCTestCase {
    override func setUp() async throws {
        let app = XCUIApplication()
    func testSpeziLLMOpenAIOnboarding() throws {    // swiftlint:disable:this function_body_length
        let alert = app.alerts["Model Selected"]
        let okButton = alert.buttons["OK"]
        let alert2 = app.alerts["Model Selected"]
        let okButton2 = alert.buttons["OK"]
    func testSpeziLLMOpenAIChat() throws {

================
File: Tests/UITests/UITests.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist
================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>IDEDidComputeMac32BitWarning</key>
	<true/>
</dict>
</plist>

================
File: Tests/UITests/UITests.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/UITests.xcodeproj/project.xcworkspace/contents.xcworkspacedata
================
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>

================
File: Tests/UITests/UITests.xcodeproj/project.xcworkspace/contents.xcworkspacedata.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/UITests.xcodeproj/xcshareddata/xcschemes/TestApp.xcscheme
================
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "1600"
   version = "1.7">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES">
      <BuildActionEntries>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "NO"
            buildForProfiling = "NO"
            buildForArchiving = "NO"
            buildForAnalyzing = "NO">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "SpeziLLM"
               BuildableName = "SpeziLLM"
               BlueprintName = "SpeziLLM"
               ReferencedContainer = "container:../..">
            </BuildableReference>
         </BuildActionEntry>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "NO"
            buildForProfiling = "NO"
            buildForArchiving = "NO"
            buildForAnalyzing = "NO">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "SpeziLLMLocal"
               BuildableName = "SpeziLLMLocal"
               BlueprintName = "SpeziLLMLocal"
               ReferencedContainer = "container:../..">
            </BuildableReference>
         </BuildActionEntry>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "NO"
            buildForProfiling = "NO"
            buildForArchiving = "NO"
            buildForAnalyzing = "NO">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "SpeziLLMLocalDownload"
               BuildableName = "SpeziLLMLocalDownload"
               BlueprintName = "SpeziLLMLocalDownload"
               ReferencedContainer = "container:../..">
            </BuildableReference>
         </BuildActionEntry>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "NO"
            buildForProfiling = "NO"
            buildForArchiving = "NO"
            buildForAnalyzing = "NO">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "SpeziLLMOpenAI"
               BuildableName = "SpeziLLMOpenAI"
               BlueprintName = "SpeziLLMOpenAI"
               ReferencedContainer = "container:../..">
            </BuildableReference>
         </BuildActionEntry>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "2F6D139128F5F384007C25D6"
               BuildableName = "TestApp.app"
               BlueprintName = "TestApp"
               ReferencedContainer = "container:UITests.xcodeproj">
            </BuildableReference>
         </BuildActionEntry>
      </BuildActionEntries>
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES"
      codeCoverageEnabled = "YES">
      <CodeCoverageTargets>
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "SpeziLLM"
            BuildableName = "SpeziLLM"
            BlueprintName = "SpeziLLM"
            ReferencedContainer = "container:../..">
         </BuildableReference>
      </CodeCoverageTargets>
      <TestPlans>
         <TestPlanReference
            reference = "container:TestApp.xctestplan"
            default = "YES">
         </TestPlanReference>
      </TestPlans>
      <Testables>
         <TestableReference
            skipped = "NO">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "2F6D13AB28F5F386007C25D6"
               BuildableName = "TestAppUITests.xctest"
               BlueprintName = "TestAppUITests"
               ReferencedContainer = "container:UITests.xcodeproj">
            </BuildableReference>
         </TestableReference>
      </Testables>
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "2F6D139128F5F384007C25D6"
            BuildableName = "TestApp.app"
            BlueprintName = "TestApp"
            ReferencedContainer = "container:UITests.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
      <CommandLineArguments>
         <CommandLineArgument
            argument = "--showOnboarding"
            isEnabled = "NO">
         </CommandLineArgument>
         <CommandLineArgument
            argument = "--mockMode"
            isEnabled = "NO">
         </CommandLineArgument>
         <CommandLineArgument
            argument = "--resetSecureStorage"
            isEnabled = "NO">
         </CommandLineArgument>
      </CommandLineArguments>
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Release"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "2F6D139128F5F384007C25D6"
            BuildableName = "TestApp.app"
            BlueprintName = "TestApp"
            ReferencedContainer = "container:UITests.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>

================
File: Tests/UITests/UITests.xcodeproj/xcshareddata/xcschemes/TestApp.xcscheme.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/UITests.xcodeproj/xcshareddata/xcschemes/TestAppRelease.xcscheme.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/UITests.xcodeproj/project.pbxproj
================
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 56;
	objects = {

/* Begin PBXBuildFile section */
		2F6D139A28F5F386007C25D6 /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = 2F6D139928F5F386007C25D6 /* Assets.xcassets */; };
		2F8A431329130A8C005D2B8F /* TestAppLLMOpenAIUITests.swift in Sources */ = {isa = PBXBuildFile; fileRef = 2F8A431229130A8C005D2B8F /* TestAppLLMOpenAIUITests.swift */; };
		2FA7382C290ADFAA007ACEB9 /* TestApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = 2FA7382B290ADFAA007ACEB9 /* TestApp.swift */; };
		2FAD21CB2CBDC82500C9665B /* SpeziFirebaseAccountStorage in Frameworks */ = {isa = PBXBuildFile; platformFilter = ios; productRef = 2FAD21CA2CBDC82500C9665B /* SpeziFirebaseAccountStorage */; };
		2FD590522A19E9F000153BE4 /* XCTestExtensions in Frameworks */ = {isa = PBXBuildFile; productRef = 2FD590512A19E9F000153BE4 /* XCTestExtensions */; };
		63EF9CF02BA7398C001D92D7 /* TestAppTestingSetup.swift in Sources */ = {isa = PBXBuildFile; fileRef = 63EF9CEF2BA7398C001D92D7 /* TestAppTestingSetup.swift */; };
		9722A5A02B5B5CB20005645E /* SpeziLLM in Frameworks */ = {isa = PBXBuildFile; productRef = 9722A59F2B5B5CB20005645E /* SpeziLLM */; };
		9722A5A22B5B5CB20005645E /* SpeziLLMLocal in Frameworks */ = {isa = PBXBuildFile; productRef = 9722A5A12B5B5CB20005645E /* SpeziLLMLocal */; };
		9722A5A42B5B5CB20005645E /* SpeziLLMLocalDownload in Frameworks */ = {isa = PBXBuildFile; productRef = 9722A5A32B5B5CB20005645E /* SpeziLLMLocalDownload */; };
		9722A5A62B5B5CB20005645E /* SpeziLLMOpenAI in Frameworks */ = {isa = PBXBuildFile; productRef = 9722A5A52B5B5CB20005645E /* SpeziLLMOpenAI */; };
		9748DBED2B5C811F00B917EE /* LLMOpenAIFunctionHealthData.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9748DBEB2B5C811E00B917EE /* LLMOpenAIFunctionHealthData.swift */; };
		9748DBEE2B5C811F00B917EE /* LLMOpenAIFunctionPerson.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9748DBEC2B5C811E00B917EE /* LLMOpenAIFunctionPerson.swift */; };
		9756D2512B0316240006B6BD /* Binding+Negate.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9756D24B2B0316240006B6BD /* Binding+Negate.swift */; };
		9756D2522B0316240006B6BD /* StorageKeys.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9756D24C2B0316240006B6BD /* StorageKeys.swift */; };
		9756D2532B0316240006B6BD /* LLMLocalOnboardingDownloadView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9756D24E2B0316240006B6BD /* LLMLocalOnboardingDownloadView.swift */; };
		9756D2542B0316240006B6BD /* LLMLocalOnboardingWelcomeView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9756D24F2B0316240006B6BD /* LLMLocalOnboardingWelcomeView.swift */; };
		9756D2552B0316240006B6BD /* LLMLocalOnboardingFlow.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9756D2502B0316240006B6BD /* LLMLocalOnboardingFlow.swift */; };
		9756D2582B0316740006B6BD /* Localizable.xcstrings in Resources */ = {isa = PBXBuildFile; fileRef = 9756D2562B0316740006B6BD /* Localizable.xcstrings */; };
		9756D2592B0316740006B6BD /* Localizable.xcstrings.license in Resources */ = {isa = PBXBuildFile; fileRef = 9756D2572B0316740006B6BD /* Localizable.xcstrings.license */; };
		9756D25E2B0316A30006B6BD /* LLMLocalChatTestView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9756D25C2B0316A30006B6BD /* LLMLocalChatTestView.swift */; };
		976179502B034E0400E1046E /* TestAppDelegate.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9761794F2B034E0400E1046E /* TestAppDelegate.swift */; };
		976179522B034F0900E1046E /* TestAppLLMLocalUITests.swift in Sources */ = {isa = PBXBuildFile; fileRef = 976179512B034F0900E1046E /* TestAppLLMLocalUITests.swift */; };
		976179542B03501100E1046E /* FeatureFlags.swift in Sources */ = {isa = PBXBuildFile; fileRef = 976179532B03501100E1046E /* FeatureFlags.swift */; };
		976CE5172BA2C05100E21810 /* GoogleService-Info.plist in Resources */ = {isa = PBXBuildFile; fileRef = 976CE5162BA2C05100E21810 /* GoogleService-Info.plist */; };
		976CE5192BA2C49900E21810 /* AccountSheet.swift in Sources */ = {isa = PBXBuildFile; fileRef = 976CE5182BA2C49900E21810 /* AccountSheet.swift */; };
		976CE51C2BA2C51E00E21810 /* AccountSetupHeader.swift in Sources */ = {isa = PBXBuildFile; fileRef = 976CE51B2BA2C51E00E21810 /* AccountSetupHeader.swift */; };
		9770F2912BB3C40C00478571 /* SpeziFirebaseAccount in Frameworks */ = {isa = PBXBuildFile; platformFilter = ios; productRef = 9770F2902BB3C40C00478571 /* SpeziFirebaseAccount */; };
		9772D6802B03381400E62B9D /* LLMOpenAIOnboardingView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9772D67F2B03381400E62B9D /* LLMOpenAIOnboardingView.swift */; };
		9772D6822B033D5500E62B9D /* LLMOpenAIChatTestView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 9772D6812B033D5500E62B9D /* LLMOpenAIChatTestView.swift */; };
		977C052F2B4CCA0A00BA9861 /* LLMOpenAIFunctionWeather.swift in Sources */ = {isa = PBXBuildFile; fileRef = 977C052E2B4CCA0A00BA9861 /* LLMOpenAIFunctionWeather.swift */; };
		977E49A02B035563001485D4 /* LLMLocalTestView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 977E499F2B035563001485D4 /* LLMLocalTestView.swift */; };
		979D41902BB3EBD8001953BD /* SpeziAccount in Frameworks */ = {isa = PBXBuildFile; platformFilter = ios; productRef = 979D418F2BB3EBD8001953BD /* SpeziAccount */; };
		97A25C942B28DDAB0073B990 /* LLMOpenAIModelOnboarding.swift in Sources */ = {isa = PBXBuildFile; fileRef = 97A25C922B28DDAB0073B990 /* LLMOpenAIModelOnboarding.swift */; };
		97A25C952B28DDAB0073B990 /* LLMOpenAITokenOnboarding.swift in Sources */ = {isa = PBXBuildFile; fileRef = 97A25C932B28DDAB0073B990 /* LLMOpenAITokenOnboarding.swift */; };
		97A36E152B999EA60034D821 /* SpeziLLMFog in Frameworks */ = {isa = PBXBuildFile; productRef = 97A36E142B999EA60034D821 /* SpeziLLMFog */; };
		97C93FD02B999DD10023F4B9 /* LLMFogChatTestView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 97C93FCF2B999DD10023F4B9 /* LLMFogChatTestView.swift */; };
/* End PBXBuildFile section */

/* Begin PBXContainerItemProxy section */
		2F6D13AD28F5F386007C25D6 /* PBXContainerItemProxy */ = {
			isa = PBXContainerItemProxy;
			containerPortal = 2F6D138A28F5F384007C25D6 /* Project object */;
			proxyType = 1;
			remoteGlobalIDString = 2F6D139128F5F384007C25D6;
			remoteInfo = Example;
		};
/* End PBXContainerItemProxy section */

/* Begin PBXFileReference section */
		2F68C3C6292E9F8F00B3E12C /* SpeziLLM */ = {isa = PBXFileReference; lastKnownFileType = wrapper; name = SpeziLLM; path = ../..; sourceTree = "<group>"; };
		2F6D139228F5F384007C25D6 /* TestApp.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = TestApp.app; sourceTree = BUILT_PRODUCTS_DIR; };
		2F6D139928F5F386007C25D6 /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
		2F6D13AC28F5F386007C25D6 /* TestAppUITests.xctest */ = {isa = PBXFileReference; explicitFileType = wrapper.cfbundle; includeInIndex = 0; path = TestAppUITests.xctest; sourceTree = BUILT_PRODUCTS_DIR; };
		2F8A431229130A8C005D2B8F /* TestAppLLMOpenAIUITests.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = TestAppLLMOpenAIUITests.swift; sourceTree = "<group>"; };
		2FA7382B290ADFAA007ACEB9 /* TestApp.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = TestApp.swift; sourceTree = "<group>"; };
		2FB0758A299DDB9000C0B37F /* TestApp.xctestplan */ = {isa = PBXFileReference; lastKnownFileType = text; path = TestApp.xctestplan; sourceTree = "<group>"; };
		63EF9CEF2BA7398C001D92D7 /* TestAppTestingSetup.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = TestAppTestingSetup.swift; sourceTree = "<group>"; };
		9748DBEB2B5C811E00B917EE /* LLMOpenAIFunctionHealthData.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMOpenAIFunctionHealthData.swift; sourceTree = "<group>"; };
		9748DBEC2B5C811E00B917EE /* LLMOpenAIFunctionPerson.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMOpenAIFunctionPerson.swift; sourceTree = "<group>"; };
		9756D24B2B0316240006B6BD /* Binding+Negate.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = "Binding+Negate.swift"; sourceTree = "<group>"; };
		9756D24C2B0316240006B6BD /* StorageKeys.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = StorageKeys.swift; sourceTree = "<group>"; };
		9756D24E2B0316240006B6BD /* LLMLocalOnboardingDownloadView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMLocalOnboardingDownloadView.swift; sourceTree = "<group>"; };
		9756D24F2B0316240006B6BD /* LLMLocalOnboardingWelcomeView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMLocalOnboardingWelcomeView.swift; sourceTree = "<group>"; };
		9756D2502B0316240006B6BD /* LLMLocalOnboardingFlow.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMLocalOnboardingFlow.swift; sourceTree = "<group>"; };
		9756D2562B0316740006B6BD /* Localizable.xcstrings */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.json.xcstrings; path = Localizable.xcstrings; sourceTree = "<group>"; };
		9756D2572B0316740006B6BD /* Localizable.xcstrings.license */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text; path = Localizable.xcstrings.license; sourceTree = "<group>"; };
		9756D25C2B0316A30006B6BD /* LLMLocalChatTestView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMLocalChatTestView.swift; sourceTree = "<group>"; };
		9761794F2B034E0400E1046E /* TestAppDelegate.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = TestAppDelegate.swift; sourceTree = "<group>"; };
		976179512B034F0900E1046E /* TestAppLLMLocalUITests.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = TestAppLLMLocalUITests.swift; sourceTree = "<group>"; };
		976179532B03501100E1046E /* FeatureFlags.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = FeatureFlags.swift; sourceTree = "<group>"; };
		976CE5162BA2C05100E21810 /* GoogleService-Info.plist */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.plist.xml; path = "GoogleService-Info.plist"; sourceTree = "<group>"; };
		976CE5182BA2C49900E21810 /* AccountSheet.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = AccountSheet.swift; sourceTree = "<group>"; };
		976CE51B2BA2C51E00E21810 /* AccountSetupHeader.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = AccountSetupHeader.swift; sourceTree = "<group>"; };
		9772D67F2B03381400E62B9D /* LLMOpenAIOnboardingView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMOpenAIOnboardingView.swift; sourceTree = "<group>"; };
		9772D6812B033D5500E62B9D /* LLMOpenAIChatTestView.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMOpenAIChatTestView.swift; sourceTree = "<group>"; };
		977438092B05709700EC6527 /* libc++.tbd */ = {isa = PBXFileReference; lastKnownFileType = "sourcecode.text-based-dylib-definition"; name = "libc++.tbd"; path = "usr/lib/libc++.tbd"; sourceTree = SDKROOT; };
		977447212B992D3A00D1F85E /* TestApp.entitlements */ = {isa = PBXFileReference; lastKnownFileType = text.plist.entitlements; path = TestApp.entitlements; sourceTree = "<group>"; };
		977C052E2B4CCA0A00BA9861 /* LLMOpenAIFunctionWeather.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = LLMOpenAIFunctionWeather.swift; sourceTree = "<group>"; };
		977E499F2B035563001485D4 /* LLMLocalTestView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = LLMLocalTestView.swift; sourceTree = "<group>"; };
		97A25C922B28DDAB0073B990 /* LLMOpenAIModelOnboarding.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMOpenAIModelOnboarding.swift; sourceTree = "<group>"; };
		97A25C932B28DDAB0073B990 /* LLMOpenAITokenOnboarding.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = LLMOpenAITokenOnboarding.swift; sourceTree = "<group>"; };
		97C93FCF2B999DD10023F4B9 /* LLMFogChatTestView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = LLMFogChatTestView.swift; sourceTree = "<group>"; };
/* End PBXFileReference section */

/* Begin PBXFrameworksBuildPhase section */
		2F6D138F28F5F384007C25D6 /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
				9722A5A02B5B5CB20005645E /* SpeziLLM in Frameworks */,
				2FAD21CB2CBDC82500C9665B /* SpeziFirebaseAccountStorage in Frameworks */,
				9770F2912BB3C40C00478571 /* SpeziFirebaseAccount in Frameworks */,
				97A36E152B999EA60034D821 /* SpeziLLMFog in Frameworks */,
				9722A5A22B5B5CB20005645E /* SpeziLLMLocal in Frameworks */,
				9722A5A62B5B5CB20005645E /* SpeziLLMOpenAI in Frameworks */,
				9722A5A42B5B5CB20005645E /* SpeziLLMLocalDownload in Frameworks */,
				979D41902BB3EBD8001953BD /* SpeziAccount in Frameworks */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		2F6D13A928F5F386007C25D6 /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
				2FD590522A19E9F000153BE4 /* XCTestExtensions in Frameworks */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		2F6D138928F5F384007C25D6 = {
			isa = PBXGroup;
			children = (
				2FB0758A299DDB9000C0B37F /* TestApp.xctestplan */,
				2F68C3C6292E9F8F00B3E12C /* SpeziLLM */,
				2F6D139428F5F384007C25D6 /* TestApp */,
				2F6D13AF28F5F386007C25D6 /* TestAppUITests */,
				2F6D139328F5F384007C25D6 /* Products */,
				2F6D13C228F5F3BE007C25D6 /* Frameworks */,
			);
			sourceTree = "<group>";
		};
		2F6D139328F5F384007C25D6 /* Products */ = {
			isa = PBXGroup;
			children = (
				2F6D139228F5F384007C25D6 /* TestApp.app */,
				2F6D13AC28F5F386007C25D6 /* TestAppUITests.xctest */,
			);
			name = Products;
			sourceTree = "<group>";
		};
		2F6D139428F5F384007C25D6 /* TestApp */ = {
			isa = PBXGroup;
			children = (
				977447212B992D3A00D1F85E /* TestApp.entitlements */,
				9756D25A2B0316790006B6BD /* Resources */,
				97DD56B32B02F72D00389331 /* LLMLocal */,
				97C93FCE2B999DA40023F4B9 /* LLMFog */,
				97DD56B42B02F72D00389331 /* LLMOpenAI */,
				2FA7382B290ADFAA007ACEB9 /* TestApp.swift */,
				9761794F2B034E0400E1046E /* TestAppDelegate.swift */,
				63EF9CEF2BA7398C001D92D7 /* TestAppTestingSetup.swift */,
				976179532B03501100E1046E /* FeatureFlags.swift */,
				2F6D139928F5F386007C25D6 /* Assets.xcassets */,
			);
			path = TestApp;
			sourceTree = "<group>";
		};
		2F6D13AF28F5F386007C25D6 /* TestAppUITests */ = {
			isa = PBXGroup;
			children = (
				976179512B034F0900E1046E /* TestAppLLMLocalUITests.swift */,
				2F8A431229130A8C005D2B8F /* TestAppLLMOpenAIUITests.swift */,
			);
			path = TestAppUITests;
			sourceTree = "<group>";
		};
		2F6D13C228F5F3BE007C25D6 /* Frameworks */ = {
			isa = PBXGroup;
			children = (
				977438092B05709700EC6527 /* libc++.tbd */,
			);
			name = Frameworks;
			sourceTree = "<group>";
		};
		9748DBEF2B5C813000B917EE /* Functions */ = {
			isa = PBXGroup;
			children = (
				9748DBEB2B5C811E00B917EE /* LLMOpenAIFunctionHealthData.swift */,
				977C052E2B4CCA0A00BA9861 /* LLMOpenAIFunctionWeather.swift */,
				9748DBEC2B5C811E00B917EE /* LLMOpenAIFunctionPerson.swift */,
			);
			path = Functions;
			sourceTree = "<group>";
		};
		9756D24A2B0316240006B6BD /* Helpers */ = {
			isa = PBXGroup;
			children = (
				9756D24B2B0316240006B6BD /* Binding+Negate.swift */,
				9756D24C2B0316240006B6BD /* StorageKeys.swift */,
			);
			path = Helpers;
			sourceTree = "<group>";
		};
		9756D24D2B0316240006B6BD /* Onboarding */ = {
			isa = PBXGroup;
			children = (
				9756D24E2B0316240006B6BD /* LLMLocalOnboardingDownloadView.swift */,
				9756D24F2B0316240006B6BD /* LLMLocalOnboardingWelcomeView.swift */,
				9756D2502B0316240006B6BD /* LLMLocalOnboardingFlow.swift */,
			);
			path = Onboarding;
			sourceTree = "<group>";
		};
		9756D25A2B0316790006B6BD /* Resources */ = {
			isa = PBXGroup;
			children = (
				976CE5162BA2C05100E21810 /* GoogleService-Info.plist */,
				9756D2562B0316740006B6BD /* Localizable.xcstrings */,
				9756D2572B0316740006B6BD /* Localizable.xcstrings.license */,
			);
			path = Resources;
			sourceTree = "<group>";
		};
		976CE51A2BA2C4C600E21810 /* Account */ = {
			isa = PBXGroup;
			children = (
				976CE5182BA2C49900E21810 /* AccountSheet.swift */,
				976CE51B2BA2C51E00E21810 /* AccountSetupHeader.swift */,
			);
			path = Account;
			sourceTree = "<group>";
		};
		97A25C912B28DDAB0073B990 /* Onboarding */ = {
			isa = PBXGroup;
			children = (
				97A25C922B28DDAB0073B990 /* LLMOpenAIModelOnboarding.swift */,
				97A25C932B28DDAB0073B990 /* LLMOpenAITokenOnboarding.swift */,
			);
			path = Onboarding;
			sourceTree = "<group>";
		};
		97C93FCE2B999DA40023F4B9 /* LLMFog */ = {
			isa = PBXGroup;
			children = (
				976CE51A2BA2C4C600E21810 /* Account */,
				97C93FCF2B999DD10023F4B9 /* LLMFogChatTestView.swift */,
			);
			path = LLMFog;
			sourceTree = "<group>";
		};
		97DD56B32B02F72D00389331 /* LLMLocal */ = {
			isa = PBXGroup;
			children = (
				9756D24A2B0316240006B6BD /* Helpers */,
				9756D24D2B0316240006B6BD /* Onboarding */,
				9756D25C2B0316A30006B6BD /* LLMLocalChatTestView.swift */,
				977E499F2B035563001485D4 /* LLMLocalTestView.swift */,
			);
			path = LLMLocal;
			sourceTree = "<group>";
		};
		97DD56B42B02F72D00389331 /* LLMOpenAI */ = {
			isa = PBXGroup;
			children = (
				9748DBEF2B5C813000B917EE /* Functions */,
				97A25C912B28DDAB0073B990 /* Onboarding */,
				9772D6812B033D5500E62B9D /* LLMOpenAIChatTestView.swift */,
				9772D67F2B03381400E62B9D /* LLMOpenAIOnboardingView.swift */,
			);
			path = LLMOpenAI;
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		2F6D139128F5F384007C25D6 /* TestApp */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = 2F6D13B628F5F386007C25D6 /* Build configuration list for PBXNativeTarget "TestApp" */;
			buildPhases = (
				2F6D138E28F5F384007C25D6 /* Sources */,
				2F6D138F28F5F384007C25D6 /* Frameworks */,
				2F6D139028F5F384007C25D6 /* Resources */,
				43A784C02A85FDED00C4FE91 /* ShellScript */,
			);
			buildRules = (
			);
			dependencies = (
			);
			name = TestApp;
			packageProductDependencies = (
				9722A59F2B5B5CB20005645E /* SpeziLLM */,
				9722A5A12B5B5CB20005645E /* SpeziLLMLocal */,
				9722A5A32B5B5CB20005645E /* SpeziLLMLocalDownload */,
				9722A5A52B5B5CB20005645E /* SpeziLLMOpenAI */,
				97A36E142B999EA60034D821 /* SpeziLLMFog */,
				9770F2902BB3C40C00478571 /* SpeziFirebaseAccount */,
				979D418F2BB3EBD8001953BD /* SpeziAccount */,
			);
			productName = Example;
			productReference = 2F6D139228F5F384007C25D6 /* TestApp.app */;
			productType = "com.apple.product-type.application";
		};
		2F6D13AB28F5F386007C25D6 /* TestAppUITests */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = 2F6D13BC28F5F386007C25D6 /* Build configuration list for PBXNativeTarget "TestAppUITests" */;
			buildPhases = (
				2F6D13A828F5F386007C25D6 /* Sources */,
				2F6D13A928F5F386007C25D6 /* Frameworks */,
				2F6D13AA28F5F386007C25D6 /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
				2F6D13AE28F5F386007C25D6 /* PBXTargetDependency */,
			);
			name = TestAppUITests;
			packageProductDependencies = (
				2FD590512A19E9F000153BE4 /* XCTestExtensions */,
			);
			productName = ExampleUITests;
			productReference = 2F6D13AC28F5F386007C25D6 /* TestAppUITests.xctest */;
			productType = "com.apple.product-type.bundle.ui-testing";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		2F6D138A28F5F384007C25D6 /* Project object */ = {
			isa = PBXProject;
			attributes = {
				BuildIndependentTargetsInParallel = 1;
				LastSwiftUpdateCheck = 1410;
				LastUpgradeCheck = 1600;
				TargetAttributes = {
					2F6D139128F5F384007C25D6 = {
						CreatedOnToolsVersion = 14.1;
					};
					2F6D13AB28F5F386007C25D6 = {
						CreatedOnToolsVersion = 14.1;
						TestTargetID = 2F6D139128F5F384007C25D6;
					};
				};
			};
			buildConfigurationList = 2F6D138D28F5F384007C25D6 /* Build configuration list for PBXProject "UITests" */;
			compatibilityVersion = "Xcode 14.0";
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
			);
			mainGroup = 2F6D138928F5F384007C25D6;
			packageReferences = (
				2FD590502A19E9F000153BE4 /* XCRemoteSwiftPackageReference "XCTestExtensions" */,
				9770F28F2BB3C40C00478571 /* XCRemoteSwiftPackageReference "SpeziFirebase" */,
				979D418E2BB3EBD8001953BD /* XCRemoteSwiftPackageReference "SpeziAccount" */,
				DA7ED27E2C5FCAEC00BA8C47 /* XCRemoteSwiftPackageReference "swift-openapi-generator" */,
			);
			productRefGroup = 2F6D139328F5F384007C25D6 /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				2F6D139128F5F384007C25D6 /* TestApp */,
				2F6D13AB28F5F386007C25D6 /* TestAppUITests */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		2F6D139028F5F384007C25D6 /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
				9756D2582B0316740006B6BD /* Localizable.xcstrings in Resources */,
				976CE5172BA2C05100E21810 /* GoogleService-Info.plist in Resources */,
				2F6D139A28F5F386007C25D6 /* Assets.xcassets in Resources */,
				9756D2592B0316740006B6BD /* Localizable.xcstrings.license in Resources */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		2F6D13AA28F5F386007C25D6 /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXShellScriptBuildPhase section */
		43A784C02A85FDED00C4FE91 /* ShellScript */ = {
			isa = PBXShellScriptBuildPhase;
			alwaysOutOfDate = 1;
			buildActionMask = 12;
			files = (
			);
			inputFileListPaths = (
			);
			inputPaths = (
			);
			outputFileListPaths = (
			);
			outputPaths = (
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "if [ \"${CONFIGURATION}\" = \"Debug\" ]; then\n  export PATH=\"$PATH:/opt/homebrew/bin\"\n  if which swiftlint > /dev/null; then\n    cd ../.. && swiftlint\n  else\n    echo \"warning: SwiftLint not installed, download from https://github.com/realm/SwiftLint\"\n  fi\nfi\n";
		};
/* End PBXShellScriptBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		2F6D138E28F5F384007C25D6 /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
				97C93FD02B999DD10023F4B9 /* LLMFogChatTestView.swift in Sources */,
				9748DBEE2B5C811F00B917EE /* LLMOpenAIFunctionPerson.swift in Sources */,
				9772D6802B03381400E62B9D /* LLMOpenAIOnboardingView.swift in Sources */,
				9756D25E2B0316A30006B6BD /* LLMLocalChatTestView.swift in Sources */,
				9756D2532B0316240006B6BD /* LLMLocalOnboardingDownloadView.swift in Sources */,
				9756D2542B0316240006B6BD /* LLMLocalOnboardingWelcomeView.swift in Sources */,
				976CE51C2BA2C51E00E21810 /* AccountSetupHeader.swift in Sources */,
				97A25C952B28DDAB0073B990 /* LLMOpenAITokenOnboarding.swift in Sources */,
				63EF9CF02BA7398C001D92D7 /* TestAppTestingSetup.swift in Sources */,
				977E49A02B035563001485D4 /* LLMLocalTestView.swift in Sources */,
				9756D2522B0316240006B6BD /* StorageKeys.swift in Sources */,
				9772D6822B033D5500E62B9D /* LLMOpenAIChatTestView.swift in Sources */,
				976179542B03501100E1046E /* FeatureFlags.swift in Sources */,
				9748DBED2B5C811F00B917EE /* LLMOpenAIFunctionHealthData.swift in Sources */,
				976179502B034E0400E1046E /* TestAppDelegate.swift in Sources */,
				9756D2552B0316240006B6BD /* LLMLocalOnboardingFlow.swift in Sources */,
				97A25C942B28DDAB0073B990 /* LLMOpenAIModelOnboarding.swift in Sources */,
				2FA7382C290ADFAA007ACEB9 /* TestApp.swift in Sources */,
				977C052F2B4CCA0A00BA9861 /* LLMOpenAIFunctionWeather.swift in Sources */,
				976CE5192BA2C49900E21810 /* AccountSheet.swift in Sources */,
				9756D2512B0316240006B6BD /* Binding+Negate.swift in Sources */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		2F6D13A828F5F386007C25D6 /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
				2F8A431329130A8C005D2B8F /* TestAppLLMOpenAIUITests.swift in Sources */,
				976179522B034F0900E1046E /* TestAppLLMLocalUITests.swift in Sources */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin PBXTargetDependency section */
		2F6D13AE28F5F386007C25D6 /* PBXTargetDependency */ = {
			isa = PBXTargetDependency;
			target = 2F6D139128F5F384007C25D6 /* TestApp */;
			targetProxy = 2F6D13AD28F5F386007C25D6 /* PBXContainerItemProxy */;
		};
/* End PBXTargetDependency section */

/* Begin XCBuildConfiguration section */
		2F6D13B428F5F386007C25D6 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = NO;
				GCC_C_LANGUAGE_STANDARD = gnu11;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 17.0;
				MACOSX_DEPLOYMENT_TARGET = 14.0;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				ONLY_ACTIVE_ARCH = YES;
				SDKROOT = iphoneos;
				SWIFT_ACTIVE_COMPILATION_CONDITIONS = DEBUG;
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
				SWIFT_VERSION = 6.0;
				XROS_DEPLOYMENT_TARGET = 1.0;
			};
			name = Debug;
		};
		2F6D13B528F5F386007C25D6 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				ASSETCATALOG_COMPILER_GENERATE_SWIFT_ASSET_SYMBOL_EXTENSIONS = YES;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_USER_SCRIPT_SANDBOXING = NO;
				GCC_C_LANGUAGE_STANDARD = gnu11;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 17.0;
				MACOSX_DEPLOYMENT_TARGET = 14.0;
				MTL_ENABLE_DEBUG_INFO = NO;
				MTL_FAST_MATH = YES;
				SDKROOT = iphoneos;
				SWIFT_COMPILATION_MODE = wholemodule;
				SWIFT_OPTIMIZATION_LEVEL = "-O";
				SWIFT_VERSION = 6.0;
				VALIDATE_PRODUCT = YES;
				XROS_DEPLOYMENT_TARGET = 1.0;
			};
			name = Release;
		};
		2F6D13B728F5F386007C25D6 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CLANG_CXX_LANGUAGE_STANDARD = "c++0x";
				CODE_SIGN_ENTITLEMENTS = TestApp/TestApp.entitlements;
				CODE_SIGN_IDENTITY = "Apple Development";
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "";
				DEVELOPMENT_TEAM = 637867499T;
				ENABLE_PREVIEWS = YES;
				ENABLE_TESTING_SEARCH_PATHS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_KEY_NSMicrophoneUsageDescription = "The Test Application uses the micophone to test the dication functionality.";
				INFOPLIST_KEY_NSSpeechRecognitionUsageDescription = "Speech recognition necessary for transcribing voice input.";
				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MACOSX_DEPLOYMENT_TARGET = 14.0;
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = edu.stanford.spezillm.testapp;
				PRODUCT_NAME = "$(TARGET_NAME)";
				PROVISIONING_PROFILE_SPECIFIER = "";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_STRICT_CONCURRENCY = complete;
				SWIFT_VERSION = 6.0;
				TARGETED_DEVICE_FAMILY = "1,2,7";
			};
			name = Debug;
		};
		2F6D13B828F5F386007C25D6 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CLANG_CXX_LANGUAGE_STANDARD = "c++0x";
				CODE_SIGN_ENTITLEMENTS = TestApp/TestApp.entitlements;
				CODE_SIGN_IDENTITY = "Apple Development";
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "";
				DEVELOPMENT_TEAM = 637867499T;
				ENABLE_PREVIEWS = YES;
				ENABLE_TESTING_SEARCH_PATHS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				INFOPLIST_KEY_NSMicrophoneUsageDescription = "The Test Application uses the micophone to test the dication functionality.";
				INFOPLIST_KEY_NSSpeechRecognitionUsageDescription = "Speech recognition necessary for transcribing voice input.";
				INFOPLIST_KEY_UIApplicationSceneManifest_Generation = YES;
				INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents = YES;
				INFOPLIST_KEY_UILaunchScreen_Generation = YES;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				MACOSX_DEPLOYMENT_TARGET = 14.0;
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = edu.stanford.spezillm.testapp;
				PRODUCT_NAME = "$(TARGET_NAME)";
				PROVISIONING_PROFILE_SPECIFIER = "";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = NO;
				SUPPORTS_XR_DESIGNED_FOR_IPHONE_IPAD = NO;
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_STRICT_CONCURRENCY = complete;
				SWIFT_VERSION = 6.0;
				TARGETED_DEVICE_FAMILY = "1,2,7";
			};
			name = Release;
		};
		2F6D13BD28F5F386007C25D6 /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_TEAM = 637867499T;
				GENERATE_INFOPLIST_FILE = YES;
				MACOSX_DEPLOYMENT_TARGET = 14.0;
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = edu.stanford.spezillm.testappuitests;
				PRODUCT_NAME = "$(TARGET_NAME)";
				PROVISIONING_PROFILE = "";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
				SUPPORTS_MACCATALYST = NO;
				SWIFT_EMIT_LOC_STRINGS = NO;
				SWIFT_VERSION = 6.0;
				TARGETED_DEVICE_FAMILY = "1,2,7";
				TEST_TARGET_NAME = TestApp;
			};
			name = Debug;
		};
		2F6D13BE28F5F386007C25D6 /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_TEAM = 637867499T;
				GENERATE_INFOPLIST_FILE = YES;
				MACOSX_DEPLOYMENT_TARGET = 14.0;
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = edu.stanford.spezillm.testappuitests;
				PRODUCT_NAME = "$(TARGET_NAME)";
				PROVISIONING_PROFILE = "";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx xros xrsimulator";
				SUPPORTS_MACCATALYST = NO;
				SWIFT_EMIT_LOC_STRINGS = NO;
				SWIFT_VERSION = 6.0;
				TARGETED_DEVICE_FAMILY = "1,2,7";
				TEST_TARGET_NAME = TestApp;
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		2F6D138D28F5F384007C25D6 /* Build configuration list for PBXProject "UITests" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				2F6D13B428F5F386007C25D6 /* Debug */,
				2F6D13B528F5F386007C25D6 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		2F6D13B628F5F386007C25D6 /* Build configuration list for PBXNativeTarget "TestApp" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				2F6D13B728F5F386007C25D6 /* Debug */,
				2F6D13B828F5F386007C25D6 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		2F6D13BC28F5F386007C25D6 /* Build configuration list for PBXNativeTarget "TestAppUITests" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				2F6D13BD28F5F386007C25D6 /* Debug */,
				2F6D13BE28F5F386007C25D6 /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */

/* Begin XCRemoteSwiftPackageReference section */
		2FD590502A19E9F000153BE4 /* XCRemoteSwiftPackageReference "XCTestExtensions" */ = {
			isa = XCRemoteSwiftPackageReference;
			repositoryURL = "https://github.com/StanfordBDHG/XCTestExtensions.git";
			requirement = {
				kind = upToNextMajorVersion;
				minimumVersion = 1.1.0;
			};
		};
		9770F28F2BB3C40C00478571 /* XCRemoteSwiftPackageReference "SpeziFirebase" */ = {
			isa = XCRemoteSwiftPackageReference;
			repositoryURL = "https://github.com/StanfordSpezi/SpeziFirebase";
			requirement = {
				kind = upToNextMajorVersion;
				minimumVersion = 2.0.3;
			};
		};
		979D418E2BB3EBD8001953BD /* XCRemoteSwiftPackageReference "SpeziAccount" */ = {
			isa = XCRemoteSwiftPackageReference;
			repositoryURL = "https://github.com/StanfordSpezi/SpeziAccount";
			requirement = {
				kind = upToNextMajorVersion;
				minimumVersion = 2.1.3;
			};
		};
		DA7ED27E2C5FCAEC00BA8C47 /* XCRemoteSwiftPackageReference "swift-openapi-generator" */ = {
			isa = XCRemoteSwiftPackageReference;
			repositoryURL = "https://github.com/apple/swift-openapi-generator.git";
			requirement = {
				kind = upToNextMajorVersion;
				minimumVersion = 1.3.0;
			};
		};
/* End XCRemoteSwiftPackageReference section */

/* Begin XCSwiftPackageProductDependency section */
		2FAD21CA2CBDC82500C9665B /* SpeziFirebaseAccountStorage */ = {
			isa = XCSwiftPackageProductDependency;
			package = 9770F28F2BB3C40C00478571 /* XCRemoteSwiftPackageReference "SpeziFirebase" */;
			productName = SpeziFirebaseAccountStorage;
		};
		2FD590512A19E9F000153BE4 /* XCTestExtensions */ = {
			isa = XCSwiftPackageProductDependency;
			package = 2FD590502A19E9F000153BE4 /* XCRemoteSwiftPackageReference "XCTestExtensions" */;
			productName = XCTestExtensions;
		};
		9722A59F2B5B5CB20005645E /* SpeziLLM */ = {
			isa = XCSwiftPackageProductDependency;
			productName = SpeziLLM;
		};
		9722A5A12B5B5CB20005645E /* SpeziLLMLocal */ = {
			isa = XCSwiftPackageProductDependency;
			productName = SpeziLLMLocal;
		};
		9722A5A32B5B5CB20005645E /* SpeziLLMLocalDownload */ = {
			isa = XCSwiftPackageProductDependency;
			productName = SpeziLLMLocalDownload;
		};
		9722A5A52B5B5CB20005645E /* SpeziLLMOpenAI */ = {
			isa = XCSwiftPackageProductDependency;
			productName = SpeziLLMOpenAI;
		};
		9770F2902BB3C40C00478571 /* SpeziFirebaseAccount */ = {
			isa = XCSwiftPackageProductDependency;
			package = 9770F28F2BB3C40C00478571 /* XCRemoteSwiftPackageReference "SpeziFirebase" */;
			productName = SpeziFirebaseAccount;
		};
		979D418F2BB3EBD8001953BD /* SpeziAccount */ = {
			isa = XCSwiftPackageProductDependency;
			package = 979D418E2BB3EBD8001953BD /* XCRemoteSwiftPackageReference "SpeziAccount" */;
			productName = SpeziAccount;
		};
		97A36E142B999EA60034D821 /* SpeziLLMFog */ = {
			isa = XCSwiftPackageProductDependency;
			productName = SpeziLLMFog;
		};
/* End XCSwiftPackageProductDependency section */
	};
	rootObject = 2F6D138A28F5F384007C25D6 /* Project object */;
}

================
File: Tests/UITests/UITests.xcodeproj/project.pbxproj.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: Tests/UITests/TestApp.xctestplan
================
{
  "configurations" : [
    {
      "id" : "074FA9C1-7635-4C64-BF5D-90402604CC46",
      "name" : "Default",
      "options" : {

      }
    }
  ],
  "defaultOptions" : {
    "codeCoverage" : {
      "targets" : [
        {
          "containerPath" : "container:..\/..",
          "identifier" : "SpeziLLMOpenAI",
          "name" : "SpeziLLMOpenAI"
        },
        {
          "containerPath" : "container:..\/..",
          "identifier" : "SpeziLLM",
          "name" : "SpeziLLM"
        },
        {
          "containerPath" : "container:..\/..",
          "identifier" : "SpeziLLMLocal",
          "name" : "SpeziLLMLocal"
        },
        {
          "containerPath" : "container:..\/..",
          "identifier" : "SpeziLLMLocalDownload",
          "name" : "SpeziLLMLocalDownload"
        }
      ]
    },
    "targetForVariableExpansion" : {
      "containerPath" : "container:UITests.xcodeproj",
      "identifier" : "2F6D139128F5F384007C25D6",
      "name" : "TestApp"
    }
  },
  "testTargets" : [
    {
      "target" : {
        "containerPath" : "container:UITests.xcodeproj",
        "identifier" : "2F6D13AB28F5F386007C25D6",
        "name" : "TestAppUITests"
      }
    }
  ],
  "version" : 1
}

================
File: Tests/UITests/TestApp.xctestplan.license
================
This source file is part of the Stanford Spezi open-source project

SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT

================
File: .gitignore
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
# 

# Swift Package Manager
Package.resolved
*.xcodeproj
.swiftpm
.build
.xcodebuild
.derivedData
coverage.lcov
*.xcresult

# IDE related folders
.idea

# Xcode User settings
xcuserdata/

# Other files
.DS_Store
.env

# Documentation generation
*.doccarchive
docs/

# UITests Project
!UITests.xcodeproj

# Generated CA and TLS keys of the Fog webservice
*.crt
*.key
*.srl
*.csr

# npm dependencies
node_modules/

================
File: .linkspector.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2025 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
# 
dirs:
  - .
useGitIgnore: true
ignorePatterns:
  - pattern: '^https://platform.openai.com/docs/guides/.*$' # Causes false positives
  - pattern: '^doc:.*$'
  - pattern: '^http://localhost.*$'

================
File: .spi.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
# 

version: 1
builder:
  configs:
    - platform: ios
      documentation_targets:
      - SpeziLLM
      - SpeziLLMLocal
      - SpeziLLMLocalDownload
      - SpeziLLMOpenAI
      - SpeziLLMFog

================
File: .swiftlint.yml
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
# 

# The whitelist_rules configuration also includes rules that are enabled by default to provide a good overview of all rules.
only_rules:
  # All Images that provide context should have an accessibility label. Purely decorative images can be hidden from accessibility.
  - accessibility_label_for_image
  # Attributes should be on their own lines in functions and types, but on the same line as variables and imports.
  - attributes
  # Prefer using Array(seq) over seq.map { $0 } to convert a sequence into an Array.
  - array_init
  # Prefer the new block based KVO API with keypaths when using Swift 3.2 or later.
  - block_based_kvo
  # Non-constant variables should not be listed in a closure’s capture list to avoid confusion about closures capturing variables at creation time.
  - capture_variable
  # Delegate protocols should be class-only so they can be weakly referenced.
  - class_delegate_protocol
  # Closing brace with closing parenthesis should not have any whitespaces in the middle.
  - closing_brace
  # Closure bodies should not span too many lines.
  - closure_body_length
  # Closure end should have the same indentation as the line that started it.
  - closure_end_indentation
  # Closure parameters should be on the same line as opening brace.
  - closure_parameter_position
  # Closure expressions should have a single space inside each brace.
  - closure_spacing
  # Use commas to separate types in inheritance lists
  - comma_inheritance
  # Prefer at least one space after slashes for comments.
  - comment_spacing
  # All elements in a collection literal should be vertically aligned
  - collection_alignment
  # Colons should be next to the identifier when specifying a type and next to the key in dictionary literals.
  - colon
  # There should be no space before and one after any comma.
  - comma
  # The initializers declared in compiler protocols such as ExpressibleByArrayLiteral shouldn't be called directly.
  - compiler_protocol_init
  # Getter and setters in computed properties and subscripts should be in a consistent order.
  - computed_accessors_order
  # Conditional statements should always return on the next line
  - conditional_returns_on_newline
  # Prefer contains over comparing filter(where:).count to 0.
  - contains_over_filter_count
  # Prefer contains over using filter(where:).isEmpty
  - contains_over_filter_is_empty
  # Prefer `contains` over `first(where:) != nil`
  - contains_over_first_not_nil
  # Prefer contains over range(of:) != nil and range(of:) == nil
  - contains_over_range_nil_comparison
  # if, for, guard, switch, while, and catch statements shouldn't unnecessarily wrap their conditionals or arguments in parentheses.
  - control_statement
  # Types used for hosting only static members should be implemented as a caseless enum to avoid instantiation.
  - convenience_type
  # Complexity of function bodies should be limited.
  - cyclomatic_complexity
  # Availability checks or attributes shouldn’t be using older versions that are satisfied by the deployment target.
  - deployment_target
  # When registering for a notification using a block, the opaque observer that is returned should be stored so it can be removed later.
  - discarded_notification_center_observer
  # Discouraged direct initialization of types that can be harmful. e.g. UIDevice(), Bundle()
  - discouraged_direct_init
  # Prefer initializers over object literals.
  - discouraged_object_literal
  # Prefer non-optional booleans over optional booleans.
  - discouraged_optional_boolean
  # Prefer empty collection over optional collection.
  - discouraged_optional_collection
  # Dictionary literals with duplicated keys will crash in runtime.
  - duplicated_key_in_dictionary_literal
  # Duplicate Imports
  # - duplicate_imports
  # Avoid using 'dynamic' and '@inline(__always)' together.
  - dynamic_inline
  # Prefer checking isEmpty over comparing collection to an empty array or dictionary literal.
  - empty_collection_literal
  # Prefer checking `isEmpty` over comparing `count` to zero.
  - empty_count
  # Arguments can be omitted when matching enums with associated types if they are not used.
  - empty_enum_arguments
  # Prefer () -> over Void ->.
  - empty_parameters
  # When using trailing closures, empty parentheses should be avoided after the method call.
  - empty_parentheses_with_trailing_closure
  # Prefer checking `isEmpty` over comparing string to an empty string literal.
  - empty_string
  # Empty XCTest method should be avoided.
  - empty_xctest_method
  # Number of associated values in an enum case should be low
  - enum_case_associated_values_count
  # Explicitly calling .init() should be avoided.
  - explicit_init
  # A fatalError call should have a message.
  - fatal_error_message
  # Files should not span too many lines.
  # See file_length below for the exact configuration.
  - file_length
  # File name should not contain any whitespace.
  - file_name_no_space
  # Specifies how the types within a file should be ordered.
  - file_types_order
  # Prefer using ``.first(where:)`` over ``.filter { }.first` in collections.
  - first_where
  # Prefer flatMap over map followed by reduce([], +).
  - flatmap_over_map_reduce
  # where clauses are preferred over a single if inside a for.
  - for_where
  # Force casts should be avoided.
  - force_cast
  # Force tries should be avoided.
  - force_try
  # Force unwrapping should be avoided.
  - force_unwrapping
  # Prefer to locate parameters with defaults toward the end of the parameter list.
  - function_default_parameter_at_end
  # Functions bodies should not span too many lines.
  # See function_body_length below for the exact configuration.
  - function_body_length
  # Number of function parameters should be low.
  # See function_parameter_count below for the exact configuration.
  - function_parameter_count
  # Generic type name should only contain alphanumeric characters, start with an uppercase character and span between 1 and 20 characters in length.
  - generic_type_name
  # Comparing two identical operands is likely a mistake.
  - identical_operands
  # Identifier names should only contain alphanumeric characters and start with a lowercase character or should only contain capital letters.
  # In an exception to the above, variable names may start with a capital letter when they are declared static and immutable.
  # Variable names should not be too long or too short. Excluded names are listed below.
  - identifier_name
  # Computed read-only properties and subscripts should avoid using the get keyword.
  - implicit_getter
  # Prefer implicit returns in closures.
  - implicit_return
  # Implicitly unwrapped optionals should be avoided when possible.
  - implicitly_unwrapped_optional
  # Identifiers should use inclusive language that avoids discrimination against groups of people based on race, gender, or socioeconomic status
  - inclusive_language
  # Prefer using Set.isDisjoint(with:) over Set.intersection(_:).isEmpty.
  - is_disjoint
  # Discouraged explicit usage of the default separator.
  - joined_default_parameter
  # Tuples shouldn't have too many members. Create a custom type instead.
  # See large_tuple below for the exact configuration.
  - large_tuple
  # Prefer using .last(where:) over .filter { }.last in collections.
  - last_where
  # Files should not contain leading whitespace.
  - leading_whitespace
  # CGGeometry: Struct extension properties and methods are preferred over legacy functions
  - legacy_cggeometry_functions
  # Struct-scoped constants are preferred over legacy global constants (CGSize, CGRect, NSPoint, ...).
  - legacy_constant
  # Swift constructors are preferred over legacy convenience functions (CGPointMake, CGSizeMake, UIOffsetMake, ...).
  - legacy_constructor
  # Prefer using the hash(into:) function instead of overriding hashValue
  - legacy_hashing
  # Prefer using the isMultiple(of:) function instead of using the remainder operator (%).
  - legacy_multiple
  # Struct extension properties and methods are preferred over legacy functions
  - legacy_nsgeometry_functions
  # Prefer using type.random(in:) over legacy functions.
  - legacy_random
  # Lines should not span too many characters.
  # See line_length below for the exact configuration.
  - line_length
  # Array and dictionary literal end should have the same indentation as the line that started it.
  - literal_expression_end_indentation
  # Ensure definitions have a lower access control level than their enclosing parent
  - lower_acl_than_parent
  # MARK comment should be in valid format. e.g. '// MARK: ...' or '// MARK: - ...'
  - mark
  # Declarations should be documented.
  - missing_docs
  # Modifier order should be consistent.
  - modifier_order
  # Arguments should be either on the same line, or one per line.
  - multiline_arguments
  # Multiline arguments should have their surrounding brackets in a new line.
  - multiline_arguments_brackets
  # Chained function calls should be either on the same line, or one per line.
  - multiline_function_chains
  # Multiline literals should have their surrounding brackets in a new line.
  - multiline_literal_brackets
  # Functions and methods parameters should be either on the same line, or one per line.
  - multiline_parameters
  # Multiline parameters should have their surrounding brackets in a new line.
  - multiline_parameters_brackets
  # Types and functions should only be nested to a certain level deep.
  # See nesting below for the exact configuration.
  - nesting
  # Prefer Nimble operator overloads over free matcher functions.
  - nimble_operator
  # Prefer not to use extension access modifiers
  - no_extension_access_modifier
  # Fallthroughs can only be used if the case contains at least one other statement.  
  - no_fallthrough_only
  # Don’t add a space between the method name and the parentheses.
  - no_space_in_method_call
  # An object should only remove itself as an observer in deinit.
  - notification_center_detachment
  # Static strings should be used as key in NSLocalizedString in order to genstrings work.
  - nslocalizedstring_key
  # NSObject subclasses should implement isEqual instead of ==.
  - nsobject_prefer_isequal
  # Prefer object literals over image and color inits.
  - object_literal
  # Opening braces should be preceded by a single space and on the same line as the declaration.
  - opening_brace
  # Operators should be surrounded by a single whitespace when they are being used.
  - operator_usage_whitespace
  # Operators should be surrounded by a single whitespace when defining them.
  - operator_whitespace
  # Matching an enum case against an optional enum without ‘?’ is supported on Swift 5.1 and above.
  - optional_enum_case_matching
  # A doc comment should be attached to a declaration.
  - orphaned_doc_comment
  # Extensions shouldn’t override declarations.
  - override_in_extension
  # Some overridden methods should always call super
  - overridden_super_call
  # Combine multiple pattern matching bindings by moving keywords out of tuples.
  - pattern_matching_keywords
  # Prefer Self over type(of: self) when accessing properties or calling methods.
  - prefer_self_type_over_type_of_self
  # Prefer .zero over explicit init with zero parameters (e.g. CGPoint(x: 0, y: 0))
  - prefer_zero_over_explicit_init
  # Prefer private over fileprivate declarations.
  - private_over_fileprivate
  # Combine Subject should be private.
  - private_subject
  # Unit tests marked private are silently skipped.
  - private_unit_test
  # Creating views using Interface Builder should be avoided.
  - prohibited_interface_builder
  # Some methods should not call super (
    # NSFileProviderExtension: providePlaceholder(at:completionHandler:)
    # NSTextInput doCommand(by:)
    # NSView updateLayer()
    # UIViewController loadView())
  - prohibited_super_call
  # When declaring properties in protocols, the order of accessors should be get set.
  - protocol_property_accessors_order
  # Prefer using .allSatisfy() or .contains() over reduce(true) or reduce(false)
  - reduce_boolean
  # Prefer reduce(into:_:) over reduce(_:_:) for copy-on-write types
  - reduce_into
  # Prefer _ = foo() over let _ = foo() when discarding a result from a function.
  - redundant_discardable_let
  # nil coalescing operator is only evaluated if the lhs is nil, coalescing operator with nil as rhs is redundant
  - redundant_nil_coalescing
  # Objective-C attribute (@objc) is redundant in declaration.
  - redundant_objc_attribute
  # Initializing an optional variable with nil is redundant.
  - redundant_optional_initialization
  # Property setter access level shouldn't be explicit if it's the same as the variable access level.
  - redundant_set_access_control
  # String enum values can be omitted when they are equal to the enumcase name.
  - redundant_string_enum_value
  # Variables should not have redundant type annotation
  - redundant_type_annotation
  # Returning Void in a function declaration is redundant.
  - redundant_void_return
  # Return arrow and return type should be separated by a single space or on a separate line.
  - return_arrow_whitespace
  # Returning values from Void functions should be avoided.
  - return_value_from_void_function
  # Re-bind self to a consistent identifier name.
  - self_binding
  # Prefer shorthand operators (+=, -=, *=, /=) over doing the operation and assigning.
  - shorthand_operator
  # Test files should contain a single QuickSpec or XCTestCase class.
  - single_test_class
  # Prefer using `min()`` or `max()`` over `sorted().first` or `sorted().last`
  - sorted_first_last
  # Imports should be sorted.
  - sorted_imports
  # Else and catch should be on the same line, one space after the previous declaration.
  - statement_position
  # Operators should be declared as static functions, not free functions.
  - static_operator
  # SwiftLint ‘disable’ commands are superfluous when the disabled rule would not have triggered a violation in the disabled region. Use “ - ” if you wish to document a command.
  - superfluous_disable_command
  # Case statements should vertically align with their enclosing switch statement, or indented if configured otherwise.
  - switch_case_alignment
  # Shorthand syntactic sugar should be used, i.e. [Int] instead of Array.
  - syntactic_sugar
  # TODOs and FIXMEs should be resolved.
  - todo
  # Prefer someBool.toggle() over someBool = !someBool.
  - toggle_bool
  # Trailing closure syntax should be used whenever possible.
  - trailing_closure
  # Trailing commas in arrays and dictionaries should be avoided/enforced.
  - trailing_comma
  # Files should have a single trailing newline.
  - trailing_newline
  # Lines should not have trailing semicolons.
  - trailing_semicolon
  # Lines should not have trailing whitespace.
  # Ignored lines are specified below.
  - trailing_whitespace
  # Type bodies should not span too many lines.
  # See large_tuple below for the exact configuration.
  - type_body_length
  # Specifies the order of subtypes, properties, methods & more within a type.
  - type_contents_order
  # Type name should only contain alphanumeric characters, start with an uppercase character and span between 3 and 40 characters in length.
  # Excluded types are listed below.
  - type_name
  # Prefer using Array(seq) over seq.map { $0 } to convert a sequence into an Array.
  - typesafe_array_init
  # Use #unavailable/#available instead of #available/#unavailable with an empty body.
  - unavailable_condition
  # Unimplemented functions should be marked as unavailable.
  - unavailable_function
  # Avoid using unneeded break statements.
  - unneeded_break_in_switch
  # Parentheses are not needed when declaring closure arguments.
  - unneeded_parentheses_in_closure_argument
  # Prefer capturing references as weak to avoid potential crashes.
  - unowned_variable_capture
  # Catch statements should not declare error variables without type casting.
  - untyped_error_in_catch
  # Unused parameter in a closure should be replaced with _.
  - unused_closure_parameter
  # Unused control flow label should be removed.
  - unused_control_flow_label
  # Declarations should be referenced at least once within all files linted.
  - unused_declaration
  # When the index or the item is not used, .enumerated() can be removed.
  - unused_enumerated
  # All imported modules should be required to make the file compile.
  - unused_import
  # Prefer != nil over let _ =
  - unused_optional_binding
  # Setter value is not used.
  - unused_setter_value
  # @IBInspectable should be applied to variables only, have its type explicit and be of a supported type
  - valid_ibinspectable
  # Function parameters should be aligned vertically if they're in multiple lines in a declaration.
  - vertical_parameter_alignment
  # Function parameters should be aligned vertically if they're in multiple lines in a method call.
  - vertical_parameter_alignment_on_call
  # Limit vertical whitespace to a single empty line.
  # See vertical_whitespace below for the exact configuration.
  - vertical_whitespace
  # Don’t include vertical whitespace (empty line) before closing braces.
  - vertical_whitespace_closing_braces
  # Don’t include vertical whitespace (empty line) after opening braces.
  - vertical_whitespace_opening_braces
  # Using ternary to call Void functions should be avoided.
  - void_function_in_ternary
  # Prefer -> Void over -> ().
  - void_return
  # Delegates should be weak to avoid reference cycles.
  - weak_delegate
  # Prefer specific XCTest matchers over XCTAssertEqual and XCTAssertNotEqual
  - xct_specific_matcher
  # An XCTFail call should include a description of the assertion.
  - xctfail_message
  # The variable should be placed on the left, the constant on the right of a comparison operator.
  - yoda_condition

deployment_target: # Availability checks or attributes shouldn’t be using older versions that are satisfied by the deployment target.
  iOSApplicationExtension_deployment_target: 16.0
  iOS_deployment_target: 16.0

attributes:
  attributes_with_arguments_always_on_line_above: false

excluded: # paths to ignore during linting. Takes precedence over `included`.
  - .build
  - .swiftpm
  - .derivedData
  - Tests/UITests/.derivedData

closure_body_length: # Closure bodies should not span too many lines.
  - 35 # warning - default: 20
  - 35 # error - default: 100

enum_case_associated_values_count: # Number of associated values in an enum case should be low
  - 5 # warning - default: 5
  - 5 # error - default: 6

file_length: # Files should not span too many lines.
  - 500 # warning - default: 400
  - 500 # error - default: 1000

function_body_length: # Functions bodies should not span too many lines.
  - 50 # warning - default: 40
  - 50 # error - default: 100

function_parameter_count: # Number of function parameters should be low.
  - 5 # warning - default: 5
  - 5 # error - default: 8

identifier_name:
  excluded: # excluded names
    - id
    - ok
    - or
    - p8
    - of
    - s3
    - at
    - to
    - in

large_tuple: # Tuples shouldn't have too many members. Create a custom type instead.
  - 2 # warning - default: 2
  - 2 # error - default: 3

line_length: # Lines should not span too many characters.
  warning: 150 # default: 120
  error: 150 # default: 200
  ignores_comments: true # default: false
  ignores_urls: true # default: false
  ignores_function_declarations: false # default: false
  ignores_interpolated_strings: true # default: false

nesting: # Types should be nested at most 2 level deep, and functions should be nested at most 5 levels deep.
  type_level:
    warning: 2 # warning - default: 1
  function_level:
    warning: 5 # warning - default: 5
    
trailing_closure:
  only_single_muted_parameter: true

type_body_length: # Type bodies should not span too many lines.
  - 250 # warning - default: 200
  - 250 # error - default: 200

type_name:
  excluded: # excluded names
    - ID

trailing_whitespace:
  ignores_empty_lines: true # default: false
  ignores_comments: true # default: false
  
unused_optional_binding:
  ignore_optional_try: true

vertical_whitespace: # Limit vertical whitespace to a single empty line.
  max_empty_lines: 2 # warning - default: 1

================
File: CITATION.cff
================
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
# 

cff-version: 1.2.0
message: "If you use this software, please cite it as below."
authors:
- family-names: "Schmiedmayer"
  given-names: "Paul"
  orcid: "https://orcid.org/0000-0002-8607-9148"
- family-names: "Zagar"
  given-names: "Philipp"
  orcid: "https://orcid.org/0009-0001-5934-2078"
title: "SpeziLLM"
doi: 10.5281/zenodo.7538165
url: "https://github.com/StanfordSpezi/SpeziLLM"

================
File: CONTRIBUTORS.md
================
<!--
                  
#
# This source file is part of the Stanford Spezi open source project
#
# SPDX-FileCopyrightText: 2022 Stanford University and the project authors (see CONTRIBUTORS.md)
#
# SPDX-License-Identifier: MIT
# 
             
-->

SpeziLLM contributors
====================

* [Paul Schmiedmayer](https://github.com/PSchmiedmayer)
* [Vishnu Ravi](https://github.com/vishnuravi)
* [Philipp Zagar](https://github.com/philippzagar)
* [Adrit Rao](https://github.com/AdritRao)

================
File: LICENSE.md
================
MIT License

Copyright (c) 2022 Stanford University and the project authors (see CONTRIBUTORS.md)

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

================
File: Package.swift
================
let package = Package(

================
File: README.md
================
<!--
                  
This source file is part of the Stanford Spezi open source project

SPDX-FileCopyrightText: 2023 Stanford University and the project authors (see CONTRIBUTORS.md)

SPDX-License-Identifier: MIT
             
-->

# Spezi LLM

[![Build and Test](https://github.com/StanfordSpezi/SpeziLLM/actions/workflows/build-and-test.yml/badge.svg)](https://github.com/StanfordSpezi/SpeziLLM/actions/workflows/build-and-test.yml)
[![codecov](https://codecov.io/gh/StanfordSpezi/SpeziLLM/branch/main/graph/badge.svg?token=pptLyqtoNR)](https://codecov.io/gh/StanfordSpezi/SpeziLLM)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7954213.svg)](https://doi.org/10.5281/zenodo.7954213)
[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2FStanfordSpezi%2FSpeziLLM%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/StanfordSpezi/SpeziLLM)
[![](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2FStanfordSpezi%2FSpeziLLM%2Fbadge%3Ftype%3Dplatforms)](https://swiftpackageindex.com/StanfordSpezi/SpeziLLM)


## Overview

The Spezi LLM Swift Package includes modules that are helpful to integrate LLM-related functionality in your application.
The package provides all necessary tools for local LLM execution, the usage of remote OpenAI-based LLMs, as well as LLMs running on Fog node resources within the local network.

|<picture><source media="(prefers-color-scheme: dark)" srcset="Sources/SpeziLLMOpenAI/SpeziLLMOpenAI.docc/Resources/ChatView~dark.png"><img src="Sources/SpeziLLMOpenAI/SpeziLLMOpenAI.docc/Resources/ChatView.png" width="250" alt="Screenshot displaying the Chat View utilizing the OpenAI API from SpeziLLMOpenAI." /></picture>|<picture><source media="(prefers-color-scheme: dark)" srcset="Sources/SpeziLLMLocalDownload/SpeziLLMLocalDownload.docc/Resources/LLMLocalDownload~dark.png"><img src="Sources/SpeziLLMLocalDownload/SpeziLLMLocalDownload.docc/Resources/LLMLocalDownload.png" width="250" alt="Screenshot displaying the Local LLM Download View from SpeziLLMLocalDownload." /></picture>|<picture><source media="(prefers-color-scheme: dark)" srcset="Sources/SpeziLLMLocal/SpeziLLMLocal.docc/Resources/ChatView~dark.png"><img src="Sources/SpeziLLMLocal/SpeziLLMLocal.docc/Resources/ChatView.png" width="250" alt="Screenshot displaying the Chat View utilizing a locally executed LLM via SpeziLLMLocal." /></picture>|
|:--:|:--:|:--:|
|`OpenAI LLM Chat View`|`Language Model Download`|`Local LLM Chat View`|

## Setup

### 1. Add Spezi LLM as a Dependency

You need to add the SpeziLLM Swift package to
[your app in Xcode](https://developer.apple.com/documentation/xcode/adding-package-dependencies-to-your-app#) or
[Swift package](https://developer.apple.com/documentation/xcode/creating-a-standalone-swift-package-with-xcode#Add-a-dependency-on-another-Swift-package).

> [!IMPORTANT]  
> If your application is not yet configured to use Spezi, follow the [Spezi setup article](https://swiftpackageindex.com/stanfordspezi/spezi/documentation/spezi/initial-setup) to set up the core Spezi infrastructure.

### 2. Follow the setup steps of the individual targets

As Spezi LLM contains a variety of different targets for specific LLM functionalities, please follow the additional setup guide in the respective target section of this README.

## Targets

Spezi LLM provides a number of targets to help developers integrate LLMs in their Spezi-based applications:
- [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm): Base infrastructure of LLM execution in the Spezi ecosystem.
- [SpeziLLMLocal](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmlocal): Local LLM execution capabilities directly on-device. Enables running open-source LLMs from Hugging Face like [Meta's Llama2](https://ai.meta.com/llama/), [Microsoft's Phi](https://azure.microsoft.com/en-us/products/phi), [Google's Gemma](https://ai.google.dev/gemma), or [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1), among others. See [LLMLocalModel](https://swiftpackageindex.com/stanfordspezi/spezillm/main/documentation/spezillmlocal/llmlocalmodel) for a list of models tested with SpeziLLM.
- [SpeziLLMLocalDownload](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmlocaldownload): Download and storage manager of local Language Models, including onboarding views. 
- [SpeziLLMOpenAI](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmopenai): Integration with OpenAI's GPT models via using OpenAI's API service.
- [SpeziLLMFog](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmfog): Discover and dispatch LLM inference jobs to Fog node resources within the local network.

The section below highlights the setup and basic use of the [SpeziLLMLocal](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmlocal), [SpeziLLMOpenAI](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmopenai), and [SpeziLLMFog](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmfog) targets in order to integrate Language Models in a Spezi-based application. 

> [!NOTE]  
> To learn more about the usage of the individual targets, please refer to the [DocC documentation of the package](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation).

### Spezi LLM Local

The target enables developers to easily execute medium-size Language Models (LLMs) locally on-device. The module allows you to interact with the locally run LLM via purely Swift-based APIs, no interaction with low-level code is necessary, building on top of the infrastructure of the [SpeziLLM target](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm).

> [!IMPORTANT]  
> Spezi LLM Local is not compatible with simulators. The underlying [`mlx-swift`](https://github.com/ml-explore/mlx-swift) requires a modern Metal MTLGPUFamily and the simulator does not provide that.

> [!IMPORTANT]
> Important: To use the LLM local target, some LLMs require adding the *Increase Memory Limit* entitlement to the project.

#### Setup

You can configure the Spezi Local LLM execution within the typical `SpeziAppDelegate`.
In the example below, the `LLMRunner` from the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) target which is responsible for providing LLM functionality within the Spezi ecosystem is configured with the `LLMLocalPlatform` from the [SpeziLLMLocal](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmlocal) target. This prepares the `LLMRunner` to locally execute Language Models.

```swift
class TestAppDelegate: SpeziAppDelegate {
    override var configuration: Configuration {
        Configuration {
            LLMRunner {
                LLMLocalPlatform()
            }
        }
    }
}
```

[SpeziLLMLocalDownload](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmlocaldownload) can be used to download an LLM from [HuggingFace](https://huggingface.co/) and save it on the device for execution. The `LLMLocalDownloadView` provides an out-of-the-box onboarding view for downloading models locally.

```swift
struct LLMLocalOnboardingDownloadView: View {
    var body: some View {
        LLMLocalDownloadView(
            model: .llama3_8B_4bit,
            downloadDescription: "The Llama3 8B model will be downloaded",
        ) {
            // Action to perform after the model is downloaded and the user presses the next button.
        }
    }
}
```

> [!TIP]
> The `LLMLocalDownloadView` view can be included in your onboarding process using SpeziOnboarding as [demonstrated in this example](https://swiftpackageindex.com/stanfordspezi/spezillm/main/documentation/spezillmlocaldownload/llmlocaldownloadview#overview).


#### Usage

The code example below showcases the interaction with local LLMs through the the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner), which is injected into the SwiftUI `Environment` via the `Configuration` shown above.

The `LLMLocalSchema` defines the type and configurations of the to-be-executed `LLMLocalSession`. This transformation is done via the [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) that uses the `LLMLocalPlatform`. The inference via `LLMLocalSession/generate()` returns an `AsyncThrowingStream` that yields all generated `String` pieces.

```swift
struct LLMLocalDemoView: View {
    @Environment(LLMRunner.self) var runner
    @State var responseText = ""

    var body: some View {
        Text(responseText)
            .task {
                // Instantiate the `LLMLocalSchema` to an `LLMLocalSession` via the `LLMRunner`.
                let llmSession: LLMLocalSession = runner(
                    with: LLMLocalSchema(
                        model: .llama3_8B_4bit,
                    )
                )

                do {
                    for try await token in try await llmSession.generate() {
                        responseText.append(token)
                    }
                } catch {
                    // Handle errors here. E.g., you can use `ViewState` and `viewStateAlert` from SpeziViews.
                }
            }
    }
}
```

The [`LLMChatViewSchema`](https://swiftpackageindex.com/stanfordspezi/spezillm/main/documentation/spezillm/llmchatviewschema) can be used to easily create a conversational chat interface for your chatbot application with a local LLM.

```swift
struct LLMLocalChatView: View {
    var body: some View {
        LLMChatViewSchema(
            with: LLMLocalSchema(
                model: .llama3_8B_4bit
            )
        )
    }
}
```

> [!NOTE]  
> To learn more about the usage of SpeziLLMLocal, please refer to the comprehensive [DocC documentation](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmlocal).

### Spezi LLM Open AI

A module that allows you to interact with GPT-based Large Language Models (LLMs) from OpenAI within your Spezi application.
`SpeziLLMOpenAI` provides a pure Swift-based API for interacting with the OpenAI GPT API, building on top of the infrastructure of the [SpeziLLM target](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm).
In addition, `SpeziLLMOpenAI` provides developers with a declarative Domain Specific Language to utilize OpenAI function calling mechanism. This enables a structured, bidirectional, and reliable communication between the OpenAI LLMs and external tools, such as the Spezi ecosystem.

#### Setup

In order to use OpenAI LLMs within the Spezi ecosystem, the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) needs to be initialized in the Spezi `Configuration` with the `LLMOpenAIPlatform`. Only after, the `LLMRunner` can be used for inference of OpenAI LLMs.
See the [SpeziLLM documentation](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) for more details.

```swift
import Spezi
import SpeziLLM
import SpeziLLMOpenAI

class LLMOpenAIAppDelegate: SpeziAppDelegate {
    override var configuration: Configuration {
        Configuration {
            LLMRunner {
                LLMOpenAIPlatform()
            }
        }
    }
}
```

> [!IMPORTANT]
> If using `SpeziLLMOpenAI` on macOS, ensure to add the *`Keychain Access Groups` entitlement* to the enclosing Xcode project via *PROJECT_NAME > Signing&Capabilities > + Capability*. The array of keychain groups can be left empty, only the base entitlement is required.

#### Usage

The code example below showcases the interaction with an OpenAI LLM through the the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner), which is injected into the SwiftUI `Environment` via the `Configuration` shown above.

The `LLMOpenAISchema` defines the type and configurations of the to-be-executed `LLMOpenAISession`. This transformation is done via the [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) that uses the `LLMOpenAIPlatform`. The inference via `LLMOpenAISession/generate()` returns an `AsyncThrowingStream` that yields all generated `String` pieces.

```swift
import SpeziLLM
import SpeziLLMOpenAI
import SwiftUI

struct LLMOpenAIDemoView: View {
    @Environment(LLMRunner.self) var runner
    @State var responseText = ""

    var body: some View {
        Text(responseText)
            .task {
                // Instantiate the `LLMOpenAISchema` to an `LLMOpenAISession` via the `LLMRunner`.
                let llmSession: LLMOpenAISession = runner(
                    with: LLMOpenAISchema(
                        parameters: .init(
                            modelType: .gpt4_o,
                            systemPrompt: "You're a helpful assistant that answers questions from users.",
                            overwritingToken: "abc123"
                        )
                    )
                )

                do {
                    for try await token in try await llmSession.generate() {
                        responseText.append(token)
                    }
                } catch {
                    // Handle errors here. E.g., you can use `ViewState` and `viewStateAlert` from SpeziViews.
                }
            }
    }
}
```

> [!NOTE]  
> To learn more about the usage of SpeziLLMOpenAI, please refer to the [DocC documentation](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmopenai).

### Spezi LLM Fog

The `SpeziLLMFog` target enables you to use LLMs running on [Fog node](https://en.wikipedia.org/wiki/Fog_computing) computing resources within the local network. The fog nodes advertise their services via [mDNS](https://en.wikipedia.org/wiki/Multicast_DNS), enabling clients to discover all fog nodes serving a specific host within the local network.
`SpeziLLMFog` then dispatches LLM inference jobs dynamically to a random fog node within the local network and streams the response to surface it to the user.

> [!IMPORTANT]
> `SpeziLLMFog` requires a `SpeziLLMFogNode` within the local network hosted on some computing resource that actually performs the inference requests. `SpeziLLMFog` provides the `SpeziLLMFogNode` Docker-based package that enables an easy setup of these fog nodes. See the `FogNode` directory on the root level of the SPM package as well as the respective `README.md` for more details.

#### Setup

In order to use Fog LLMs within the Spezi ecosystem, the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) needs to be initialized in the Spezi `Configuration` with the `LLMFogPlatform`. Only after, the `LLMRunner` can be used for inference with Fog LLMs. See the [SpeziLLM documentation](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) for more details.
The `LLMFogPlatform` needs to be initialized with the custom root CA certificate that was used to sign the fog node web service certificate (see the `FogNode/README.md` documentation for more information). Copy the root CA certificate from the fog node as resource to the application using `SpeziLLMFog` and use it to initialize the `LLMFogPlatform` within the Spezi `Configuration`.

```swift
class LLMFogAppDelegate: SpeziAppDelegate {
    private nonisolated static var caCertificateUrl: URL {
        // Return local file URL of root CA certificate in the `.crt` format
    }
    
    override var configuration: Configuration {
         Configuration {
             LLMRunner {
                // Set up the Fog platform with the custom CA certificate
                LLMRunner {
                    LLMFogPlatform(configuration: .init(caCertificate: Self.caCertificateUrl))
                }
            }
        }
    }
}
```

#### Usage

The code example below showcases the interaction with a Fog LLM through the the [SpeziLLM](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm) [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner), which is injected into the SwiftUI `Environment` via the `Configuration` shown above.

The `LLMFogSchema` defines the type and configurations of the to-be-executed `LLMFogSession`. This transformation is done via the [`LLMRunner`](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillm/llmrunner) that uses the `LLMFogPlatform`. The inference via `LLMFogSession/generate()` returns an `AsyncThrowingStream` that yields all generated `String` pieces.
The `LLMFogSession` automatically discovers all available LLM fog nodes within the local network upon setup and the dispatches the LLM inference jobs to the fog computing resource, streaming back the response and surfaces it to the user.

> [!IMPORTANT]  
> The `LLMFogSchema` accepts a closure that returns an authorization token that is passed with every request to the Fog node in the `Bearer` HTTP field via the `LLMFogParameters/init(modelType:systemPrompt:authToken:)`. The token is created via the closure upon every LLM inference request, as the `LLMFogSession` may be long lasting and the token could therefore expire. Ensure that the closure appropriately caches the token in order to prevent unnecessary token refresh roundtrips to external systems.

```swift
struct LLMFogDemoView: View {
    @Environment(LLMRunner.self) var runner
    @State var responseText = ""

    var body: some View {
        Text(responseText)
            .task {
                // Instantiate the `LLMFogSchema` to an `LLMFogSession` via the `LLMRunner`.
                let llmSession: LLMFogSession = runner(
                    with: LLMFogSchema(
                        parameters: .init(
                            modelType: .llama7B,
                            systemPrompt: "You're a helpful assistant that answers questions from users.",
                            authToken: {
                                // Return authorization token as `String` or `nil` if no token is required by the Fog node.
                            }
                        )
                    )
                )

                do {
                    for try await token in try await llmSession.generate() {
                        responseText.append(token)
                    }
                } catch {
                    // Handle errors here. E.g., you can use `ViewState` and `viewStateAlert` from SpeziViews.
                }
            }
    }
}
```

> [!NOTE]  
> To learn more about the usage of SpeziLLMFog, please refer to the [DocC documentation](https://swiftpackageindex.com/stanfordspezi/spezillm/documentation/spezillmfog).

## Contributing

Contributions to this project are welcome. Please make sure to read the [contribution guidelines](https://github.com/StanfordSpezi/.github/blob/main/CONTRIBUTING.md) and the [contributor covenant code of conduct](https://github.com/StanfordSpezi/.github/blob/main/CODE_OF_CONDUCT.md) first.


## License

This project is licensed under the MIT License. See [Licenses](https://github.com/StanfordSpezi/SpeziLLM/tree/main/LICENSES) for more information.

![Spezi Footer](https://raw.githubusercontent.com/StanfordSpezi/.github/main/assets/FooterLight.png#gh-light-mode-only)
![Spezi Footer](https://raw.githubusercontent.com/StanfordSpezi/.github/main/assets/FooterDark.png#gh-dark-mode-only)



================================================================
End of Codebase
================================================================
